{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# opensourced models\n",
    "\n",
    "##  LLama\n",
    "* paper: https://arxiv.org/pdf/2302.13971.pdf\n",
    "  * data Wikipedia [4.5%]: which use either the Latin or Cyrillic scripts: bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk.\n",
    "* https://github.com/facebookresearch/llama\n",
    "\n",
    "### Huatuo-Llama-Med-Chinese\n",
    "* https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese\n",
    "* \n",
    "\n",
    "## MOSS\n",
    "\n",
    "## ChatGLM\n",
    "* https://github.com/THUDM/ChatGLM-6B\n",
    "* An Open Bilingual Dialogue Language Model | 开源双语对话语言模型\n",
    "\n",
    "### 衍生项目\n",
    "* https://github.com/yanqiangmiffy/Chinese-LangChain\n",
    "* https://github.com/imClumsyPanda/langchain-ChatGLM\n",
    "* https://github.com/l15y/wenda\n",
    "* https://github.com/SCIR-HI/Med-ChatGLM\n",
    "* 更多：https://www.toutiao.com/article/7225991356471312948/?log_from=57e0e99656ef9_1683088485260\n",
    "\n",
    "## xmtf\n",
    "* Crosslingual Generalization through Multitask Finetuning\n",
    "* xP3 是 46 种语言的有监督数据集，带有英语和机器翻译的 prompts\n",
    "\n",
    "## stanford_alpaca\n",
    "* https://github.com/tatsu-lab/stanford_alpaca\n",
    "  * it was finetune on LLama 7B (affordable)\n",
    "* https://crfm.stanford.edu/2023/03/13/alpaca.html\n",
    "  * how the finetuning data is collected, and the evaluation results.\n",
    "\n",
    "## Baize\n",
    "* https://github.com/project-baize/baize\n",
    "* paper: https://arxiv.org/pdf/2304.01196.pdf\n",
    "* demo: https://huggingface.co/spaces/project-baize/Baize-7B\n",
    "* propose a pipeline that can automatically generate a high-quality multi-turn chat corpus by leveraging ChatGPT to engage in a conversation with itself.\n",
    "* based on LLaMA\n",
    "\n",
    "## Cabrita\n",
    "* https://github.com/22-hours/cabrita\n",
    "* try locally: https://github.com/22-hours/cabrita/blob/main/notebooks/cabrita-lora.ipynb\n",
    "* A portuguese finetuned instruction LLaMA. \n",
    "* just translated the alpaca_data.json to portuguese using ChatGPT, and then finetune.\n",
    "\n",
    "## Chinese-Vicuna\n",
    "* A Chinese Instruction-following LLaMA-based Model. 一个中文低资源的llama+lora方案，结构参考alpaca.\n",
    "* https://github.com/Facico/Chinese-Vicuna\n",
    "\n",
    "## GPT4-x-Alpaca\n",
    "* GPT4-x-Alpaca is a LLaMA 13B model fine-tuned with a collection of GPT4 conversations, GPTeacher. There’s not a lot of information on its training and performance.\n",
    "* https://huggingface.co/chavinlo/gpt4-x-alpaca\n",
    "\n",
    "## GPT4All\n",
    "* Demo, data and code to train an assistant-style large language model with ~800k GPT-3.5-Turbo Generations based on LLaMa.\n",
    "* https://github.com/nomic-ai/gpt4all\n",
    "\n",
    "## GPTQ-for-LLaMA\n",
    "* 4 bits quantization of LLaMA using GPTQ. GPTQ is SOTA one-shot weight quantization method.\n",
    "* https://github.com/qwopqwop200/GPTQ-for-LLaMa\n",
    "\n",
    "## Koala\n",
    "* Koala is a language model fine-tuned on top of LLaMA\n",
    "* Koala: A Dialogue Model for Academic Research\n",
    "* demo: https://chat.lmsys.org/?model=koala-13b\n",
    "* https://github.com/young-geng/EasyLM/blob/main/docs/koala.md\n",
    "\n",
    "## Pygmalion-7b\n",
    "* https://huggingface.co/PygmalionAI/pygmalion-7b\n",
    "* a dialogue model based on Meta’s LLaMA-7B. This is version 1. It has been fine-tuned using a subset of the data from Pygmalion-6B-v8-pt4, for those of you familiar with the project.\n",
    "\n",
    "## Vicuna (FastChat)\n",
    "* An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality.\n",
    "* https://github.com/lm-sys/FastChat\n",
    "\n",
    "## BLOOM (BigScience)\n",
    "* BigScience Large Open-science Open-access Multilingual Language Model.\n",
    "* https://huggingface.co/bigscience/bloom\n",
    "* demo: https://huggingface.co/spaces/huggingface/bloom_demo\n",
    "\n",
    "## FastChat-T5\n",
    "* compact and commercial-friendly chatbot!\n",
    "* https://github.com/lm-sys/FastChat#FastChat-T5\n",
    "* https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/huggingface_api.py\n",
    "\n",
    "## More\n",
    "* https://medium.com/geekculture/list-of-open-sourced-fine-tuned-large-language-models-llm-8d95a2e0dc76\n",
    "\n",
    "# tools\n",
    "\n",
    "## FlagOpen\n",
    "\n",
    "* https://github.com/FlagOpen\n",
    "* 2022年2月28日，智源研究院最新发布了FlagOpen飞智大模型技术开源体系\n",
    "* 包括数据标注部分。FlagData\n",
    "\n",
    "\n",
    "## open assistant\n",
    "* https://github.com/LAION-AI/Open-Assistant\n",
    "\n",
    "## OpenChatKit\n",
    "* https://github.com/togethercomputer/OpenChatKit\n",
    "* provides a powerful, open-source base to create both specialized and general purpose chatbots for various applications. The kit includes an instruction-tuned language models, a moderation model, and an extensible retrieval system for including up-to-date responses from custom repositories. OpenChatKit models were trained on the OIG-43M training dataset, which was a collaboration between Together, LAION, and Ontocord.ai.\n",
    "* Augmenting the model with additional context from a retrieval index\n",
    "\n",
    "## ChatRWKV\n",
    "\n",
    "## xTuring\n",
    "* https://xturing.stochastic.ai/finetune/guide/\n",
    "* Easily build, customize and control your own LLMs\n",
    "\n",
    "## MLC LLM\n",
    "* Enable everyone to develop, optimize and deploy AI models natively on everyone's devices.\n",
    "* https://github.com/mlc-ai/mlc-llm\n",
    "\n",
    "\n",
    "## langchain\n",
    "* https://github.com/hwchase17/langchain\n",
    "  * https://github.com/hwchase17/chat-langchain \n",
    "    * running example: This repo is an implementation of a locally hosted chatbot specifically focused on question answering over the LangChain documentation. Built with LangChain and FastAPI.\n",
    "  * https://huggingface.co/spaces/JavaFXpert/Chat-GPT-LangChain\n",
    "* Building applications with LLMs through composability\n",
    "* do the dirty work when building apps with LLMs.\n",
    "\n",
    "## others\n",
    "* https://github.com/nichtdax/awesome-totally-open-chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q transformers accelerate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigscience/bloomz-7b1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "inputs = tokenizer.encode(\"Translate to English: Je t’aime.\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
