{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PaddleNlp\n",
    "\n",
    "## PaddlePaddle install \n",
    "\n",
    "* https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/install/pip/linux-pip.html\n",
    "\n",
    "## PaddleNlp docs.\n",
    "\n",
    "* https://paddlenlp.readthedocs.io/zh/latest -- 文档\n",
    "\n",
    "## source code \n",
    "\n",
    "### all supported transformer tokenizer\n",
    "\n",
    "* paddlenlp/transformers/auto/tokenizer.py\n",
    "* paddlenlp/transformers/ernie/tokenizer.py  # ernie相关的\n",
    "\n",
    "### transformer model configs\n",
    "\n",
    "* paddlenlp/transformers/ernie/modeling.py\n",
    "\n",
    "### transformer model zoom, and supported language.\n",
    "\n",
    "* https://paddlenlp.readthedocs.io/zh/latest/model_zoo/index.html#transformer  -- bert  有Portuguese 支持\n",
    "* https://paddlenlp.readthedocs.io/zh/latest/model_zoo/transformers/GPT/contents.html -- GPT 有Portuguese 支持"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/scipy/fft/__init__.py:97: DeprecationWarning: The module numpy.dual is deprecated.  Instead of using dual, use the functions directly from numpy or scipy.\n",
      "  from numpy.dual import register_func\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/scipy/special/orthogonal.py:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from numpy import (exp, inf, pi, sqrt, floor, sin, cos, around, int,\n",
      "\u001b[32m[2022-08-10 15:13:58,688] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'ernie-3.0-medium-zh'.\u001b[0m\n",
      "\u001b[32m[2022-08-10 15:13:58,689] [    INFO]\u001b[0m - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_medium_zh_vocab.txt and saved to /home/jeffye/.paddlenlp/models/ernie-3.0-medium-zh\u001b[0m\n",
      "\u001b[32m[2022-08-10 15:13:58,691] [    INFO]\u001b[0m - Downloading ernie_3.0_medium_zh_vocab.txt from https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_medium_zh_vocab.txt\u001b[0m\n",
      "100%|██████████| 182k/182k [00:00<00:00, 1.00MB/s]\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('ernie-3.0-medium-zh')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-08-10 15:14:33,981] [    INFO]\u001b[0m - Downloading tokenizer_config.json from https://bj.bcebos.com/paddlenlp/models/community/neuralmind/bert-base-portuguese-cased/tokenizer_config.json\u001b[0m\n",
      "100%|██████████| 167/167 [00:00<00:00, 89.6kB/s]\n",
      "\u001b[32m[2022-08-10 15:14:34,215] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.bert.tokenizer.BertTokenizer'> to load 'neuralmind/bert-base-portuguese-cased'.\u001b[0m\n",
      "\u001b[32m[2022-08-10 15:14:34,217] [    INFO]\u001b[0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/neuralmind/bert-base-portuguese-cased/vocab.txt and saved to /home/jeffye/.paddlenlp/models/neuralmind/bert-base-portuguese-cased\u001b[0m\n",
      "\u001b[32m[2022-08-10 15:14:34,218] [    INFO]\u001b[0m - Downloading vocab.txt from https://bj.bcebos.com/paddlenlp/models/community/neuralmind/bert-base-portuguese-cased/vocab.txt\u001b[0m\n",
      "100%|██████████| 205k/205k [00:00<00:00, 956kB/s] \n",
      "\u001b[32m[2022-08-10 15:14:34,699] [    INFO]\u001b[0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/neuralmind/bert-base-portuguese-cased/added_tokens.json and saved to /home/jeffye/.paddlenlp/models/neuralmind/bert-base-portuguese-cased\u001b[0m\n",
      "\u001b[32m[2022-08-10 15:14:34,700] [    INFO]\u001b[0m - Downloading added_tokens.json from https://bj.bcebos.com/paddlenlp/models/community/neuralmind/bert-base-portuguese-cased/added_tokens.json\u001b[0m\n",
      "\u001b[32m[2022-08-10 15:14:34,879] [    INFO]\u001b[0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/neuralmind/bert-base-portuguese-cased/special_tokens_map.json and saved to /home/jeffye/.paddlenlp/models/neuralmind/bert-base-portuguese-cased\u001b[0m\n",
      "\u001b[32m[2022-08-10 15:14:34,881] [    INFO]\u001b[0m - Downloading special_tokens_map.json from https://bj.bcebos.com/paddlenlp/models/community/neuralmind/bert-base-portuguese-cased/special_tokens_map.json\u001b[0m\n",
      "\u001b[32m[2022-08-10 15:14:35,067] [    INFO]\u001b[0m - Already cached /home/jeffye/.paddlenlp/models/neuralmind/bert-base-portuguese-cased/tokenizer_config.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tokenizer_pt = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-08-10 19:19:05,556] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.bert.modeling.BertModel'> to load 'bert-base-multilingual-uncased'.\u001b[0m\n",
      "\u001b[32m[2022-08-10 19:19:05,559] [    INFO]\u001b[0m - Downloading http://bj.bcebos.com/paddlenlp/models/transformers/bert-base-multilingual-uncased.pdparams and saved to /home/jeffye/.paddlenlp/models/bert-base-multilingual-uncased\u001b[0m\n",
      "\u001b[32m[2022-08-10 19:19:05,560] [    INFO]\u001b[0m - Downloading bert-base-multilingual-uncased.pdparams from http://bj.bcebos.com/paddlenlp/models/transformers/bert-base-multilingual-uncased.pdparams\u001b[0m\n",
      " 17%|█▋        | 245M/1.40G [21:21<1:43:23, 200kB/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/d/code/learn/pylearn/nlp/paddlenlp.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/code/learn/pylearn/nlp/paddlenlp.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# pretrained_model = AutoModel.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/code/learn/pylearn/nlp/paddlenlp.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# del pretrained_model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/code/learn/pylearn/nlp/paddlenlp.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# pretrained_model = AutoModel.from_pretrained(\"pierreguillou/bert-base-cased-squad-v1.1-portuguese\")\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/code/learn/pylearn/nlp/paddlenlp.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# del pretrained_model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/code/learn/pylearn/nlp/paddlenlp.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m pretrained_model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mbert-base-multilingual-uncased\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/paddlenlp/transformers/auto/modeling.py:382\u001b[0m, in \u001b[0;36mAutoModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, task, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mcls\u001b[39m,\n\u001b[1;32m    324\u001b[0m                     pretrained_model_name_or_path,\n\u001b[1;32m    325\u001b[0m                     task\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    326\u001b[0m                     \u001b[39m*\u001b[39mmodel_args,\n\u001b[1;32m    327\u001b[0m                     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    328\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[39m    Creates an instance of `AutoModel`. Model weights are loaded\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[39m    by specifying name of a built-in pretrained model, or a community contributed model,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39m            # <class 'paddlenlp.transformers.bert.modeling.BertForPretraining'>\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(pretrained_model_name_or_path, task,\n\u001b[1;32m    383\u001b[0m                                 \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/paddlenlp/transformers/auto/modeling.py:199\u001b[0m, in \u001b[0;36m_BaseAutoModelClass._from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, task, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m                     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    190\u001b[0m                         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mimport_class\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m only supports the following classes: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m                         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(m \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m all_model_classes) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m                             \u001b[39mfor\u001b[39;00m task \u001b[39min\u001b[39;00m all_tasks) \u001b[39m+\u001b[39m\n\u001b[1;32m    195\u001b[0m                         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m to load \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    196\u001b[0m                 logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m    197\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mWe are using \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to load \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    198\u001b[0m                     (model_class, pretrained_model_name_or_path))\n\u001b[0;32m--> 199\u001b[0m                 \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    200\u001b[0m                     pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args,\n\u001b[1;32m    201\u001b[0m                     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    202\u001b[0m \u001b[39m# From local dir path\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(pretrained_model_name_or_path):\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/paddlenlp/transformers/model_utils.py:252\u001b[0m, in \u001b[0;36mPretrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *args, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mDownloading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m and saved to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    250\u001b[0m             (file_path, default_root))\n\u001b[1;32m    251\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 252\u001b[0m     resolved_resource_files[file_id] \u001b[39m=\u001b[39m get_path_from_url(\n\u001b[1;32m    253\u001b[0m         file_path, default_root)\n\u001b[1;32m    254\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    255\u001b[0m     logger\u001b[39m.\u001b[39merror(err)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/paddlenlp/utils/downloader.py:162\u001b[0m, in \u001b[0;36mget_path_from_url\u001b[0;34m(url, root_dir, md5sum, check_exist)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[39mif\u001b[39;00m ParallelEnv()\u001b[39m.\u001b[39mlocal_rank \u001b[39m%\u001b[39m \u001b[39m8\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 162\u001b[0m         fullpath \u001b[39m=\u001b[39m _download(url, root_dir, md5sum)\n\u001b[1;32m    163\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m         \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(fullpath):\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/paddlenlp/utils/downloader.py:213\u001b[0m, in \u001b[0;36m_download\u001b[0;34m(url, path, md5sum)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[39mif\u001b[39;00m total_size:\n\u001b[1;32m    208\u001b[0m     \u001b[39mwith\u001b[39;00m tqdm(\n\u001b[1;32m    209\u001b[0m             total\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(total_size),\n\u001b[1;32m    210\u001b[0m             unit\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mB\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    211\u001b[0m             unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    212\u001b[0m             unit_divisor\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m--> 213\u001b[0m         \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m req\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m):\n\u001b[1;32m    214\u001b[0m             f\u001b[39m.\u001b[39mwrite(chunk)\n\u001b[1;32m    215\u001b[0m             pbar\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/requests/models.py:750\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m'\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    749\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 750\u001b[0m         \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    751\u001b[0m             \u001b[39myield\u001b[39;00m chunk\n\u001b[1;32m    752\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/urllib3/response.py:575\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    574\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[0;32m--> 575\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m    577\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m    578\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/urllib3/response.py:518\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    517\u001b[0m     cache_content \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 518\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    519\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    520\u001b[0m         amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data\n\u001b[1;32m    521\u001b[0m     ):  \u001b[39m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[39m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    528\u001b[0m         \u001b[39m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/http/client.py:459\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    457\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 459\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    461\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/http/client.py:503\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    498\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[1;32m    500\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 503\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[1;32m    505\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    506\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# pretrained_model = AutoModel.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "# del pretrained_model\n",
    "# pretrained_model = AutoModel.from_pretrained(\"pierreguillou/bert-base-cased-squad-v1.1-portuguese\")\n",
    "# del pretrained_model\n",
    "pretrained_model = AutoModel.from_pretrained(\"bert-base-multilingual-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8879477719907407"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7984598/256/3/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67f4027c7030b010d964bcecab45732ca129fbf42fed140ec1ae6d06ef81ce73"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
