{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PaddleNlp\n",
    "\n",
    "## PaddlePaddle install \n",
    "\n",
    "* https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/install/pip/linux-pip.html\n",
    "\n",
    "## PaddleNlp docs.\n",
    "\n",
    "* https://paddlenlp.readthedocs.io/zh/latest -- 文档\n",
    "* api: https://paddlenlp.readthedocs.io/en/latest/source/paddlenlp.transformers.tokenizer_utils.html\n",
    "\n",
    "## source code \n",
    "\n",
    "### all supported transformer tokenizer\n",
    "\n",
    "* paddlenlp/transformers/auto/tokenizer.py\n",
    "* paddlenlp/transformers/ernie/tokenizer.py  # ernie相关的\n",
    "\n",
    "### transformer model configs\n",
    "\n",
    "* paddlenlp/transformers/ernie/modeling.py\n",
    "\n",
    "### transformer model zoom, and supported language.\n",
    "\n",
    "* https://paddlenlp.readthedocs.io/zh/latest/model_zoo/index.html#transformer  -- bert  有Portuguese 支持\n",
    "* https://paddlenlp.readthedocs.io/zh/latest/model_zoo/transformers/GPT/contents.html -- GPT 有Portuguese 支持"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PaddleNlp 数据集、模型下载到默认路径\n",
    "\n",
    "* A: 内置的数据集、模型默认会下载到$HOME/.paddlenlp/下，通过配置环境变量可下载到指定路径：\n",
    "\n",
    "    *（1）Linux下，设置 export PPNLP_HOME=\"xxxx\"，注意不要设置带有中文字符的路径。\n",
    "\n",
    "    *（2）Windows下，同样配置环境变量 PPNLP_HOME 到其他非中文字符路径，重启即可。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer 返回结果含义\n",
    "\n",
    "* model_outputs.py 中 BaseModelOutputWithPoolingAndCrossAttentions 有详细说明\n",
    "    * https://huggingface.co/transformers/v3.0.2/model_doc/bert.html 解释, BertModel\n",
    "* modeling.py 中有 ErnieForSequenceClassification， ErnieForQuestionAnswering， ErnieForTokenClassification， ErnieLMPredictionHead， ErnieForMultipleChoice， ErnieForMaskedLM\n",
    "\n",
    "### Transformers pytorch return\n",
    "\n",
    "* last_hidden_state (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size)):\n",
    "    * Sequence of hidden-states at the output of the last layer of the model.\n",
    "* pooler_output (torch.FloatTensor: of shape (batch_size, hidden_size)):\n",
    "    * Last layer hidden-state of the first token of the sequence (classification token) further processed by a Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence prediction (classification) objective during pre-training.\n",
    "\n",
    "    * This output is usually not a good summary of the semantic content of the input, you’re often better with averaging or pooling the sequence of hidden-states for the whole input sequence.\n",
    "\n",
    "* hidden_states (tuple(torch.FloatTensor), optional, returned when output_hidden_states=True is passed or when config.output_hidden_states=True):\n",
    "    * Tuple of torch.FloatTensor (one for the output of the embeddings + one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).\n",
    "    * Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "\n",
    "* attentions (tuple(torch.FloatTensor), optional, returned when output_attentions=True is passed or when config.output_attentions=True):\n",
    "    * Tuple of torch.FloatTensor (one for each layer) of shape (batch_size, num_heads, sequence_length, sequence_length).\n",
    "    * Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "\n",
    "### ernie 超详细中文预训练模型ERNIE使用指南\n",
    "* https://aistudio.baidu.com/paddle/forum/topic/show/954092\n",
    "\n",
    "### tokenizer 参数文档\n",
    "\n",
    "* https://github.com/PaddlePaddle/PaddleNLP/blob/2000ea2ea88f5ef7804d1ee31f6b57f9feb6f006/paddlenlp/transformers/tokenizer_utils.py#L622-L625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-10-16 23:04:31,673] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'ernie-3.0-medium-zh'.\u001b[0m\n",
      "\u001b[32m[2022-10-16 23:04:31,680] [    INFO]\u001b[0m - Already cached /mnt/d/dataset/nlp/paddle/.paddlenlp/models/ernie-3.0-medium-zh/ernie_3.0_medium_zh_vocab.txt\u001b[0m\n",
      "\u001b[32m[2022-10-16 23:04:31,736] [    INFO]\u001b[0m - tokenizer config file saved in /mnt/d/dataset/nlp/paddle/.paddlenlp/models/ernie-3.0-medium-zh/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2022-10-16 23:04:31,743] [    INFO]\u001b[0m - Special tokens file saved in /mnt/d/dataset/nlp/paddle/.paddlenlp/models/ernie-3.0-medium-zh/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 647, 358, 9, 340, 9, 635, 3016, 1764, 292, 210, 7, 314, 5, 138, 362, 563, 2, 9, 14, 3016, 1764, 292, 210, 7, 314, 5, 305, 563, 1114, 12045, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import AutoModel, AutoTokenizer\n",
    "from paddlenlp.transformers import ErnieForTokenClassification, ErnieTokenizer\n",
    "\n",
    "import paddle\n",
    "\n",
    "model_name = 'ernie-3.0-medium-zh'\n",
    "# model_name = \"ernie-3.0-xbase-zh\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = ErnieForTokenClassification.from_pretrained(model_name)\n",
    "# model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# 请问有没有像琉璃神社一样的资源站 有和琉璃神社一样的网站吗？\n",
    "\n",
    "# inputs = tokenizer(\"请问有没有像琉璃神社一样的资源站\",\n",
    "#                    \"有和琉璃神社一样的网站吗？\", max_seq_len=64, pad_to_max_seq_len=True)\n",
    "\n",
    "query = \"请问有没有像琉璃神社一样的资源站\"\n",
    "title = \"有和琉璃神社一样的网站吗？\"\n",
    "max_seq_length = 64\n",
    "pad_to_max = True\n",
    "\n",
    "inputs = tokenizer(query, title, max_seq_len=max_seq_length, pad_to_max_seq_len=pad_to_max, return_attention_mask=True)\n",
    "\n",
    "print(inputs)\n",
    "\n",
    "print(len(inputs['input_ids']))\n",
    "# inputs = {k:paddle.to_tensor([v]) for (k, v) in inputs.items()}\n",
    "# # logits = model(**inputs, return_dict=True, output_hidden_states=True, output_attentions=True)\n",
    "# logits = model(**inputs)\n",
    "# print(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[1, 15, 768], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       [[[ 0.06013136,  0.08001949,  0.05949559, ...,  0.03834157,\n",
      "           0.16428161, -0.02553395],\n",
      "         [ 0.17710927,  1.07631350,  0.22704059, ..., -0.01483721,\n",
      "          -0.58641475, -0.37783957],\n",
      "         [ 0.63684541,  0.03024112,  0.39377603, ...,  0.12226901,\n",
      "           0.43927503,  1.50784373],\n",
      "         ...,\n",
      "         [ 1.05572450,  2.52401543, -0.73081750, ..., -0.19437474,\n",
      "           0.94409478,  1.50580680],\n",
      "         [-0.25393343,  0.31314465,  0.06259530, ..., -0.44231522,\n",
      "          -0.11833040, -0.35460761],\n",
      "         [ 0.34953618, -1.13955617,  0.45758203, ...,  0.39407942,\n",
      "           0.17412712, -0.47145003]]])\n"
     ]
    }
   ],
   "source": [
    "print(logits.hidden_states[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-08-10 15:14:33,981] [    INFO]\u001b[0m - Downloading tokenizer_config.json from https://bj.bcebos.com/paddlenlp/models/community/neuralmind/bert-base-portuguese-cased/tokenizer_config.json\u001b[0m\n",
      "100%|██████████| 167/167 [00:00<00:00, 89.6kB/s]\n",
      "\u001b[32m[2022-08-10 15:14:34,215] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.bert.tokenizer.BertTokenizer'> to load 'neuralmind/bert-base-portuguese-cased'.\u001b[0m\n",
      "\u001b[32m[2022-08-10 15:14:34,217] [    INFO]\u001b[0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/neuralmind/bert-base-portuguese-cased/vocab.txt and saved to /home/jeffye/.paddlenlp/models/neuralmind/bert-base-portuguese-cased\u001b[0m\n",
      "\u001b[32m[2022-08-10 15:14:34,218] [    INFO]\u001b[0m - Downloading vocab.txt from https://bj.bcebos.com/paddlenlp/models/community/neuralmind/bert-base-portuguese-cased/vocab.txt\u001b[0m\n",
      "100%|██████████| 205k/205k [00:00<00:00, 956kB/s] \n",
      "\u001b[32m[2022-08-10 15:14:34,699] [    INFO]\u001b[0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/neuralmind/bert-base-portuguese-cased/added_tokens.json and saved to /home/jeffye/.paddlenlp/models/neuralmind/bert-base-portuguese-cased\u001b[0m\n",
      "\u001b[32m[2022-08-10 15:14:34,700] [    INFO]\u001b[0m - Downloading added_tokens.json from https://bj.bcebos.com/paddlenlp/models/community/neuralmind/bert-base-portuguese-cased/added_tokens.json\u001b[0m\n",
      "\u001b[32m[2022-08-10 15:14:34,879] [    INFO]\u001b[0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/neuralmind/bert-base-portuguese-cased/special_tokens_map.json and saved to /home/jeffye/.paddlenlp/models/neuralmind/bert-base-portuguese-cased\u001b[0m\n",
      "\u001b[32m[2022-08-10 15:14:34,881] [    INFO]\u001b[0m - Downloading special_tokens_map.json from https://bj.bcebos.com/paddlenlp/models/community/neuralmind/bert-base-portuguese-cased/special_tokens_map.json\u001b[0m\n",
      "\u001b[32m[2022-08-10 15:14:35,067] [    INFO]\u001b[0m - Already cached /home/jeffye/.paddlenlp/models/neuralmind/bert-base-portuguese-cased/tokenizer_config.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tokenizer_pt = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_model = AutoModel.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "# del pretrained_model\n",
    "# pretrained_model = AutoModel.from_pretrained(\"pierreguillou/bert-base-cased-squad-v1.1-portuguese\")\n",
    "# del pretrained_model\n",
    "pretrained_model = AutoModel.from_pretrained(\"bert-base-multilingual-uncased\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizer api\n",
    "* 文档： https://paddlenlp.readthedocs.io/en/latest/source/paddlenlp.transformers.ernie.tokenizer.html\n",
    "* huggingface: https://huggingface.co/docs/transformers/main/en/main_classes/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-05-07 21:31:14,609] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'ernie-3.0-medium-zh'.\u001b[0m\n",
      "\u001b[32m[2023-05-07 21:31:14,610] [    INFO]\u001b[0m - Already cached C:\\Users\\73915\\.paddlenlp\\models\\ernie-3.0-medium-zh\\ernie_3.0_medium_zh_vocab.txt\u001b[0m\n",
      "\u001b[32m[2023-05-07 21:31:14,644] [    INFO]\u001b[0m - tokenizer config file saved in C:\\Users\\73915\\.paddlenlp\\models\\ernie-3.0-medium-zh\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-05-07 21:31:14,646] [    INFO]\u001b[0m - Special tokens file saved in C:\\Users\\73915\\.paddlenlp\\models\\ernie-3.0-medium-zh\\special_tokens_map.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import AutoModel, AutoTokenizer\n",
    "from paddlenlp.transformers import ErnieForTokenClassification, ErnieTokenizer\n",
    "\n",
    "import paddle\n",
    "# from paddlenlp.transformers import ErnieTinyTokenizer\n",
    "# tokenizer = ErnieTinyTokenizer.from_pretrained('ernie-tiny')\n",
    "\n",
    "# model_name = 'ernie-3.0-medium-zh'\n",
    "# # model_name = \"ernie-3.0-xbase-zh\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized: ['this', 'is', '我', '所', '要', '的']\n",
      "convert_tokens_to_ids: [3730, 2775, 75, 110, 41, 5]\n",
      "token to index [3730, 2775, 75, 110, 41, 5]\n",
      "token ids: {'input_ids': [1, 3730, 2775, 75, 110, 41, 5, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0]} 开始插入1[CLS], 结束加入[SEP]2\n",
      "id2token: ['[CLS]', 'this', 'is', '我', '所', '要', '的', '[SEP]']\n",
      "[MASK] id= [3]\n",
      "id=0, token= ['[PAD]']\n"
     ]
    }
   ],
   "source": [
    "vocabs = tokenizer.get_vocab()\n",
    "# 分词\n",
    "tokens = tokenizer.tokenize(\"this is我所要的\")\n",
    "\n",
    "# tokenizer.get_vocab()[token] is equivalent to tokenizer.convert_tokens_to_ids(token) when token is in the vocab.\n",
    "strings = tokenizer.convert_tokens_to_ids(tokens)\n",
    "token_ids_vocab2index = [vocabs[tok] for tok in tokens]\n",
    "# text to ids\n",
    "token_ids = tokenizer(\"this is我所要的\")\n",
    "\n",
    "# id to token\n",
    "id2tokens = tokenizer.convert_ids_to_tokens(token_ids['input_ids'])\n",
    "\n",
    "print('tokenized:', tokens)\n",
    "print('convert_tokens_to_ids:', strings)\n",
    "print('token to index', token_ids_vocab2index)\n",
    "print('token ids:', token_ids, '开始插入1[CLS], 结束加入[SEP]2')\n",
    "print('id2token:', id2tokens)\n",
    "print('[MASK] id=', tokenizer.convert_tokens_to_ids(['[MASK]']))\n",
    "print('id=0, token=', tokenizer.convert_ids_to_tokens([0]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer (Ernie) modeling 说明\n",
    "* 源码位置： paddlenlp\\transformers\\ernie\\modeling.py\n",
    "* 可以查看ModelOutput，每种model 返回格式： paddlenlp\\transformers\\model_outputs.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAC\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'LAC是个优秀的分词工具',\n",
       "  'segs': ['LAC', '是', '个', '优秀', '的', '分词', '工具'],\n",
       "  'tags': ['nz', 'v', 'q', 'a', 'u', 'n', 'n']}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from paddlenlp import Taskflow\n",
    "\n",
    "lac = Taskflow(\"lexical_analysis\")\n",
    "lac(\"LAC是个优秀的分词工具\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d76abebb651b803d7523773c2538185af67bbe68e12b1f9d8b8e0e281792ff9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
