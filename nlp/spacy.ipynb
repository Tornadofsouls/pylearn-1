{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download models\n",
    "* https://spacy.io/usage/models\n",
    "\n",
    "## everything about spacy\n",
    "* https://spacy.io/usage/spacy-101 \n",
    "* https://spacy.io/api/matcher\n",
    "\n",
    "## add a pipeline\n",
    "\n",
    "* how to use nlp.pipe and disable some component\n",
    "* Multiprocessing: https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods\n",
    "* https://spacy.io/usage/processing-pipelines\n",
    "\n",
    "## make a classifier\n",
    "\n",
    "* https://spacy.io/universe/project/classyclassification\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Add match ID \"HelloWorld\" with no callback and one pattern\n",
    "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
    "matcher.add(\"HelloWorld\", [pattern])\n",
    "\n",
    "doc = nlp(\"Hello, new world! Hello old world!\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angela Merkel\n",
      "Barack Obama\n",
      "Washington, D.C.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "terms = [\"Barack Obama\", \"Angela Merkel\", \"Washington, D.C.\"]\n",
    "# Only run nlp.make_doc to speed things up\n",
    "patterns = [nlp.make_doc(text) for text in terms]\n",
    "matcher.add(\"TerminologyList\", patterns)\n",
    "\n",
    "doc = nlp(\"German Chancellor Angela Merkel and US President Barack Obama \"\n",
    "          \"converse in the Oval Office inside the White House in Washington, D.C.\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <spacy_lookup.Entity object at 0x7f894c68d400> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/d/code/learn/pylearn/nlp/spacy.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/code/learn/pylearn/nlp/spacy.ipynb#ch0000005vscode-remote?line=3'>4</a>\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39men_core_web_sm\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/code/learn/pylearn/nlp/spacy.ipynb#ch0000005vscode-remote?line=4'>5</a>\u001b[0m entity \u001b[39m=\u001b[39m Entity(keywords_list\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mpython\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mproduct manager\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mjava platform\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/code/learn/pylearn/nlp/spacy.ipynb#ch0000005vscode-remote?line=5'>6</a>\u001b[0m nlp\u001b[39m.\u001b[39;49madd_pipe(entity, last\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/code/learn/pylearn/nlp/spacy.ipynb#ch0000005vscode-remote?line=7'>8</a>\u001b[0m doc \u001b[39m=\u001b[39m nlp(\u001b[39mu\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mI am a product manager for a java and python.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/d/code/learn/pylearn/nlp/spacy.ipynb#ch0000005vscode-remote?line=8'>9</a>\u001b[0m \u001b[39massert\u001b[39;00m doc\u001b[39m.\u001b[39m_\u001b[39m.\u001b[39mhas_entities \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/spacy/language.py:773\u001b[0m, in \u001b[0;36mLanguage.add_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    771\u001b[0m     bad_val \u001b[39m=\u001b[39m \u001b[39mrepr\u001b[39m(factory_name)\n\u001b[1;32m    772\u001b[0m     err \u001b[39m=\u001b[39m Errors\u001b[39m.\u001b[39mE966\u001b[39m.\u001b[39mformat(component\u001b[39m=\u001b[39mbad_val, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m--> 773\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err)\n\u001b[1;32m    774\u001b[0m name \u001b[39m=\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m factory_name\n\u001b[1;32m    775\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomponent_names:\n",
      "\u001b[0;31mValueError\u001b[0m: [E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <spacy_lookup.Entity object at 0x7f894c68d400> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy_lookup import Entity\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "entity = Entity(keywords_list=['python', 'product manager', 'java platform'])\n",
    "nlp.add_pipe(entity, last=True)\n",
    "\n",
    "doc = nlp(u\"I am a product manager for a java and python.\")\n",
    "assert doc._.has_entities == True\n",
    "assert doc[0]._.is_entity == False\n",
    "assert doc[3]._.entity_desc == 'product manager'\n",
    "assert doc[3]._.is_entity == True\n",
    "\n",
    "print([(token.text, token._.canonical) for token in doc if token._.is_entity])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashtext import KeywordProcessor\n",
    "keyword_processor = KeywordProcessor()\n",
    "# keyword_processor.add_keyword(<unclean name>, <standardised name>)\n",
    "keyword_processor.add_keyword('Big Apple', 'New York')\n",
    "keyword_processor.add_keyword('Bay Area')\n",
    "keywords_found = keyword_processor.extract_keywords('I love Big Apple and Bay Area.', span_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('SF', 7, 20), ('ORG1', 29, 41)]\n"
     ]
    }
   ],
   "source": [
    "keyword_processor = KeywordProcessor()\n",
    "clean_names  = ['ORG', 'LOC', 'SF', \"ORG\", 'ORG1']\n",
    "keyword_names = ['new york', 'new york', 'san francisco', 'new york', 'new york loc']\n",
    "for keyword_name, clean_name in zip(keyword_names, clean_names):\n",
    "    keyword_processor.add_keyword(keyword_name, clean_name)\n",
    "keywords_found = keyword_processor.extract_keywords('I love san francisco and NY. new york loc is the best.', span_info=True)\n",
    "print(keywords_found)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple', 'ORG', 2, 3, 9, 14), ('San Francisco', 'GPE', 10, 12, 50, 63)]\n",
      "6550644825991781785 Radio 0 11 en hello Apple is opening its first big office in San\n",
      "6550644825991781785 Radio 0 12 en hello Apple is opening its first big office in San Francisco\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "\n",
    "nlp = English()\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(\"en hello Apple is opening its first big office in San Francisco. again\")\n",
    "print([(ent.text, ent.label_, ent.start, ent.end, ent.start_char, ent.end_char) for ent in doc.ents])\n",
    "\n",
    "# for token in doc:\n",
    "#     print(token.text, token.pos_, token.dep_, token.ent_type_)\n",
    "    \n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Add match ID \"HelloWorld\" with no callback and one pattern\n",
    "radio_pattern = [{\"OP\": \"*\"}, {\"LOWER\": {\"IN\": [\"en\", \"cute\"]}},\n",
    "                 {\"OP\": \"*\"}, {\"ENT_TYPE\": \"ORG\"}, {\"OP\": \"*\"}, {\"ENT_TYPE\": \"GPE\", \"OP\": \"+\"}]\n",
    "matcher.add(\"RadioClas\", [radio_pattern])\n",
    "\n",
    "# doc = nlp(\"Hello, world! Hello world!\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-27 16:15:33.688354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:73:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-07-27 16:15:33.688838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:d5:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-07-27 16:15:33.727083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:73:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-07-27 16:15:33.727469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:d5:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-07-27 16:15:33.727800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:73:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-07-27 16:15:33.728151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:d5:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "Tocar True  \n",
      "a True  \n",
      "música True  \n",
      "não True  \n",
      "deixe True  \n",
      "o True  \n",
      "samba True  \n",
      "morrer True  \n",
      "123 False  \n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.pt import Portuguese\n",
    "\n",
    "nlp = Portuguese()\n",
    "doc = nlp(\"Tocar a música não deixe o samba morrer 123\")\n",
    "\n",
    "print(len(doc))\n",
    "for token in doc:\n",
    "    print(token.text, token.is_alpha, token.ent_type_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d76abebb651b803d7523773c2538185af67bbe68e12b1f9d8b8e0e281792ff9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
