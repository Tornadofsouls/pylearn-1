{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/qlib/blob/main/examples/workflow_by_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# qlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/code/quant/qlib/scripts\n"
     ]
    }
   ],
   "source": [
    "#  Copyright (c) Microsoft Corporation.\n",
    "#  Licensed under the MIT License.\n",
    "import sys, site\n",
    "from pathlib import Path\n",
    "scripts_dir = Path.cwd().parent.joinpath(\"scripts\")\n",
    "print(scripts_dir)\n",
    "assert scripts_dir.joinpath(\"get_data.py\").exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.13 (default, Mar 28 2022, 11:38:47) \\n[GCC 7.5.0]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "################################# NOTE #################################\n",
    "#  Please be aware that if colab installs the latest numpy and pyqlib  #\n",
    "#  in this cell, users should RESTART the runtime in order to run the  #\n",
    "#  following cells successfully.                                       #\n",
    "########################################################################\n",
    "\n",
    "try:\n",
    "    import qlib\n",
    "except ImportError:\n",
    "    # install qlib\n",
    "    ! pip install --upgrade numpy\n",
    "    ! pip install pyqlib\n",
    "    # reload\n",
    "    site.main()\n",
    "\n",
    "# scripts_dir = Path.cwd().parent.joinpath(\"scripts\")\n",
    "if not scripts_dir.joinpath(\"get_data.py\").exists():\n",
    "    # download get_data.py script\n",
    "    scripts_dir = Path(\"~/tmp/qlib_code/scripts\").expanduser().resolve()\n",
    "    scripts_dir.mkdir(parents=True, exist_ok=True)\n",
    "    import requests\n",
    "    with requests.get(\"https://raw.githubusercontent.com/microsoft/qlib/main/scripts/get_data.py\") as resp:\n",
    "        with open(scripts_dir.joinpath(\"get_data.py\"), \"wb\") as fp:\n",
    "            fp.write(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import qlib\n",
    "import pandas as pd\n",
    "from qlib.constant import REG_CN\n",
    "from qlib.utils import exists_qlib_data, init_instance_by_config\n",
    "from qlib.workflow import R\n",
    "from qlib.workflow.record_temp import SignalRecord, PortAnaRecord\n",
    "from qlib.utils import flatten_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18564:MainThread](2022-04-20 08:41:38,030) INFO - qlib.Initialization - [config.py:402] - default_conf: client.\n",
      "[18564:MainThread](2022-04-20 08:41:38,035) INFO - qlib.Initialization - [__init__.py:74] - qlib successfully initialized based on client settings.\n",
      "[18564:MainThread](2022-04-20 08:41:38,036) INFO - qlib.Initialization - [__init__.py:76] - data_path={'__DEFAULT_FREQ': PosixPath('/home/jeffye/.qlib/qlib_data/cn_data')}\n"
     ]
    }
   ],
   "source": [
    "# use default data\n",
    "# NOTE: need to download data from remote: python scripts/get_data.py qlib_data_cn --target_dir ~/.qlib/qlib_data/cn_data\n",
    "provider_uri = \"~/.qlib/qlib_data/cn_data\"  # target_dir\n",
    "if not exists_qlib_data(provider_uri):\n",
    "    print(f\"Qlib data is not found in {provider_uri}\")\n",
    "    sys.path.append(str(scripts_dir))\n",
    "    from get_data import GetData\n",
    "    GetData().qlib_data(target_dir=provider_uri, region=REG_CN)\n",
    "qlib.init(provider_uri=provider_uri, region=REG_CN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "market = \"csi300\"\n",
    "benchmark = \"SH000300\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install necessary libs for CatBoostModel.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "[18564:MainThread](2022-04-20 08:41:39,891) WARNING - qlib.TRA - [pytorch_tra.py:98] - `eval_train` will be ignored when using TRA.router\n",
      "[18564:MainThread](2022-04-20 08:41:39,896) WARNING - qlib.TRA - [pytorch_tra.py:131] - logdir output/Alpha360 is not empty\n",
      "[18564:MainThread](2022-04-20 08:41:41,786) INFO - qlib.TRA - [pytorch_tra.py:140] - init TRAModel...\n",
      "[18564:MainThread](2022-04-20 08:41:47,052) INFO - qlib.TRA - [pytorch_tra.py:170] - # model params: 55936\n",
      "[18564:MainThread](2022-04-20 08:41:47,053) INFO - qlib.TRA - [pytorch_tra.py:171] - # tra params: 5606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): LSTM(6, 64, num_layers=2, batch_first=True)\n",
      "  (W): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (u): Linear(in_features=64, out_features=1, bias=False)\n",
      ")\n",
      "TRA(\n",
      "  (predictors): Linear(in_features=128, out_features=3, bias=True)\n",
      "  (router): LSTM(3, 32, batch_first=True)\n",
      "  (fc): Linear(in_features=160, out_features=3, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18564:MainThread](2022-04-20 08:42:15,427) INFO - qlib.timer - [log.py:113] - Time cost: 28.371s | Loading data Done\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/numpy/lib/nanfunctions.py:1096: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanmedian1d, axis, a, overwrite_input)\n",
      "[18564:MainThread](2022-04-20 08:44:14,423) INFO - qlib.timer - [log.py:113] - Time cost: 117.149s | RobustZScoreNorm Done\n",
      "[18564:MainThread](2022-04-20 08:45:54,785) INFO - qlib.timer - [log.py:113] - Time cost: 100.360s | Fillna Done\n",
      "[18564:MainThread](2022-04-20 08:45:57,883) INFO - qlib.timer - [log.py:113] - Time cost: 0.806s | CSRankNorm Done\n",
      "[18564:MainThread](2022-04-20 08:45:57,884) INFO - qlib.timer - [log.py:113] - Time cost: 222.455s | fit & process data Done\n",
      "[18564:MainThread](2022-04-20 08:45:57,885) INFO - qlib.timer - [log.py:113] - Time cost: 250.829s | Init data Done\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/qlib/contrib/data/dataset.py:171: UserWarning: the data has different shape from input_size and the data will be reshaped\n",
      "  warnings.warn(\"the data has different shape from input_size and the data will be reshaped\")\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# train model\n",
    "###################################\n",
    "data_handler_config = {\n",
    "    \"start_time\": \"2008-01-01\",\n",
    "    \"end_time\": \"2022-04-01\",\n",
    "    \"fit_start_time\": \"2008-01-01\",\n",
    "    \"fit_end_time\": \"2014-12-31\",\n",
    "    \"instruments\": market,\n",
    "    \"infer_processors\": [\n",
    "      {\n",
    "        \"class\": \"RobustZScoreNorm\",\n",
    "        \"kwargs\": {\n",
    "          \"fields_group\": \"feature\",\n",
    "          \"clip_outlier\": True\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"Fillna\",\n",
    "        \"kwargs\": {\n",
    "          \"fields_group\": \"feature\"\n",
    "        }\n",
    "      }\n",
    "    ],\n",
    "    \"learn_processors\": [\n",
    "      {\n",
    "        \"class\": \"CSRankNorm\",\n",
    "        \"kwargs\": {\n",
    "          \"fields_group\": \"label\"\n",
    "        }\n",
    "      }\n",
    "    ],\n",
    "    \"label\": [\n",
    "      \"Ref($close, -2) / Ref($close, -1) - 1\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# port_analysis_config = {\n",
    "#     \"executor\": {\n",
    "#         \"class\": \"SimulatorExecutor\",\n",
    "#         \"module_path\": \"qlib.backtest.executor\",\n",
    "#         \"kwargs\": {\n",
    "#             \"time_per_step\": \"day\",\n",
    "#             \"generate_portfolio_metrics\": True,\n",
    "#         },\n",
    "#     },\n",
    "#     \"strategy\": {\n",
    "#         \"class\": \"TopkDropoutStrategy\",\n",
    "#         \"module_path\": \"qlib.contrib.strategy.signal_strategy\",\n",
    "#         \"kwargs\": {\n",
    "#             \"model\": model,\n",
    "#             \"dataset\": dataset,\n",
    "#             \"topk\": 50,\n",
    "#             \"n_drop\": 5,\n",
    "#         },\n",
    "#     },\n",
    "#     \"backtest\": {\n",
    "#         \"start_time\": \"2017-01-01\",\n",
    "#         \"end_time\": \"2020-08-01\",\n",
    "#         \"account\": 100000000,\n",
    "#         \"benchmark\": benchmark,\n",
    "#         \"exchange_kwargs\": {\n",
    "#             \"freq\": \"day\",\n",
    "#             \"limit_threshold\": 0.095,\n",
    "#             \"deal_price\": \"close\",\n",
    "#             \"open_cost\": 0.0005,\n",
    "#             \"close_cost\": 0.0015,\n",
    "#             \"min_cost\": 5,\n",
    "#         },\n",
    "#     },\n",
    "# }\n",
    "\n",
    "task = {\n",
    "   \"model\": {\n",
    "      \"class\": \"TRAModel\",\n",
    "      \"module_path\": \"qlib.contrib.model.pytorch_tra\",\n",
    "       \"GPU\": 1,\n",
    "      \"kwargs\": {\n",
    "        \"tra_config\": {\n",
    "          \"num_states\": 3,\n",
    "          \"rnn_arch\": \"LSTM\",\n",
    "          \"hidden_size\": 32,\n",
    "          \"num_layers\": 1,\n",
    "          \"dropout\": 0.0,\n",
    "          \"tau\": 1.0,\n",
    "          \"src_info\": \"LR_TPE\"\n",
    "        },\n",
    "        \"model_config\": {\n",
    "          \"input_size\": 6,\n",
    "          \"hidden_size\": 64,\n",
    "          \"num_layers\": 2,\n",
    "          \"rnn_arch\": \"LSTM\",\n",
    "          \"use_attn\": True,\n",
    "          \"dropout\": 0.0\n",
    "        },\n",
    "        \"model_type\": \"RNN\",\n",
    "        \"lr\": 0.001,\n",
    "        \"n_epochs\": 100,\n",
    "        \"max_steps_per_epoch\": None,\n",
    "        \"early_stop\": 20,\n",
    "        \"logdir\": \"output/Alpha360\",\n",
    "        \"seed\": 0,\n",
    "        \"lamb\": 1.0,\n",
    "        \"rho\": 0.99,\n",
    "        \"alpha\": 0.5,\n",
    "        \"transport_method\": \"router\",\n",
    "        \"memory_mode\": \"sample\",\n",
    "        \"eval_train\": False,\n",
    "        \"eval_test\": True,\n",
    "        \"pretrain\": True,\n",
    "        \"init_state\": None,\n",
    "        \"freeze_model\": False,\n",
    "        \"freeze_predictors\": False\n",
    "      }\n",
    "    },\n",
    "    \"dataset\": {\n",
    "      \"class\": \"MTSDatasetH\",\n",
    "      \"module_path\": \"qlib.contrib.data.dataset\",\n",
    "      \"kwargs\": {\n",
    "        \"handler\": {\n",
    "          \"class\": \"Alpha360\",\n",
    "          \"module_path\": \"qlib.contrib.data.handler\",\n",
    "          \"kwargs\": data_handler_config,\n",
    "        },\n",
    "        \"segments\": {\n",
    "                \"train\": (\"2008-01-01\", \"2014-12-31\"),\n",
    "                \"valid\": (\"2015-01-01\", \"2016-12-31\"),\n",
    "                \"test\": (\"2021-01-01\", \"2022-03-31\"),\n",
    "            },\n",
    "        \"seq_len\": 60,\n",
    "        \"horizon\": 2,\n",
    "        \"input_size\": 6,\n",
    "        \"num_states\": 3,\n",
    "        \"batch_size\": 1024,\n",
    "        \"n_samples\": None,\n",
    "        \"memory_mode\": \"sample\",\n",
    "        \"drop_last\": True\n",
    "      }\n",
    "    },\n",
    "  # \"record\": [\n",
    "  #     {\n",
    "  #       \"class\": \"SignalRecord\",\n",
    "  #       \"module_path\": \"qlib.workflow.record_temp\",\n",
    "  #       \"kwargs\": {\n",
    "  #         \"model\": model,\n",
    "  #         \"dataset\": dataset\n",
    "  #       }\n",
    "  #     },\n",
    "  #     {\n",
    "  #       \"class\": \"SigAnaRecord\",\n",
    "  #       \"module_path\": \"qlib.workflow.record_temp\",\n",
    "  #       \"kwargs\": {\n",
    "  #         \"ana_long_short\": False,\n",
    "  #         \"ann_scaler\": 252\n",
    "  #       }\n",
    "  #     },\n",
    "  #     {\n",
    "  #       \"class\": \"PortAnaRecord\",\n",
    "  #       \"module_path\": \"qlib.workflow.record_temp\",\n",
    "  #       \"kwargs\": {\n",
    "  #         \"config\": {\n",
    "  #           \"strategy\": {\n",
    "  #             \"class\": \"TopkDropoutStrategy\",\n",
    "  #             \"module_path\": \"qlib.contrib.strategy\",\n",
    "  #             \"kwargs\": {\n",
    "  #               \"signal\": [\n",
    "  #                 model,\n",
    "  #                 dataset\n",
    "  #               ],\n",
    "  #               \"topk\": 50,\n",
    "  #               \"n_drop\": 5\n",
    "  #             }\n",
    "  #           },\n",
    "  #           \"backtest\": {\n",
    "  #             \"config\": port_analysis_config\n",
    "  #           }\n",
    "  #         }\n",
    "  #       }\n",
    "  #     }\n",
    "  #   ]\n",
    "}\n",
    "\n",
    "# model initiaiton\n",
    "model = init_instance_by_config(task[\"model\"])\n",
    "dataset = init_instance_by_config(task[\"dataset\"])\n",
    "\n",
    "# start exp to train model\n",
    "# with R.start(experiment_name=\"TRAModel_train_model\"):\n",
    "#     R.log_params(**flatten_dict(task))\n",
    "#     model.fit(dataset)\n",
    "#     R.save_objects(trained_model=model)\n",
    "#     rid = R.get_recorder().id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'numpy.ndarray'> datetime64[ns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "print(type(dataset._data))\n",
    "print(type(dataset._daily_index))\n",
    "print(type(dataset._daily_index.values), dataset._daily_index.dtype)\n",
    "\n",
    "from_date = datetime.date(2002,3,17)\n",
    "\n",
    "# to_date=from_date+datetime.timedelta(days=1)\n",
    "dataset._daily_index.values > pd.Timestamp(from_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scl= ('2008-01-01', '2014-12-31') 2008-01-01\n",
      "scl= ('2015-01-01', '2016-12-31') 2015-01-01\n"
     ]
    }
   ],
   "source": [
    "from qlib.data.dataset.handler import DataHandlerLP\n",
    "label_train, label_valid = dataset.prepare(\n",
    "    [\"train\", \"valid\"],\n",
    "    # col_set=[\"label\"],\n",
    "    # data_key=DataHandlerLP.DK_R,\n",
    ")\n",
    "# self.fit_thresh(label_train)\n",
    "# df_train, df_valid = dataset.prepare(\n",
    "#     [\"train\", \"valid\"],\n",
    "#     col_set=[\"feature\", \"label\"],\n",
    "#     data_key=DataHandlerLP.DK_L,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(label_valid.iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction, backtest & analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "# prediction, backtest & analysis\n",
    "###################################\n",
    "port_analysis_config = {\n",
    "    \"executor\": {\n",
    "        \"class\": \"SimulatorExecutor\",\n",
    "        \"module_path\": \"qlib.backtest.executor\",\n",
    "        \"kwargs\": {\n",
    "            \"time_per_step\": \"day\",\n",
    "            \"generate_portfolio_metrics\": True,\n",
    "        },\n",
    "    },\n",
    "    \"strategy\": {\n",
    "        \"class\": \"TopkDropoutStrategy\",\n",
    "        \"module_path\": \"qlib.contrib.strategy.signal_strategy\",\n",
    "        \"kwargs\": {\n",
    "            \"model\": model,\n",
    "            \"dataset\": dataset,\n",
    "            \"topk\": 50,\n",
    "            \"n_drop\": 5,\n",
    "        },\n",
    "    },\n",
    "    \"backtest\": {\n",
    "        \"start_time\": \"2021-01-01\",\n",
    "        \"end_time\": \"2022-03-31\",\n",
    "        \"account\": 100000000,\n",
    "        \"benchmark\": benchmark,\n",
    "        \"exchange_kwargs\": {\n",
    "            \"freq\": \"day\",\n",
    "            \"limit_threshold\": 0.095,\n",
    "            \"deal_price\": \"close\",\n",
    "            \"open_cost\": 0.0005,\n",
    "            \"close_cost\": 0.0015,\n",
    "            \"min_cost\": 5,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# # backtest and analysis\n",
    "# with R.start(experiment_name=\"backtest_analysis\"):\n",
    "#     recorder = R.get_recorder(recorder_id=rid, experiment_name=\"train_model\")\n",
    "#     model = recorder.load_object(\"trained_model\")\n",
    "\n",
    "#     # prediction\n",
    "#     recorder = R.get_recorder()\n",
    "#     ba_rid = recorder.id\n",
    "#     sr = SignalRecord(model, dataset, recorder)\n",
    "#     sr.generate()\n",
    "\n",
    "#     # backtest & analysis\n",
    "#     par = PortAnaRecord(recorder, port_analysis_config, \"day\")\n",
    "#     par.generate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18564:MainThread](2022-04-20 08:46:07,050) INFO - qlib.workflow - [expm.py:315] - <mlflow.tracking.client.MlflowClient object at 0x7fd5c2c92fa0>\n",
      "[18564:MainThread](2022-04-20 08:46:07,236) INFO - qlib.workflow - [exp.py:257] - Experiment 4 starts running ...\n",
      "[18564:MainThread](2022-04-20 08:46:07,237) WARNING - qlib.workflow - [exp.py:307] - Please make sure the recorder name mlflow_recorder is unique, we will only return the latest recorder if there exist several matched the given name.\n",
      "[18564:MainThread](2022-04-20 08:46:07,679) INFO - qlib.workflow - [recorder.py:293] - Recorder c8cd1d9bca3045aca03b2f54d8945b8e starts running under Experiment 4 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/Alpha360\n",
      "<class 'torch.utils.tensorboard.writer.SummaryWriter'>\n",
      "scl= ('2021-01-01', '2022-03-31') 2021-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                           | 0/106 [00:00<?, ?it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "  3%|███▋                                                                                                                               | 3/106 [00:00<00:03, 26.79it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "  7%|████████▋                                                                                                                          | 7/106 [00:00<00:03, 29.05it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "  9%|████████████▎                                                                                                                     | 10/106 [00:00<00:03, 28.54it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 13%|█████████████████▏                                                                                                                | 14/106 [00:00<00:02, 32.46it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 17%|██████████████████████                                                                                                            | 18/106 [00:00<00:02, 34.51it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 21%|██████████████████████████▉                                                                                                       | 22/106 [00:00<00:02, 35.81it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 25%|█████████████████████████████████                                                                                                 | 27/106 [00:00<00:02, 37.65it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 30%|███████████████████████████████████████▏                                                                                          | 32/106 [00:00<00:01, 39.42it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 35%|█████████████████████████████████████████████▍                                                                                    | 37/106 [00:01<00:01, 40.99it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 40%|███████████████████████████████████████████████████▌                                                                              | 42/106 [00:01<00:01, 41.89it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 44%|█████████████████████████████████████████████████████████▋                                                                        | 47/106 [00:01<00:01, 42.89it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 49%|███████████████████████████████████████████████████████████████▊                                                                  | 52/106 [00:01<00:01, 43.74it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 54%|█████████████████████████████████████████████████████████████████████▉                                                            | 57/106 [00:01<00:01, 44.15it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 58%|████████████████████████████████████████████████████████████████████████████                                                      | 62/106 [00:01<00:00, 44.15it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 63%|██████████████████████████████████████████████████████████████████████████████████▏                                               | 67/106 [00:01<00:00, 43.65it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 68%|████████████████████████████████████████████████████████████████████████████████████████▎                                         | 72/106 [00:01<00:00, 43.27it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 73%|██████████████████████████████████████████████████████████████████████████████████████████████▍                                   | 77/106 [00:01<00:00, 42.97it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 77%|████████████████████████████████████████████████████████████████████████████████████████████████████▌                             | 82/106 [00:02<00:00, 42.78it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 82%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▋                       | 87/106 [00:02<00:00, 42.32it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 87%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 92/106 [00:02<00:00, 42.20it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉           | 97/106 [00:02<00:00, 42.59it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏    | 102/106 [00:02<00:00, 42.80it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 106/106 [00:02<00:00, 40.77it/s]\n",
      "[18564:MainThread](2022-04-20 08:46:11,430) INFO - qlib.TRA - [pytorch_tra.py:515] - test metrics: {'MSE': 0.1593495730538765, 'MAE': 0.31855818321228596, 'IC': 0.043864721409758875, 'ICIR': 0.29140647490770055}\n",
      "[18564:MainThread](2022-04-20 08:46:11,602) INFO - qlib.workflow - [record_temp.py:194] - Signal record 'pred.pkl' has been saved as the artifact of the Experiment 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The following are prediction results of the TRAModel model.'\n",
      "                          score     label   score_0   score_1   score_2\n",
      "datetime   instrument                                                  \n",
      "2021-01-04 SH600000    0.018230  0.818243  0.049377  0.012907  0.018230\n",
      "           SH600004   -0.110520 -1.203987 -0.082736 -0.101145 -0.110520\n",
      "           SH600009   -0.131467  0.151959 -0.076913 -0.096610 -0.131467\n",
      "           SH600010   -0.018740 -0.005845  0.005471 -0.024033 -0.018740\n",
      "           SH600011    0.031496  0.385743  0.023085  0.010871  0.031496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18564:MainThread](2022-04-20 08:46:13,108) INFO - qlib.timer - [log.py:113] - Time cost: 0.000s | waiting `async_log` Done\n"
     ]
    }
   ],
   "source": [
    "# backtest and analysis\n",
    "with R.start(experiment_name=\"TRAModel_backtest_analysis\", resume=True):\n",
    "    rid = \"93ce1a9cf5a84ef981d1f7f5baa0d082\"\n",
    "    recorder = R.get_recorder(recorder_id=rid, experiment_name=\"train_model\")\n",
    "    model = recorder.load_object(\"trained_model\")\n",
    "    model.fitted = True\n",
    "    port_analysis_config[\"strategy\"][\"kwargs\"]['model'] = model\n",
    "    # due to bug of qlib \n",
    "    import os\n",
    "    print(model.logdir)\n",
    "    os.path.exists(model.logdir)\n",
    "    try:\n",
    "        from torch.utils.tensorboard import SummaryWriter\n",
    "    except ImportError:\n",
    "        SummaryWriter = None\n",
    "        print(\"SummaryWriter = None\")\n",
    "    print(SummaryWriter)\n",
    "    model._writer = SummaryWriter(log_dir=model.logdir)  # here is the bug in pytorch_tra.py L134\n",
    "    ##############################\n",
    "\n",
    "    # prediction\n",
    "    recorder = R.get_recorder()\n",
    "    ba_rid = recorder.id\n",
    "    sr = SignalRecord(model, dataset, recorder)\n",
    "    sr.generate()\n",
    "\n",
    "    # backtest & analysis\n",
    "    # par = PortAnaRecord(recorder, port_analysis_config, \"day\")\n",
    "    # par.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18564:MainThread](2022-04-20 08:46:13,157) INFO - qlib.backtest caller - [__init__.py:83] - Create new exchange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scl= ('2021-01-01', '2022-03-31') 2021-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                           | 0/106 [00:00<?, ?it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "  4%|████▉                                                                                                                              | 4/106 [00:00<00:02, 35.15it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "  8%|█████████▉                                                                                                                         | 8/106 [00:00<00:02, 33.46it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 11%|██████████████▋                                                                                                                   | 12/106 [00:00<00:02, 35.65it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 15%|███████████████████▌                                                                                                              | 16/106 [00:00<00:02, 35.54it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 20%|█████████████████████████▊                                                                                                        | 21/106 [00:00<00:02, 37.72it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 25%|███████████████████████████████▉                                                                                                  | 26/106 [00:00<00:02, 38.61it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 29%|██████████████████████████████████████                                                                                            | 31/106 [00:00<00:01, 39.69it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 34%|████████████████████████████████████████████▏                                                                                     | 36/106 [00:00<00:01, 40.60it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 39%|██████████████████████████████████████████████████▎                                                                               | 41/106 [00:01<00:01, 41.14it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 43%|████████████████████████████████████████████████████████▍                                                                         | 46/106 [00:01<00:01, 40.97it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 48%|██████████████████████████████████████████████████████████████▌                                                                   | 51/106 [00:01<00:01, 41.08it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 53%|████████████████████████████████████████████████████████████████████▋                                                             | 56/106 [00:01<00:01, 41.11it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 58%|██████████████████████████████████████████████████████████████████████████▊                                                       | 61/106 [00:01<00:01, 41.84it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 62%|████████████████████████████████████████████████████████████████████████████████▉                                                 | 66/106 [00:01<00:00, 42.58it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 67%|███████████████████████████████████████████████████████████████████████████████████████                                           | 71/106 [00:01<00:00, 42.87it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 72%|█████████████████████████████████████████████████████████████████████████████████████████████▏                                    | 76/106 [00:01<00:00, 43.07it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 76%|███████████████████████████████████████████████████████████████████████████████████████████████████▎                              | 81/106 [00:01<00:00, 42.46it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 81%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▍                        | 86/106 [00:02<00:00, 42.60it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 86%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                  | 91/106 [00:02<00:00, 42.41it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋            | 96/106 [00:02<00:00, 42.55it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      " 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉      | 101/106 [00:02<00:00, 42.53it/s]/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 106/106 [00:02<00:00, 41.01it/s]\n",
      "[18564:MainThread](2022-04-20 08:46:23,155) INFO - qlib.TRA - [pytorch_tra.py:515] - test metrics: {'MSE': 0.15930309643234192, 'MAE': 0.3185009594883554, 'IC': 0.04414338072026046, 'ICIR': 0.29278198382625986}\n",
      "[18564:MainThread](2022-04-20 08:46:23,182) WARNING - qlib.BaseExecutor - [executor.py:111] - `common_infra` is not set for <qlib.backtest.executor.SimulatorExecutor object at 0x7fd5c2642040>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd221dc7be640c199497e6955336ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "backtest loop:   0%|          | 0/243 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/qlib/utils/index_data.py:480: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(self.data)\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/qlib/utils/index_data.py:480: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(self.data)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4132 is out of bounds for axis 0 with size 4132",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m port_analysis_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbacktest\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_time\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2022-03-29\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m par \u001b[38;5;241m=\u001b[39m PortAnaRecord(recorder, port_analysis_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mpar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/qlib/workflow/record_temp.py:232\u001b[0m, in \u001b[0;36mACRecordTemp.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe dependent data does not exists. Generation skipped.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/qlib/workflow/record_temp.py:468\u001b[0m, in \u001b[0;36mPortAnaRecord._generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbacktest_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_time\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m get_date_by_shift(dt_values\u001b[38;5;241m.\u001b[39mmax(), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;66;03m# custom strategy and get backtest\u001b[39;00m\n\u001b[0;32m--> 468\u001b[0m portfolio_metric_dict, indicator_dict \u001b[38;5;241m=\u001b[39m \u001b[43mnormal_backtest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecutor_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbacktest_config\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _freq, (report_normal, positions_normal) \u001b[38;5;129;01min\u001b[39;00m portfolio_metric_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreport_normal_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_freq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m: report_normal})\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/qlib/backtest/__init__.py:255\u001b[0m, in \u001b[0;36mbacktest\u001b[0;34m(start_time, end_time, strategy, executor, benchmark, account, exchange_kwargs, pos_type)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m\"\"\"initialize the strategy and executor, then backtest function for the interaction of the outermost strategy and executor in the nested decision execution\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m \n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    245\u001b[0m trade_strategy, trade_executor \u001b[38;5;241m=\u001b[39m get_strategy_executor(\n\u001b[1;32m    246\u001b[0m     start_time,\n\u001b[1;32m    247\u001b[0m     end_time,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m     pos_type\u001b[38;5;241m=\u001b[39mpos_type,\n\u001b[1;32m    254\u001b[0m )\n\u001b[0;32m--> 255\u001b[0m portfolio_metrics, indicator \u001b[38;5;241m=\u001b[39m \u001b[43mbacktest_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrade_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrade_executor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m portfolio_metrics, indicator\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/qlib/backtest/backtest.py:28\u001b[0m, in \u001b[0;36mbacktest_loop\u001b[0;34m(start_time, end_time, trade_strategy, trade_executor)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"backtest function for the interaction of the outermost strategy and executor in the nested decision execution\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03mplease refer to the docs of `collect_data_loop`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    it computes the trading indicator\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m return_value \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _decision \u001b[38;5;129;01min\u001b[39;00m collect_data_loop(start_time, end_time, trade_strategy, trade_executor, return_value):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m return_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mportfolio_metrics\u001b[39m\u001b[38;5;124m\"\u001b[39m), return_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindicator\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/qlib/backtest/backtest.py:65\u001b[0m, in \u001b[0;36mcollect_data_loop\u001b[0;34m(start_time, end_time, trade_strategy, trade_executor, return_value)\u001b[0m\n\u001b[1;32m     63\u001b[0m _execute_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trade_executor\u001b[38;5;241m.\u001b[39mfinished():\n\u001b[0;32m---> 65\u001b[0m     _trade_decision: BaseTradeDecision \u001b[38;5;241m=\u001b[39m \u001b[43mtrade_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_trade_decision\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_execute_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     _execute_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m trade_executor\u001b[38;5;241m.\u001b[39mcollect_data(_trade_decision, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     67\u001b[0m     bar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/qlib/contrib/strategy/signal_strategy.py:122\u001b[0m, in \u001b[0;36mTopkDropoutStrategy.generate_trade_decision\u001b[0;34m(self, execute_result)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_trade_decision\u001b[39m(\u001b[38;5;28mself\u001b[39m, execute_result\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# get the number of trading step finished, trade_step can be [0, 1, 2, ..., trade_len - 1]\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     trade_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrade_calendar\u001b[38;5;241m.\u001b[39mget_trade_step()\n\u001b[0;32m--> 122\u001b[0m     trade_start_time, trade_end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrade_calendar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_step_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrade_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     pred_start_time, pred_end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrade_calendar\u001b[38;5;241m.\u001b[39mget_step_time(trade_step, shift\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    124\u001b[0m     pred_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignal\u001b[38;5;241m.\u001b[39mget_signal(start_time\u001b[38;5;241m=\u001b[39mpred_start_time, end_time\u001b[38;5;241m=\u001b[39mpred_end_time)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/qlib/backtest/utils.py:117\u001b[0m, in \u001b[0;36mTradeCalendarManager.get_step_time\u001b[0;34m(self, trade_step, shift)\u001b[0m\n\u001b[1;32m    115\u001b[0m trade_step \u001b[38;5;241m=\u001b[39m trade_step \u001b[38;5;241m-\u001b[39m shift\n\u001b[1;32m    116\u001b[0m calendar_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_index \u001b[38;5;241m+\u001b[39m trade_step\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calendar[calendar_index], epsilon_change(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calendar\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcalendar_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4132 is out of bounds for axis 0 with size 4132"
     ]
    }
   ],
   "source": [
    "port_analysis_config[\"strategy\"][\"kwargs\"]['model'] = model # 之前指定的model.fitted = False, 所以需要load_model后的修改。\n",
    "port_analysis_config['backtest'][\"end_time\"] = \"2022-03-29\"\n",
    "par = PortAnaRecord(recorder, port_analysis_config, \"day\")\n",
    "par.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis: Evaluation & Results Analysis, not in Experiment Manager\n",
    "https://qlib.readthedocs.io/en/latest/component/report.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18564:MainThread](2022-04-20 08:47:01,021) WARNING - qlib.BaseExecutor - [executor.py:111] - `common_infra` is not set for <qlib.backtest.executor.SimulatorExecutor object at 0x7fd344163f70>\n",
      "[18564:MainThread](2022-04-20 08:47:01,029) INFO - qlib.backtest caller - [__init__.py:83] - Create new exchange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8752fa703796458d9e363e0fe4f779db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "backtest loop:   0%|          | 0/243 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/qlib/utils/index_data.py:480: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(self.data)\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/qlib/utils/index_data.py:480: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(self.data)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4132 is out of bounds for axis 0 with size 4132",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m executor_obj \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mSimulatorExecutor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mEXECUTOR_CONFIG)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# backtest\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m portfolio_metric_dict, indicator_dict \u001b[38;5;241m=\u001b[39m \u001b[43mbacktest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbacktest_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m analysis_freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39mFreq\u001b[38;5;241m.\u001b[39mparse(FREQ))\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# backtest info\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/qlib/backtest/__init__.py:255\u001b[0m, in \u001b[0;36mbacktest\u001b[0;34m(start_time, end_time, strategy, executor, benchmark, account, exchange_kwargs, pos_type)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m\"\"\"initialize the strategy and executor, then backtest function for the interaction of the outermost strategy and executor in the nested decision execution\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m \n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    245\u001b[0m trade_strategy, trade_executor \u001b[38;5;241m=\u001b[39m get_strategy_executor(\n\u001b[1;32m    246\u001b[0m     start_time,\n\u001b[1;32m    247\u001b[0m     end_time,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m     pos_type\u001b[38;5;241m=\u001b[39mpos_type,\n\u001b[1;32m    254\u001b[0m )\n\u001b[0;32m--> 255\u001b[0m portfolio_metrics, indicator \u001b[38;5;241m=\u001b[39m \u001b[43mbacktest_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrade_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrade_executor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m portfolio_metrics, indicator\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/qlib/backtest/backtest.py:28\u001b[0m, in \u001b[0;36mbacktest_loop\u001b[0;34m(start_time, end_time, trade_strategy, trade_executor)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"backtest function for the interaction of the outermost strategy and executor in the nested decision execution\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03mplease refer to the docs of `collect_data_loop`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    it computes the trading indicator\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m return_value \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _decision \u001b[38;5;129;01min\u001b[39;00m collect_data_loop(start_time, end_time, trade_strategy, trade_executor, return_value):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m return_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mportfolio_metrics\u001b[39m\u001b[38;5;124m\"\u001b[39m), return_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindicator\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/qlib/backtest/backtest.py:65\u001b[0m, in \u001b[0;36mcollect_data_loop\u001b[0;34m(start_time, end_time, trade_strategy, trade_executor, return_value)\u001b[0m\n\u001b[1;32m     63\u001b[0m _execute_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trade_executor\u001b[38;5;241m.\u001b[39mfinished():\n\u001b[0;32m---> 65\u001b[0m     _trade_decision: BaseTradeDecision \u001b[38;5;241m=\u001b[39m \u001b[43mtrade_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_trade_decision\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_execute_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     _execute_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m trade_executor\u001b[38;5;241m.\u001b[39mcollect_data(_trade_decision, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     67\u001b[0m     bar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/qlib/contrib/strategy/signal_strategy.py:122\u001b[0m, in \u001b[0;36mTopkDropoutStrategy.generate_trade_decision\u001b[0;34m(self, execute_result)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_trade_decision\u001b[39m(\u001b[38;5;28mself\u001b[39m, execute_result\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# get the number of trading step finished, trade_step can be [0, 1, 2, ..., trade_len - 1]\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     trade_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrade_calendar\u001b[38;5;241m.\u001b[39mget_trade_step()\n\u001b[0;32m--> 122\u001b[0m     trade_start_time, trade_end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrade_calendar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_step_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrade_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     pred_start_time, pred_end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrade_calendar\u001b[38;5;241m.\u001b[39mget_step_time(trade_step, shift\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    124\u001b[0m     pred_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignal\u001b[38;5;241m.\u001b[39mget_signal(start_time\u001b[38;5;241m=\u001b[39mpred_start_time, end_time\u001b[38;5;241m=\u001b[39mpred_end_time)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/qlib/backtest/utils.py:117\u001b[0m, in \u001b[0;36mTradeCalendarManager.get_step_time\u001b[0;34m(self, trade_step, shift)\u001b[0m\n\u001b[1;32m    115\u001b[0m trade_step \u001b[38;5;241m=\u001b[39m trade_step \u001b[38;5;241m-\u001b[39m shift\n\u001b[1;32m    116\u001b[0m calendar_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_index \u001b[38;5;241m+\u001b[39m trade_step\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calendar[calendar_index], epsilon_change(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calendar\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcalendar_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4132 is out of bounds for axis 0 with size 4132"
     ]
    }
   ],
   "source": [
    "from qlib.backtest import backtest, executor\n",
    "from qlib.contrib.evaluate import risk_analysis\n",
    "from qlib.contrib.strategy import TopkDropoutStrategy\n",
    "import qlib.contrib.report as qcr\n",
    "from qlib.utils.time import Freq\n",
    "from qlib.utils import flatten_dict\n",
    "\n",
    "pred_score = par.load(\"pred.pkl\")\n",
    "\n",
    "CSI300_BENCH = \"SH000300\"\n",
    "FREQ = \"day\"\n",
    "STRATEGY_CONFIG = {\n",
    "    \"topk\": 50,\n",
    "    \"n_drop\": 5,\n",
    "    # pred_score, pd.Series\n",
    "    \"signal\": pred_score,\n",
    "}\n",
    "\n",
    "EXECUTOR_CONFIG = {\n",
    "    \"time_per_step\": \"day\",\n",
    "    \"generate_portfolio_metrics\": True,\n",
    "}\n",
    "\n",
    "backtest_config = {\n",
    "    \"start_time\": \"2021-01-01\",\n",
    "    \"end_time\": \"2021-12-31\",\n",
    "    \"account\": 100000000,\n",
    "    \"benchmark\": CSI300_BENCH,\n",
    "    \"exchange_kwargs\": {\n",
    "        \"freq\": FREQ,\n",
    "        \"limit_threshold\": 0.095,\n",
    "        \"deal_price\": \"close\",\n",
    "        \"open_cost\": 0.0005,\n",
    "        \"close_cost\": 0.0015,\n",
    "        \"min_cost\": 5,\n",
    "    },\n",
    "}\n",
    "\n",
    "# strategy object\n",
    "strategy_obj = TopkDropoutStrategy(**STRATEGY_CONFIG)\n",
    "# executor object\n",
    "executor_obj = executor.SimulatorExecutor(**EXECUTOR_CONFIG)\n",
    "# backtest\n",
    "portfolio_metric_dict, indicator_dict = backtest(executor=executor_obj, strategy=strategy_obj, **backtest_config)\n",
    "analysis_freq = \"{0}{1}\".format(*Freq.parse(FREQ))\n",
    "# backtest info\n",
    "report_normal_df, positions_normal = portfolio_metric_dict.get(analysis_freq)\n",
    "\n",
    "qcr.analysis_position.report_graph(report_normal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18564:MainThread](2022-04-20 08:47:23,420) INFO - qlib.backtest caller - [__init__.py:83] - Create new exchange\n"
     ]
    }
   ],
   "source": [
    "# portfolio_metric_dict, indicator_dict = backtest()\n",
    "from qlib.backtest import get_strategy_executor, backtest_loop\n",
    "trade_strategy, trade_executor = get_strategy_executor(\n",
    "        executor=executor_obj, strategy=strategy_obj, **backtest_config\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy object at 0x7fd5d440a340> <qlib.backtest.executor.SimulatorExecutor object at 0x7fd344163f70>\n"
     ]
    }
   ],
   "source": [
    "print(trade_strategy, trade_executor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-04 00:00:00 2021-01-04 23:59:59 0 3889\n",
      "2021-01-05 00:00:00 2021-01-05 23:59:59 1 3890\n",
      "2021-01-06 00:00:00 2021-01-06 23:59:59 2 3891\n",
      "2021-01-07 00:00:00 2021-01-07 23:59:59 3 3892\n",
      "2021-01-08 00:00:00 2021-01-10 23:59:59 4 3893\n",
      "2021-01-11 00:00:00 2021-01-11 23:59:59 5 3894\n",
      "2021-01-12 00:00:00 2021-01-12 23:59:59 6 3895\n",
      "2021-01-13 00:00:00 2021-01-13 23:59:59 7 3896\n",
      "2021-01-14 00:00:00 2021-01-14 23:59:59 8 3897\n",
      "2021-01-15 00:00:00 2021-01-17 23:59:59 9 3898\n",
      "2021-01-18 00:00:00 2021-01-18 23:59:59 10 3899\n",
      "2021-01-19 00:00:00 2021-01-19 23:59:59 11 3900\n",
      "2021-01-20 00:00:00 2021-01-20 23:59:59 12 3901\n",
      "2021-01-21 00:00:00 2021-01-21 23:59:59 13 3902\n",
      "2021-01-22 00:00:00 2021-01-24 23:59:59 14 3903\n",
      "2021-01-25 00:00:00 2021-01-25 23:59:59 15 3904\n",
      "2021-01-26 00:00:00 2021-01-26 23:59:59 16 3905\n",
      "2021-01-27 00:00:00 2021-01-27 23:59:59 17 3906\n",
      "2021-01-28 00:00:00 2021-01-28 23:59:59 18 3907\n",
      "2021-01-29 00:00:00 2021-01-31 23:59:59 19 3908\n",
      "2021-02-01 00:00:00 2021-02-01 23:59:59 20 3909\n",
      "2021-02-02 00:00:00 2021-02-02 23:59:59 21 3910\n",
      "2021-02-03 00:00:00 2021-02-03 23:59:59 22 3911\n",
      "2021-02-04 00:00:00 2021-02-04 23:59:59 23 3912\n",
      "2021-02-05 00:00:00 2021-02-07 23:59:59 24 3913\n",
      "2021-02-08 00:00:00 2021-02-08 23:59:59 25 3914\n",
      "2021-02-09 00:00:00 2021-02-09 23:59:59 26 3915\n",
      "2021-02-10 00:00:00 2021-02-17 23:59:59 27 3916\n",
      "2021-02-18 00:00:00 2021-02-18 23:59:59 28 3917\n",
      "2021-02-19 00:00:00 2021-02-21 23:59:59 29 3918\n",
      "2021-02-22 00:00:00 2021-02-22 23:59:59 30 3919\n",
      "2021-02-23 00:00:00 2021-02-23 23:59:59 31 3920\n",
      "2021-02-24 00:00:00 2021-02-24 23:59:59 32 3921\n",
      "2021-02-25 00:00:00 2021-02-25 23:59:59 33 3922\n",
      "2021-02-26 00:00:00 2021-02-28 23:59:59 34 3923\n",
      "2021-03-01 00:00:00 2021-03-01 23:59:59 35 3924\n",
      "2021-03-02 00:00:00 2021-03-02 23:59:59 36 3925\n",
      "2021-03-03 00:00:00 2021-03-03 23:59:59 37 3926\n",
      "2021-03-04 00:00:00 2021-03-04 23:59:59 38 3927\n",
      "2021-03-05 00:00:00 2021-03-07 23:59:59 39 3928\n",
      "2021-03-08 00:00:00 2021-03-08 23:59:59 40 3929\n",
      "2021-03-09 00:00:00 2021-03-09 23:59:59 41 3930\n",
      "2021-03-10 00:00:00 2021-03-10 23:59:59 42 3931\n",
      "2021-03-11 00:00:00 2021-03-11 23:59:59 43 3932\n",
      "2021-03-12 00:00:00 2021-03-14 23:59:59 44 3933\n",
      "2021-03-15 00:00:00 2021-03-15 23:59:59 45 3934\n",
      "2021-03-16 00:00:00 2021-03-16 23:59:59 46 3935\n",
      "2021-03-17 00:00:00 2021-03-17 23:59:59 47 3936\n",
      "2021-03-18 00:00:00 2021-03-18 23:59:59 48 3937\n",
      "2021-03-19 00:00:00 2021-03-21 23:59:59 49 3938\n",
      "2021-03-22 00:00:00 2021-03-22 23:59:59 50 3939\n",
      "2021-03-23 00:00:00 2021-03-23 23:59:59 51 3940\n",
      "2021-03-24 00:00:00 2021-03-24 23:59:59 52 3941\n",
      "2021-03-25 00:00:00 2021-03-25 23:59:59 53 3942\n",
      "2021-03-26 00:00:00 2021-03-28 23:59:59 54 3943\n",
      "2021-03-29 00:00:00 2021-03-29 23:59:59 55 3944\n",
      "2021-03-30 00:00:00 2021-03-30 23:59:59 56 3945\n",
      "2021-03-31 00:00:00 2021-03-31 23:59:59 57 3946\n",
      "2021-04-01 00:00:00 2021-04-01 23:59:59 58 3947\n",
      "2021-04-02 00:00:00 2021-04-05 23:59:59 59 3948\n",
      "2021-04-06 00:00:00 2021-04-06 23:59:59 60 3949\n",
      "2021-04-07 00:00:00 2021-04-07 23:59:59 61 3950\n",
      "2021-04-08 00:00:00 2021-04-08 23:59:59 62 3951\n",
      "2021-04-09 00:00:00 2021-04-11 23:59:59 63 3952\n",
      "2021-04-12 00:00:00 2021-04-12 23:59:59 64 3953\n",
      "2021-04-13 00:00:00 2021-04-13 23:59:59 65 3954\n",
      "2021-04-14 00:00:00 2021-04-14 23:59:59 66 3955\n",
      "2021-04-15 00:00:00 2021-04-15 23:59:59 67 3956\n",
      "2021-04-16 00:00:00 2021-04-18 23:59:59 68 3957\n",
      "2021-04-19 00:00:00 2021-04-19 23:59:59 69 3958\n",
      "2021-04-20 00:00:00 2021-04-20 23:59:59 70 3959\n",
      "2021-04-21 00:00:00 2021-04-21 23:59:59 71 3960\n",
      "2021-04-22 00:00:00 2021-04-22 23:59:59 72 3961\n",
      "2021-04-23 00:00:00 2021-04-25 23:59:59 73 3962\n",
      "2021-04-26 00:00:00 2021-04-26 23:59:59 74 3963\n",
      "2021-04-27 00:00:00 2021-04-27 23:59:59 75 3964\n",
      "2021-04-28 00:00:00 2021-04-28 23:59:59 76 3965\n",
      "2021-04-29 00:00:00 2021-04-29 23:59:59 77 3966\n",
      "2021-04-30 00:00:00 2021-05-05 23:59:59 78 3967\n",
      "2021-05-06 00:00:00 2021-05-06 23:59:59 79 3968\n",
      "2021-05-07 00:00:00 2021-05-09 23:59:59 80 3969\n",
      "2021-05-10 00:00:00 2021-05-10 23:59:59 81 3970\n",
      "2021-05-11 00:00:00 2021-05-11 23:59:59 82 3971\n",
      "2021-05-12 00:00:00 2021-05-12 23:59:59 83 3972\n",
      "2021-05-13 00:00:00 2021-05-13 23:59:59 84 3973\n",
      "2021-05-14 00:00:00 2021-05-16 23:59:59 85 3974\n",
      "2021-05-17 00:00:00 2021-05-17 23:59:59 86 3975\n",
      "2021-05-18 00:00:00 2021-05-18 23:59:59 87 3976\n",
      "2021-05-19 00:00:00 2021-05-19 23:59:59 88 3977\n",
      "2021-05-20 00:00:00 2021-05-20 23:59:59 89 3978\n",
      "2021-05-21 00:00:00 2021-05-23 23:59:59 90 3979\n",
      "2021-05-24 00:00:00 2021-05-24 23:59:59 91 3980\n",
      "2021-05-25 00:00:00 2021-05-25 23:59:59 92 3981\n",
      "2021-05-26 00:00:00 2021-05-26 23:59:59 93 3982\n",
      "2021-05-27 00:00:00 2021-05-27 23:59:59 94 3983\n",
      "2021-05-28 00:00:00 2021-05-30 23:59:59 95 3984\n",
      "2021-05-31 00:00:00 2021-05-31 23:59:59 96 3985\n",
      "2021-06-01 00:00:00 2021-06-01 23:59:59 97 3986\n",
      "2021-06-02 00:00:00 2021-06-02 23:59:59 98 3987\n",
      "2021-06-03 00:00:00 2021-06-03 23:59:59 99 3988\n",
      "2021-06-04 00:00:00 2021-06-06 23:59:59 100 3989\n",
      "2021-06-07 00:00:00 2021-06-07 23:59:59 101 3990\n",
      "2021-06-08 00:00:00 2021-06-08 23:59:59 102 3991\n",
      "2021-06-09 00:00:00 2021-06-09 23:59:59 103 3992\n",
      "2021-06-10 00:00:00 2021-06-10 23:59:59 104 3993\n",
      "2021-06-11 00:00:00 2021-06-14 23:59:59 105 3994\n",
      "2021-06-15 00:00:00 2021-06-15 23:59:59 106 3995\n",
      "2021-06-16 00:00:00 2021-06-16 23:59:59 107 3996\n",
      "2021-06-17 00:00:00 2021-06-17 23:59:59 108 3997\n",
      "2021-06-18 00:00:00 2021-06-20 23:59:59 109 3998\n",
      "2021-06-21 00:00:00 2021-06-21 23:59:59 110 3999\n",
      "2021-06-22 00:00:00 2021-06-22 23:59:59 111 4000\n",
      "2021-06-23 00:00:00 2021-06-23 23:59:59 112 4001\n",
      "2021-06-24 00:00:00 2021-06-24 23:59:59 113 4002\n",
      "2021-06-25 00:00:00 2021-06-27 23:59:59 114 4003\n",
      "2021-06-28 00:00:00 2021-06-28 23:59:59 115 4004\n",
      "2021-06-29 00:00:00 2021-06-29 23:59:59 116 4005\n",
      "2021-06-30 00:00:00 2021-06-30 23:59:59 117 4006\n",
      "2021-07-01 00:00:00 2021-07-01 23:59:59 118 4007\n",
      "2021-07-02 00:00:00 2021-07-04 23:59:59 119 4008\n",
      "2021-07-05 00:00:00 2021-07-05 23:59:59 120 4009\n",
      "2021-07-06 00:00:00 2021-07-06 23:59:59 121 4010\n",
      "2021-07-07 00:00:00 2021-07-07 23:59:59 122 4011\n",
      "2021-07-08 00:00:00 2021-07-08 23:59:59 123 4012\n",
      "2021-07-09 00:00:00 2021-07-11 23:59:59 124 4013\n",
      "2021-07-12 00:00:00 2021-07-12 23:59:59 125 4014\n",
      "2021-07-13 00:00:00 2021-07-13 23:59:59 126 4015\n",
      "2021-07-14 00:00:00 2021-07-14 23:59:59 127 4016\n",
      "2021-07-15 00:00:00 2021-07-15 23:59:59 128 4017\n",
      "2021-07-16 00:00:00 2021-07-18 23:59:59 129 4018\n",
      "2021-07-19 00:00:00 2021-07-19 23:59:59 130 4019\n",
      "2021-07-20 00:00:00 2021-07-20 23:59:59 131 4020\n",
      "2021-07-21 00:00:00 2021-07-21 23:59:59 132 4021\n",
      "2021-07-22 00:00:00 2021-07-22 23:59:59 133 4022\n",
      "2021-07-23 00:00:00 2021-07-25 23:59:59 134 4023\n",
      "2021-07-26 00:00:00 2021-07-26 23:59:59 135 4024\n",
      "2021-07-27 00:00:00 2021-07-27 23:59:59 136 4025\n",
      "2021-07-28 00:00:00 2021-07-28 23:59:59 137 4026\n",
      "2021-07-29 00:00:00 2021-07-29 23:59:59 138 4027\n",
      "2021-07-30 00:00:00 2021-08-01 23:59:59 139 4028\n",
      "2021-08-02 00:00:00 2021-08-02 23:59:59 140 4029\n",
      "2021-08-03 00:00:00 2021-08-03 23:59:59 141 4030\n",
      "2021-08-04 00:00:00 2021-08-04 23:59:59 142 4031\n",
      "2021-08-05 00:00:00 2021-08-05 23:59:59 143 4032\n",
      "2021-08-06 00:00:00 2021-08-08 23:59:59 144 4033\n",
      "2021-08-09 00:00:00 2021-08-09 23:59:59 145 4034\n",
      "2021-08-10 00:00:00 2021-08-10 23:59:59 146 4035\n",
      "2021-08-11 00:00:00 2021-08-11 23:59:59 147 4036\n",
      "2021-08-12 00:00:00 2021-08-12 23:59:59 148 4037\n",
      "2021-08-13 00:00:00 2021-08-15 23:59:59 149 4038\n",
      "2021-08-16 00:00:00 2021-08-16 23:59:59 150 4039\n",
      "2021-08-17 00:00:00 2021-08-17 23:59:59 151 4040\n",
      "2021-08-18 00:00:00 2021-08-18 23:59:59 152 4041\n",
      "2021-08-19 00:00:00 2021-08-19 23:59:59 153 4042\n",
      "2021-08-20 00:00:00 2021-08-22 23:59:59 154 4043\n",
      "2021-08-23 00:00:00 2021-08-23 23:59:59 155 4044\n",
      "2021-08-24 00:00:00 2021-08-24 23:59:59 156 4045\n",
      "2021-08-25 00:00:00 2021-08-25 23:59:59 157 4046\n",
      "2021-08-26 00:00:00 2021-08-26 23:59:59 158 4047\n",
      "2021-08-27 00:00:00 2021-08-29 23:59:59 159 4048\n",
      "2021-08-30 00:00:00 2021-08-30 23:59:59 160 4049\n",
      "2021-08-31 00:00:00 2021-08-31 23:59:59 161 4050\n",
      "2021-09-01 00:00:00 2021-09-01 23:59:59 162 4051\n",
      "2021-09-02 00:00:00 2021-09-02 23:59:59 163 4052\n",
      "2021-09-03 00:00:00 2021-09-05 23:59:59 164 4053\n",
      "2021-09-06 00:00:00 2021-09-06 23:59:59 165 4054\n",
      "2021-09-07 00:00:00 2021-09-07 23:59:59 166 4055\n",
      "2021-09-08 00:00:00 2021-09-08 23:59:59 167 4056\n",
      "2021-09-09 00:00:00 2021-09-09 23:59:59 168 4057\n",
      "2021-09-10 00:00:00 2021-09-12 23:59:59 169 4058\n",
      "2021-09-13 00:00:00 2021-09-13 23:59:59 170 4059\n",
      "2021-09-14 00:00:00 2021-09-14 23:59:59 171 4060\n",
      "2021-09-15 00:00:00 2021-09-15 23:59:59 172 4061\n",
      "2021-09-16 00:00:00 2021-09-16 23:59:59 173 4062\n",
      "2021-09-17 00:00:00 2021-09-21 23:59:59 174 4063\n",
      "2021-09-22 00:00:00 2021-09-22 23:59:59 175 4064\n",
      "2021-09-23 00:00:00 2021-09-23 23:59:59 176 4065\n",
      "2021-09-24 00:00:00 2021-09-26 23:59:59 177 4066\n",
      "2021-09-27 00:00:00 2021-09-27 23:59:59 178 4067\n",
      "2021-09-28 00:00:00 2021-09-28 23:59:59 179 4068\n",
      "2021-09-29 00:00:00 2021-09-29 23:59:59 180 4069\n",
      "2021-09-30 00:00:00 2021-10-07 23:59:59 181 4070\n",
      "2021-10-08 00:00:00 2021-10-10 23:59:59 182 4071\n",
      "2021-10-11 00:00:00 2021-10-11 23:59:59 183 4072\n",
      "2021-10-12 00:00:00 2021-10-12 23:59:59 184 4073\n",
      "2021-10-13 00:00:00 2021-10-13 23:59:59 185 4074\n",
      "2021-10-14 00:00:00 2021-10-14 23:59:59 186 4075\n",
      "2021-10-15 00:00:00 2021-10-17 23:59:59 187 4076\n",
      "2021-10-18 00:00:00 2021-10-18 23:59:59 188 4077\n",
      "2021-10-19 00:00:00 2021-10-19 23:59:59 189 4078\n",
      "2021-10-20 00:00:00 2021-10-20 23:59:59 190 4079\n",
      "2021-10-21 00:00:00 2021-10-21 23:59:59 191 4080\n",
      "2021-10-22 00:00:00 2021-10-24 23:59:59 192 4081\n",
      "2021-10-25 00:00:00 2021-10-25 23:59:59 193 4082\n",
      "2021-10-26 00:00:00 2021-10-26 23:59:59 194 4083\n",
      "2021-10-27 00:00:00 2021-10-27 23:59:59 195 4084\n",
      "2021-10-28 00:00:00 2021-10-28 23:59:59 196 4085\n",
      "2021-10-29 00:00:00 2021-10-31 23:59:59 197 4086\n",
      "2021-11-01 00:00:00 2021-11-01 23:59:59 198 4087\n",
      "2021-11-02 00:00:00 2021-11-02 23:59:59 199 4088\n",
      "2021-11-03 00:00:00 2021-11-03 23:59:59 200 4089\n",
      "2021-11-04 00:00:00 2021-11-04 23:59:59 201 4090\n",
      "2021-11-05 00:00:00 2021-11-07 23:59:59 202 4091\n",
      "2021-11-08 00:00:00 2021-11-08 23:59:59 203 4092\n",
      "2021-11-09 00:00:00 2021-11-09 23:59:59 204 4093\n",
      "2021-11-10 00:00:00 2021-11-10 23:59:59 205 4094\n",
      "2021-11-11 00:00:00 2021-11-11 23:59:59 206 4095\n",
      "2021-11-12 00:00:00 2021-11-14 23:59:59 207 4096\n",
      "2021-11-15 00:00:00 2021-11-15 23:59:59 208 4097\n",
      "2021-11-16 00:00:00 2021-11-16 23:59:59 209 4098\n",
      "2021-11-17 00:00:00 2021-11-17 23:59:59 210 4099\n",
      "2021-11-18 00:00:00 2021-11-18 23:59:59 211 4100\n",
      "2021-11-19 00:00:00 2021-11-21 23:59:59 212 4101\n",
      "2021-11-22 00:00:00 2021-11-22 23:59:59 213 4102\n",
      "2021-11-23 00:00:00 2021-11-23 23:59:59 214 4103\n",
      "2021-11-24 00:00:00 2021-11-24 23:59:59 215 4104\n",
      "2021-11-25 00:00:00 2021-11-25 23:59:59 216 4105\n",
      "2021-11-26 00:00:00 2021-11-28 23:59:59 217 4106\n",
      "2021-11-29 00:00:00 2021-11-29 23:59:59 218 4107\n",
      "2021-11-30 00:00:00 2021-11-30 23:59:59 219 4108\n",
      "2021-12-01 00:00:00 2021-12-01 23:59:59 220 4109\n",
      "2021-12-02 00:00:00 2021-12-02 23:59:59 221 4110\n",
      "2021-12-03 00:00:00 2021-12-05 23:59:59 222 4111\n",
      "2021-12-06 00:00:00 2021-12-06 23:59:59 223 4112\n",
      "2021-12-07 00:00:00 2021-12-07 23:59:59 224 4113\n",
      "2021-12-08 00:00:00 2021-12-08 23:59:59 225 4114\n",
      "2021-12-09 00:00:00 2021-12-09 23:59:59 226 4115\n",
      "2021-12-10 00:00:00 2021-12-12 23:59:59 227 4116\n",
      "2021-12-13 00:00:00 2021-12-13 23:59:59 228 4117\n",
      "2021-12-14 00:00:00 2021-12-14 23:59:59 229 4118\n",
      "2021-12-15 00:00:00 2021-12-15 23:59:59 230 4119\n",
      "2021-12-16 00:00:00 2021-12-16 23:59:59 231 4120\n",
      "2021-12-17 00:00:00 2021-12-19 23:59:59 232 4121\n",
      "2021-12-20 00:00:00 2021-12-20 23:59:59 233 4122\n",
      "2021-12-21 00:00:00 2021-12-21 23:59:59 234 4123\n",
      "2021-12-22 00:00:00 2021-12-22 23:59:59 235 4124\n",
      "2021-12-23 00:00:00 2021-12-23 23:59:59 236 4125\n",
      "2021-12-24 00:00:00 2021-12-26 23:59:59 237 4126\n",
      "2021-12-27 00:00:00 2021-12-27 23:59:59 238 4127\n",
      "2021-12-28 00:00:00 2021-12-28 23:59:59 239 4128\n",
      "2021-12-29 00:00:00 2021-12-29 23:59:59 240 4129\n",
      "2021-12-30 00:00:00 2021-12-30 23:59:59 241 4130\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4132 is out of bounds for axis 0 with size 4132",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m calendar_manager \u001b[38;5;241m=\u001b[39m TradeCalendarManager(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m\"\u001b[39m, start_time, end_time)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(calendar_manager\u001b[38;5;241m.\u001b[39mget_trade_len()):\n\u001b[0;32m---> 14\u001b[0m     trade_start_time, trade_end_time \u001b[38;5;241m=\u001b[39m \u001b[43mcalendar_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_step_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(trade_start_time, trade_end_time, i, calendar_manager\u001b[38;5;241m.\u001b[39mstart_index \u001b[38;5;241m+\u001b[39m i)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/qlib/backtest/utils.py:117\u001b[0m, in \u001b[0;36mTradeCalendarManager.get_step_time\u001b[0;34m(self, trade_step, shift)\u001b[0m\n\u001b[1;32m    115\u001b[0m trade_step \u001b[38;5;241m=\u001b[39m trade_step \u001b[38;5;241m-\u001b[39m shift\n\u001b[1;32m    116\u001b[0m calendar_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_index \u001b[38;5;241m+\u001b[39m trade_step\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calendar[calendar_index], epsilon_change(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calendar\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcalendar_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4132 is out of bounds for axis 0 with size 4132"
     ]
    }
   ],
   "source": [
    "# portfolio_metrics, indicator = backtest_loop(\"2021-01-01\", \"2021-12-31\", trade_strategy, trade_executor)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from qlib.backtest.utils import TradeCalendarManager\n",
    "start_time = \"2021-01-01\"\n",
    "end_time = \"2021-12-31\"\n",
    "# end_time = \"2021-08-01\"\n",
    "# start_time = \"2017-01-01\"\n",
    "# end_time = \"2020-08-01\"\n",
    "calendar_manager = TradeCalendarManager(\"day\", start_time, end_time)\n",
    "\n",
    "\n",
    "for i in range(calendar_manager.get_trade_len()):\n",
    "    trade_start_time, trade_end_time = calendar_manager.get_step_time(i)\n",
    "    print(trade_start_time, trade_end_time, i, calendar_manager.start_index + i)\n",
    "\n",
    "\n",
    "# trade_executor.reset(start_time=start_time, end_time=end_time)\n",
    "# trade_strategy.reset(level_infra=trade_executor.get_level_infra())\n",
    "# print(trade_executor.trade_calendar, trade_executor.trade_calendar.get_trade_len())\n",
    "# print(trade_strategy.trade_calendar, trade_strategy.trade_calendar.get_trade_len())\n",
    "# print(trade_strategy.trade_calendar.freq, trade_strategy.trade_calendar.start_time, trade_strategy.trade_calendar.end_time)\n",
    "# print(trade_strategy.trade_calendar.start_index, trade_strategy.trade_calendar.end_index)\n",
    "\n",
    "# # trade_strategy.trade_calendar.end_index = 4130\n",
    "# for i in range(trade_strategy.trade_calendar.get_trade_len()):\n",
    "#     trade_start_time, trade_end_time = trade_strategy.trade_calendar.get_step_time(i)\n",
    "#     print(trade_start_time, trade_end_time, i, trade_strategy.trade_calendar.start_index + i)\n",
    "\n",
    "\n",
    "# with tqdm(total=trade_executor.trade_calendar.get_trade_len()-1, desc=\"backtest loop\") as bar:\n",
    "# with tqdm(total=trade_strategy.trade_calendar.get_trade_len()-1, desc=\"backtest loop\") as bar:\n",
    "#     print('start one step')\n",
    "#     _execute_result = None\n",
    "#     while not trade_executor.finished():\n",
    "#         print(\"calendar step\", trade_executor.trade_calendar.trade_step, trade_executor.trade_calendar.trade_len, trade_strategy.trade_calendar.get_step_time(trade_executor.trade_calendar.trade_step))\n",
    "#         _trade_decision = trade_strategy.generate_trade_decision(_execute_result)\n",
    "#         # print(\"trade_decision:\", _trade_decision)\n",
    "#         _execute_result = trade_executor.collect_data(_trade_decision, level=0)\n",
    "#         print(\"innner:\", _execute_result)\n",
    "#         trade_executor.trade_calendar.step()\n",
    "#         # for _execute_result in trade_executor.collect_data(_trade_decision, level=0):\n",
    "#         #     print(\"innner:\", _execute_result)\n",
    "#         bar.update(1)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyze graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import qlib\n",
    "import pandas as pd\n",
    "from qlib.utils.time import Freq\n",
    "from qlib.utils import flatten_dict\n",
    "from qlib.contrib.evaluate import backtest_daily\n",
    "from qlib.contrib.evaluate import risk_analysis\n",
    "from qlib.contrib.strategy import TopkDropoutStrategy\n",
    "\n",
    "CSI300_BENCH = \"SH000300\"\n",
    "STRATEGY_CONFIG = {\n",
    "    \"topk\": 50,\n",
    "    \"n_drop\": 5,\n",
    "    # pred_score, pd.Series\n",
    "    \"signal\": pred_score,\n",
    "}\n",
    "\n",
    "\n",
    "strategy_obj = TopkDropoutStrategy(**STRATEGY_CONFIG)\n",
    "report_normal, positions_normal = backtest_daily(\n",
    "    start_time=\"2021-01-01\", end_time=\"2022-04-01\", strategy=strategy_obj\n",
    ")\n",
    "analysis = dict()\n",
    "# default frequency will be daily (i.e. \"day\")\n",
    "analysis[\"excess_return_without_cost\"] = risk_analysis(report_normal[\"return\"] - report_normal[\"bench\"])\n",
    "analysis[\"excess_return_with_cost\"] = risk_analysis(report_normal[\"return\"] - report_normal[\"bench\"] - report_normal[\"cost\"])\n",
    "\n",
    "analysis_df = pd.concat(analysis)  # type: pd.DataFrame\n",
    "pprint(analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qlib.contrib.report import analysis_model, analysis_position\n",
    "from qlib.data import D\n",
    "recorder = R.get_recorder(recorder_id=ba_rid, experiment_name=\"backtest_analysis\")\n",
    "print(recorder)\n",
    "pred_df = recorder.load_object(\"pred.pkl\")\n",
    "report_normal_df = recorder.load_object(\"portfolio_analysis/report_normal_1day.pkl\")\n",
    "positions = recorder.load_object(\"portfolio_analysis/positions_normal_1day.pkl\")\n",
    "analysis_df = recorder.load_object(\"portfolio_analysis/port_analysis_1day.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analysis position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_position.report_graph(report_normal_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### risk analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_position.risk_analysis_graph(analysis_df, report_normal_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = dataset.prepare(\"test\", col_set=\"label\")\n",
    "label_df.columns = ['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### score IC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label = pd.concat([label_df, pred_df], axis=1, sort=True).reindex(label_df.index)\n",
    "analysis_position.score_ic_graph(pred_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_model.model_performance_graph(pred_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "315px",
    "width": "322px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
