{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* add a market index for qlib models\n",
    "* write prediction process for kaggle under qlib framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/code/quant/qlib/scripts/get_data.py\n"
     ]
    }
   ],
   "source": [
    "#  Copyright (c) Microsoft Corporation.\n",
    "#  Licensed under the MIT License.\n",
    "import sys, site\n",
    "from pathlib import Path\n",
    "# scripts_dir = Path.cwd().parent.joinpath(\"scripts\")\n",
    "scripts_dir = Path(\"/mnt/d/code/quant/qlib/scripts\")\n",
    "print(scripts_dir.joinpath(\"get_data.py\"))\n",
    "assert scripts_dir.joinpath(\"get_data.py\").exists()\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.13 (default, Mar 28 2022, 11:38:47) \\n[GCC 7.5.0]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# NOTE #################################\n",
    "#  Please be aware that if colab installs the latest numpy and pyqlib  #\n",
    "#  in this cell, users should RESTART the runtime in order to run the  #\n",
    "#  following cells successfully.                                       #\n",
    "########################################################################\n",
    "\n",
    "try:\n",
    "    import qlib\n",
    "except ImportError:\n",
    "    # install qlib\n",
    "    ! pip install --upgrade numpy\n",
    "    ! pip install pyqlib\n",
    "    # reload\n",
    "    site.main()\n",
    "\n",
    "# scripts_dir = Path.cwd().parent.joinpath(\"scripts\")\n",
    "if not scripts_dir.joinpath(\"get_data.py\").exists():\n",
    "    # download get_data.py script\n",
    "    scripts_dir = Path(\"~/tmp/qlib_code/scripts\").expanduser().resolve()\n",
    "    scripts_dir.mkdir(parents=True, exist_ok=True)\n",
    "    import requests\n",
    "    with requests.get(\"https://raw.githubusercontent.com/microsoft/qlib/main/scripts/get_data.py\") as resp:\n",
    "        with open(scripts_dir.joinpath(\"get_data.py\"), \"wb\") as fp:\n",
    "            fp.write(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qlib\n",
    "import pandas as pd\n",
    "from qlib.constant import REG_CN\n",
    "from qlib.utils import exists_qlib_data, init_instance_by_config\n",
    "from qlib.workflow import R\n",
    "from qlib.workflow.record_temp import SignalRecord, PortAnaRecord\n",
    "from qlib.utils import flatten_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/mnt/d/code/learn/pylearn/qlibs', '/home/jeffye/anaconda3/envs/py38/lib/python38.zip', '/home/jeffye/anaconda3/envs/py38/lib/python3.8', '/home/jeffye/anaconda3/envs/py38/lib/python3.8/lib-dynload', '', '/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages', '/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/wheel-0.37.1-py3.8.egg', '/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/yelib-0.1.0-py3.8.egg', '/mnt/d/code/quant/qlib/scripts']\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"/mnt/d/code/quant/qlib/scripts\")\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dump_bin import DumpDataUpdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_collector.base import BaseCollector, BaseNormalize, BaseRun, Normalize\n",
    "from data_collector.utils import (\n",
    "    deco_retry,\n",
    "    get_calendar_list,\n",
    "    get_hs_stock_symbols,\n",
    "    get_us_stock_symbols,\n",
    "    get_in_stock_symbols,\n",
    "    get_br_stock_symbols,\n",
    "    generate_minutes_calendar_from_daily,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# explore kaggle jpx data\n",
    "* \"start_time\": \"2017-01-04\", \"end_time\": \"2022-02-28\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16816:MainThread](2022-05-23 22:44:47,896) INFO - qlib.Initialization - [config.py:402] - default_conf: client.\n",
      "[16816:MainThread](2022-05-23 22:44:47,901) INFO - qlib.Initialization - [__init__.py:74] - qlib successfully initialized based on client settings.\n",
      "[16816:MainThread](2022-05-23 22:44:47,902) INFO - qlib.Initialization - [__init__.py:76] - data_path={'__DEFAULT_FREQ': PosixPath('/home/jeffye/.qlib/qlib_data/kaggle')}\n"
     ]
    }
   ],
   "source": [
    "# use default data\n",
    "# NOTE: need to download data from remote: python scripts/get_data.py qlib_data_cn --target_dir ~/.qlib/qlib_data/cn_data\n",
    "provider_uri = \"~/.qlib/qlib_data/kaggle\"  # target_dir\n",
    "# if not exists_qlib_data(provider_uri):\n",
    "#     print(f\"Qlib data is not found in {provider_uri}\")\n",
    "#     sys.path.append(str(scripts_dir))\n",
    "#     from get_data import GetData\n",
    "#     GetData().qlib_data(target_dir=provider_uri, region=REG_CN)\n",
    "qlib.init(provider_uri=provider_uri, region=REG_CN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "market = \"all\"\n",
    "benchmark = \"1301\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qlib.data import D\n",
    "# D.calendar(start_time='2010-01-01', end_time='2017-12-31', freq='day')[:2]\n",
    "from qlib.data.filter import ExpressionDFilter\n",
    "expressionDFilter = ExpressionDFilter(rule_expression='$close>100')\n",
    "instruments = D.instruments(market='all', filter_pipe=[expressionDFilter])\n",
    "# D.list_instruments(instruments=instruments, start_time='2017-01-04', end_time='2022-02-28', as_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$close             0\n",
       "$volume            0\n",
       "$factor            0\n",
       "Ref($close, 1)     1\n",
       "Mean($close, 3)    0\n",
       "$high-$low         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruments = ['1301']\n",
    "fields = ['$close', '$volume', '$factor', 'Ref($close, 1)', 'Mean($close, 3)', '$high-$low']\n",
    "f_d = D.features(instruments, fields, start_time='2017-01-04', end_time='2019-02-04', freq='day')\n",
    "f_d.isna().sum()\n",
    "# f_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['$close', '$volume', '$factor', 'Ref($close, 1)', 'Mean($close, 3)',\n",
       "       '$high-$low'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_d['$close'].max()\n",
    "f_d.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-04 00:00:00 2019-02-04 00:00:00\n"
     ]
    }
   ],
   "source": [
    "df = f_d\n",
    "df.index = df.index.get_level_values('datetime')\n",
    "print(df.index.min(), df.index.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define static parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name=\"online_srv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16816:MainThread](2022-05-23 23:13:02,540) INFO - qlib.LSTM - [pytorch_lstm.py:58] - LSTM pytorch version...\n",
      "[16816:MainThread](2022-05-23 23:13:02,543) INFO - qlib.LSTM - [pytorch_lstm.py:75] - LSTM parameters setting:\n",
      "d_feat : 6\n",
      "hidden_size : 64\n",
      "num_layers : 2\n",
      "dropout : 0.1\n",
      "n_epochs : 1\n",
      "lr : 0.001\n",
      "metric : loss\n",
      "batch_size : 800\n",
      "early_stop : 3\n",
      "optimizer : adam\n",
      "loss_type : mse\n",
      "visible_GPU : 1\n",
      "use_GPU : True\n",
      "seed : None\n",
      "[16816:MainThread](2022-05-23 23:13:42,078) INFO - qlib.timer - [log.py:113] - Time cost: 39.495s | Loading data Done\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/numpy/lib/nanfunctions.py:1096: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanmedian1d, axis, a, overwrite_input)\n",
      "[16816:MainThread](2022-05-23 23:15:46,810) INFO - qlib.timer - [log.py:113] - Time cost: 122.347s | RobustZScoreNorm Done\n",
      "[16816:MainThread](2022-05-23 23:15:47,700) INFO - qlib.timer - [log.py:113] - Time cost: 0.888s | Fillna Done\n",
      "[16816:MainThread](2022-05-23 23:15:49,044) INFO - qlib.timer - [log.py:113] - Time cost: 0.681s | DropnaLabel Done\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/qlib/data/dataset/processor.py:352: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[cols] = t\n",
      "[16816:MainThread](2022-05-23 23:15:50,050) INFO - qlib.timer - [log.py:113] - Time cost: 1.005s | CSRankNorm Done\n",
      "[16816:MainThread](2022-05-23 23:15:50,051) INFO - qlib.timer - [log.py:113] - Time cost: 127.971s | fit & process data Done\n",
      "[16816:MainThread](2022-05-23 23:15:50,052) INFO - qlib.timer - [log.py:113] - Time cost: 167.470s | Init data Done\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# train model\n",
    "###################################\n",
    "# '2017-01-04', end_time='2022-02-28'\n",
    "data_handler_config = {\n",
    "    \"start_time\": \"2017-01-04\",\n",
    "    \"end_time\": \"2019-02-04\",\n",
    "    \"fit_start_time\": \"2017-01-04\",\n",
    "    \"fit_end_time\": \"2018-07-31\",\n",
    "    \"instruments\": market,\n",
    "    \"infer_processors\": [\n",
    "      {\n",
    "        \"class\": \"RobustZScoreNorm\",\n",
    "        \"kwargs\": {\n",
    "          \"fields_group\": \"feature\",\n",
    "          \"clip_outlier\": True\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"Fillna\",\n",
    "        \"kwargs\": {\n",
    "          \"fields_group\": \"feature\"\n",
    "        }\n",
    "      }\n",
    "    ],\n",
    "    \"learn_processors\": [\n",
    "      {\n",
    "        \"class\": \"DropnaLabel\"\n",
    "      },\n",
    "      {\n",
    "        \"class\": \"CSRankNorm\",\n",
    "        \"kwargs\": {\n",
    "          \"fields_group\": \"label\"\n",
    "        }\n",
    "      }\n",
    "    ],\n",
    "    \"label\": [\n",
    "      \"Ref($close, -2) / Ref($close, -1) - 1\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "task = {   \n",
    "    \"model\": {\n",
    "        \"class\": \"LSTM\",\n",
    "        \"module_path\": \"qlib.contrib.model.pytorch_lstm\",\n",
    "        \"kwargs\": {\n",
    "            \"d_feat\": 6,\n",
    "            \"hidden_size\": 64,\n",
    "            \"num_layers\": 2,\n",
    "            \"dropout\": 0.1,\n",
    "            \"dec_dropout\": 0.0,\n",
    "            \"n_epochs\": 1,\n",
    "            \"lr\": 1e-3,\n",
    "            \"early_stop\": 3,\n",
    "            \"batch_size\": 800,\n",
    "            \"metric\": \"loss\",\n",
    "            \"loss\": \"mse\",\n",
    "            \"optimizer\": \"adam\",\n",
    "            \"GPU\": 1\n",
    "        },\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"class\": \"DatasetH\",\n",
    "        \"module_path\": \"qlib.data.dataset\",\n",
    "        \"kwargs\": {\n",
    "            \"handler\": {\n",
    "                \"class\": \"Alpha360\",\n",
    "                \"module_path\": \"qlib.contrib.data.handler\",\n",
    "                \"kwargs\": data_handler_config,\n",
    "            },\n",
    "            \"segments\": {\n",
    "                \"train\": (\"2017-01-04\", \"2018-07-31\"),\n",
    "                \"valid\": (\"2018-08-01\", \"2019-02-04\"),\n",
    "                \"test\": (\"2018-08-01\", \"2019-02-04\"),\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# model initiaiton\n",
    "model = init_instance_by_config(task[\"model\"])\n",
    "dataset = init_instance_by_config(task[\"dataset\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'online_srv'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset\n",
    "experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16816:MainThread](2022-05-23 23:15:50,069) INFO - qlib.workflow - [expm.py:315] - <mlflow.tracking.client.MlflowClient object at 0x7f358f5c2d00>\n",
      "[16816:MainThread](2022-05-23 23:15:50,080) WARNING - qlib.workflow - [expm.py:195] - No valid experiment found. Create a new experiment with name online_srv.\n",
      "[16816:MainThread](2022-05-23 23:15:50,109) INFO - qlib.workflow - [exp.py:257] - Experiment 0 starts running ...\n",
      "[16816:MainThread](2022-05-23 23:15:50,777) INFO - qlib.workflow - [recorder.py:293] - Recorder 2d603eaa09454aa48191ae138a237701 starts running under Experiment 0 ...\n",
      "[16816:MainThread](2022-05-23 23:15:54,005) INFO - qlib.LSTM - [pytorch_lstm.py:236] - training...\n",
      "[16816:MainThread](2022-05-23 23:15:54,007) INFO - qlib.LSTM - [pytorch_lstm.py:240] - Epoch0:\n",
      "[16816:MainThread](2022-05-23 23:15:54,008) INFO - qlib.LSTM - [pytorch_lstm.py:241] - training...\n",
      "[16816:MainThread](2022-05-23 23:16:23,440) INFO - qlib.LSTM - [pytorch_lstm.py:243] - evaluating...\n",
      "[16816:MainThread](2022-05-23 23:16:35,892) INFO - qlib.LSTM - [pytorch_lstm.py:246] - train -0.997192, valid -0.997405\n",
      "[16816:MainThread](2022-05-23 23:16:35,895) INFO - qlib.LSTM - [pytorch_lstm.py:261] - best score: -0.997405 @ 0\n",
      "[16816:MainThread](2022-05-23 23:16:49,555) INFO - qlib.timer - [log.py:113] - Time cost: 0.000s | waiting `async_log` Done\n"
     ]
    }
   ],
   "source": [
    "# start exp to train model\n",
    "\n",
    "experiment_id = 'jpx'\n",
    "# experiment_name: Optional[Text] = None,\n",
    "# recorder_id: Optional[Text] = None,\n",
    "\n",
    "with R.start(experiment_name=experiment_name, experiment_id=experiment_id):\n",
    "    R.log_params(**flatten_dict(task))\n",
    "    model.fit(dataset)\n",
    "    R.save_objects(trained_model=model)\n",
    "    rid = R.get_recorder().id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2d603eaa09454aa48191ae138a237701 online_srv\n"
     ]
    }
   ],
   "source": [
    "print(rid, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction, backtest & analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16816:MainThread](2022-05-23 23:16:49,573) INFO - qlib.workflow - [expm.py:315] - <mlflow.tracking.client.MlflowClient object at 0x7f35a09d7d30>\n",
      "[16816:MainThread](2022-05-23 23:16:49,585) WARNING - qlib.workflow - [expm.py:195] - No valid experiment found. Create a new experiment with name backtest1.\n",
      "[16816:MainThread](2022-05-23 23:16:49,644) INFO - qlib.workflow - [exp.py:257] - Experiment 1 starts running ...\n",
      "[16816:MainThread](2022-05-23 23:16:50,061) INFO - qlib.workflow - [recorder.py:293] - Recorder e070ad10f4094c198d6b0c4163383d2e starts running under Experiment 1 ...\n",
      "/home/jeffye/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py:661: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "[16816:MainThread](2022-05-23 23:16:54,963) INFO - qlib.workflow - [record_temp.py:194] - Signal record 'pred.pkl' has been saved as the artifact of the Experiment 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The following are prediction results of the LSTM model.'\n",
      "                          score\n",
      "datetime   instrument          \n",
      "2018-08-01 1301        0.006857\n",
      "           1332        0.014292\n",
      "           1333        0.003809\n",
      "           1376        0.018791\n",
      "           1377        0.015568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16816:MainThread](2022-05-23 23:16:56,461) INFO - qlib.timer - [log.py:113] - Time cost: 0.000s | waiting `async_log` Done\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# prediction, backtest & analysis\n",
    "###################################\n",
    "port_analysis_config = {\n",
    "    \"executor\": {\n",
    "        \"class\": \"SimulatorExecutor\",\n",
    "        \"module_path\": \"qlib.backtest.executor\",\n",
    "        \"kwargs\": {\n",
    "            \"time_per_step\": \"day\",\n",
    "            \"generate_portfolio_metrics\": True,\n",
    "        },\n",
    "    },\n",
    "    \"strategy\": {\n",
    "        \"class\": \"TopkDropoutStrategy\",\n",
    "        \"module_path\": \"qlib.contrib.strategy.signal_strategy\",\n",
    "        \"kwargs\": {\n",
    "            \"model\": model,\n",
    "            \"dataset\": dataset,\n",
    "            \"topk\": 200,\n",
    "            \"n_drop\": 20,\n",
    "        },\n",
    "    },\n",
    "    \"backtest\": {\n",
    "        \"start_time\": \"2021-12-06\",\n",
    "        \"end_time\": \"2022-02-27\",\n",
    "        \"account\": 100000000,\n",
    "        \"benchmark\": benchmark,\n",
    "        \"exchange_kwargs\": {\n",
    "            \"freq\": \"day\",\n",
    "            \"limit_threshold\": 0.095,\n",
    "            \"deal_price\": \"close\",\n",
    "            \"open_cost\": 0.0005,\n",
    "            \"close_cost\": 0.0015,\n",
    "            \"min_cost\": 5,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# backtest and analysis\n",
    "backtest_experiment_name = \"backtest1\"\n",
    "with R.start(experiment_name=backtest_experiment_name, experiment_id='back0'): \n",
    "    recorder = R.get_recorder(recorder_id=rid, experiment_name=experiment_name)\n",
    "    model = recorder.load_object(\"trained_model\")\n",
    "\n",
    "    # prediction\n",
    "    recorder = R.get_recorder()\n",
    "    ba_rid = recorder.id\n",
    "    sr = SignalRecord(model, dataset, recorder)\n",
    "    sr.generate()\n",
    "\n",
    "    # # backtest & analysis\n",
    "    # par = PortAnaRecord(recorder, port_analysis_config, \"day\")\n",
    "    # par.generate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'par' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         raw_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_label(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m: raw_label})\n\u001b[0;32m---> 23\u001b[0m pred_score \u001b[38;5;241m=\u001b[39m \u001b[43mpar\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m pred_score\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'par' is not defined"
     ]
    }
   ],
   "source": [
    "# 模型的predict score，ADD模型： ADDModel.forward() 输出\"excess\"，[\"market\"]，[\"adv_market\"]，[\"adv_excess\"]，\"reconstructed_feature\"\n",
    "# ADD.predict() 取的是“excess”, 成为pred.pkl 中的score.\n",
    "\n",
    "# SignalRecord中实现如下：\n",
    "def generate(self, **kwargs):\n",
    "    # generate prediciton\n",
    "    pred = self.model.predict(self.dataset)\n",
    "    if isinstance(pred, pd.Series):\n",
    "        pred = pred.to_frame(\"score\")\n",
    "    self.save(**{\"pred.pkl\": pred})\n",
    "\n",
    "    logger.info(\n",
    "        f\"Signal record 'pred.pkl' has been saved as the artifact of the Experiment {self.recorder.experiment_id}\"\n",
    "    )\n",
    "    # print out results\n",
    "    pprint(f\"The following are prediction results of the {type(self.model).__name__} model.\")\n",
    "    pprint(pred.head(5))\n",
    "\n",
    "    if isinstance(self.dataset, DatasetH):\n",
    "        raw_label = self.generate_label(self.dataset)\n",
    "        self.save(**{\"label.pkl\": raw_label})\n",
    "\n",
    "pred_score = par.load(\"pred.pkl\")\n",
    "pred_score.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([\"a\", \"b\", \"c\"], name='vals')\n",
    "print(s)\n",
    "s.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# analyze graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import qlib\n",
    "import pandas as pd\n",
    "from qlib.utils.time import Freq\n",
    "from qlib.utils import flatten_dict\n",
    "from qlib.contrib.evaluate import backtest_daily\n",
    "from qlib.contrib.evaluate import risk_analysis\n",
    "from qlib.contrib.strategy import TopkDropoutStrategy\n",
    "\n",
    "CSI300_BENCH = benchmark\n",
    "STRATEGY_CONFIG = {\n",
    "    \"topk\": 50,\n",
    "    \"n_drop\": 5,\n",
    "    # pred_score, pd.Series\n",
    "    \"signal\": pred_score,\n",
    "}\n",
    "\n",
    "\n",
    "strategy_obj = TopkDropoutStrategy(**STRATEGY_CONFIG)\n",
    "report_normal, positions_normal = backtest_daily(\n",
    "    start_time=\"2021-12-06\", end_time=\"2022-02-27\", strategy=strategy_obj\n",
    ")\n",
    "analysis = dict()\n",
    "# default frequency will be daily (i.e. \"day\")\n",
    "analysis[\"excess_return_without_cost\"] = risk_analysis(report_normal[\"return\"] - report_normal[\"bench\"])\n",
    "analysis[\"excess_return_with_cost\"] = risk_analysis(report_normal[\"return\"] - report_normal[\"bench\"] - report_normal[\"cost\"])\n",
    "\n",
    "analysis_df = pd.concat(analysis)  # type: pd.DataFrame\n",
    "pprint(analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report_normal, positions_normal = backtest_daily(\n",
    "#     start_time=\"2017-01-01\", end_time=\"2020-08-01\", strategy=strategy_obj\n",
    "# )\n",
    "from typing import Union\n",
    "from qlib.backtest import backtest\n",
    "from qlib.backtest.executor import SimulatorExecutor\n",
    "\n",
    "start_time = \"2017-01-01\"\n",
    "end_time = \"2020-08-01\"\n",
    "strategy = strategy_obj\n",
    "executor = None\n",
    "account = 1.e8\n",
    "benchmark: str = benchmark\n",
    "exchange_kwargs: dict = None\n",
    "pos_type: str = \"Position\"\n",
    "\n",
    "freq = \"day\"\n",
    "if executor is None:\n",
    "    executor_config = {\n",
    "        \"time_per_step\": freq,\n",
    "        \"generate_portfolio_metrics\": True,\n",
    "    }\n",
    "    executor = SimulatorExecutor(**executor_config)\n",
    "    \n",
    "_exchange_kwargs = {\n",
    "    \"freq\": freq,\n",
    "    \"limit_threshold\": None,\n",
    "    \"deal_price\": None,\n",
    "    \"open_cost\": 0.0005,\n",
    "    \"close_cost\": 0.0015,\n",
    "    \"min_cost\": 5,\n",
    "}\n",
    "# if exchange_kwargs is not None:\n",
    "#     _exchange_kwargs.update(exchange_kwargs)\n",
    "\n",
    "print(account)\n",
    "\n",
    "portfolio_metric_dict, indicator_dict = backtest(\n",
    "    start_time=start_time,\n",
    "    end_time=end_time,\n",
    "    strategy=strategy,\n",
    "    executor=executor,\n",
    "    account=account,\n",
    "    benchmark=benchmark,\n",
    "    exchange_kwargs=_exchange_kwargs,\n",
    "    pos_type=pos_type,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# analysis_freq = \"{0}{1}\".format(*Freq.parse(freq))\n",
    "\n",
    "# report_normal, positions_normal = portfolio_metric_dict.get(analysis_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy.signal.signal_cache.loc['2017-01-10']\n",
    "# pred_score.loc['2017-01-04']\n",
    "print('position type', type(strategy.common_infra.get(\"trade_account\").current_position))\n",
    "position_dict = strategy.common_infra.get(\"trade_account\").current_position.position\n",
    "\n",
    "\n",
    "print(len(position_dict), position_dict)  # 默认维护52只股票 + cash + now_account_value\n",
    "# 单只股票信息：\n",
    "# 'SZ000001': {'amount': 392925.71123170934, 'price': 8.12631893157959, 'weight': 0.016886521190966564, 'count_day': 55}\n",
    "\n",
    "# executor.track_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portfolio_metric_dict --> 计算在account类中, def _update_state_from_order(self, order, trade_val, cost, trade_price) --》AccumulatedInfo()\n",
    "# report 中也记录了turnover,\n",
    "# turnover 定义：https://www.investopedia.com/terms/p/portfolioturnover.asp\n",
    "\n",
    "# type(portfolio_metric_dict)\n",
    "executor.trade_account.accum_info.to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# portfolio_metric_dict 的具体计算方法见：account.update_portfolio_metrics, 计算依据position 类中记录的信息qlib.backtest.position.py\n",
    "# SimulatorExecutor 中维护（生成）： 1. self.level_infra = LevelInfrastructure() -->def reset_common_infra L117 生成trade_account --> \n",
    "# account中维护position \n",
    "\n",
    "# portfolio_metric_dict\n",
    "\n",
    "# executor 和 strategy中使用/维护的是同一个account 和 同一个common_infra\n",
    "print(executor.trade_account)\n",
    "strategy.common_infra.get(\"trade_account\")\n",
    "print(strategy.common_infra, executor.common_infra)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicator_dict 主要也在account中计算。\n",
    "# ffr: order fulfill rate,  pa: order price advantage, pos:positive_rate 更多参考 BaseOrderIndicator\n",
    "# class Indicator:\n",
    "#     \"\"\"\n",
    "#     `Indicator` is implemented in a aggregate way.\n",
    "#     All the metrics are calculated aggregately.\n",
    "#     All the metrics are calculated for a separated stock and in a specific step on a specific level.\n",
    "\n",
    "#     | indicator    | desc.                                                        |\n",
    "#     |--------------+--------------------------------------------------------------|\n",
    "#     | amount       | the *target* amount given by the outer strategy              |\n",
    "#     | deal_amount  | the real deal amount                                         |\n",
    "#     | inner_amount | the total *target* amount of inner strategy                  |\n",
    "#     | trade_price  | the average deal price                                       |\n",
    "#     | trade_value  | the total trade value                                        |\n",
    "#     | trade_cost   | the total trade cost  (base price need drection)             |\n",
    "#     | trade_dir    | the trading direction                                        |\n",
    "#     | ffr          | full fill rate                                               |\n",
    "#     | pa           | price advantage                                              |\n",
    "#     | pos          | win rate                                                     |\n",
    "#     | base_price   | the price of baseline                                        |\n",
    "#     | base_volume  | the volume of baseline (for weighted aggregating base_price) |\n",
    "\n",
    "#     **NOTE**:\n",
    "#     The `base_price` and `base_volume` can't be NaN when there are not trading on that step. Otherwise\n",
    "#     aggregating get wrong results.\n",
    "\n",
    "#     So `base_price` will not be calculated in a aggregate way!!\n",
    "\n",
    "#     \"\"\"\n",
    "indicator_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qlib.contrib.report import analysis_model, analysis_position\n",
    "from qlib.data import D\n",
    "recorder = R.get_recorder(recorder_id=ba_rid, experiment_name=\"backtest_analysis\")\n",
    "print(recorder)\n",
    "pred_df = recorder.load_object(\"pred.pkl\")\n",
    "report_normal_df = recorder.load_object(\"portfolio_analysis/report_normal_1day.pkl\")\n",
    "positions = recorder.load_object(\"portfolio_analysis/positions_normal_1day.pkl\")\n",
    "analysis_df = recorder.load_object(\"portfolio_analysis/port_analysis_1day.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analysis position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_position.report_graph(report_normal_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### risk analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_position.risk_analysis_graph(analysis_df, report_normal_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = dataset.prepare(\"test\", col_set=\"label\")\n",
    "label_df.columns = ['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### score IC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label = pd.concat([label_df, pred_df], axis=1, sort=True).reindex(label_df.index)\n",
    "analysis_position.score_ic_graph(pred_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_model.model_performance_graph(pred_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jpx prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: compiletime version 3.7 of module 'jpx_tokyo_market_prediction.competition' does not match runtime version 3.8\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/d/dataset/quant/kaggle22/\")\n",
    "\n",
    "import jpx_tokyo_market_prediction\n",
    "env = jpx_tokyo_market_prediction.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n"
     ]
    }
   ],
   "source": [
    "prices, options, financials, trades, secondary_prices, sample_prediction = next(iter_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> 2021-12-06 2021-12-06\n"
     ]
    }
   ],
   "source": [
    "prices\n",
    "start_date = prices['Date'].min()\n",
    "end_date = prices['Date'].max()\n",
    "print(type(start_date), start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import copy\n",
    "\n",
    "# save price to source_dir, to prepare the normalized data\n",
    "\n",
    "source_dir = Path('~/kaggle_jpx/source').expanduser().resolve()\n",
    "source_dir.mkdir(parents=True, exist_ok=True)\n",
    "norm_dir = Path('~/kaggle_jpx/normalized').expanduser().resolve()\n",
    "norm_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_instrument(symbol, df: pd.DataFrame, save_dir, symbol_field='symbol'):\n",
    "    \"\"\"save instrument data to file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    symbol: str\n",
    "        instrument code\n",
    "    df : pd.DataFrame\n",
    "        df.columns must contain \"symbol\" and \"datetime\"\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        logger.warning(f\"{symbol} is empty\")\n",
    "        return\n",
    "\n",
    "    instrument_path = save_dir.joinpath(f\"{symbol}.csv\")\n",
    "    df[symbol_field] = symbol\n",
    "    if instrument_path.exists():\n",
    "        _old_df = pd.read_csv(instrument_path)\n",
    "        df = pd.concat([_old_df, df], sort=False)\n",
    "    df.to_csv(instrument_path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "df = prices.rename(columns={\"SecuritiesCode\": \"instrument\", \"Open\": \"open\", \"High\": \"high\", \"Low\": \"low\", \"Close\": \"close\", \"Volume\": \"volume\", \"AdjustmentFactor\": \"factor\", \"Date\": \"date\"})\n",
    "for sc, group in df.groupby(['instrument']):\n",
    "    _path = source_dir.joinpath(f\"{sc}.csv\")\n",
    "    ret_value = group\n",
    "    if _path.exists():\n",
    "        _old_df = pd.read_csv(_path)\n",
    "        ret_value = pd.concat([_old_df, group], sort=False)\n",
    "        ret_value.drop_duplicates(subset='date', keep=\"last\", inplace=True)\n",
    "    ret_value.to_csv(_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16816:MainThread](2022-05-24 20:17:09,148) INFO - qlib.Initialization - [config.py:402] - default_conf: client.\n",
      "[16816:MainThread](2022-05-24 20:17:09,151) INFO - qlib.Initialization - [__init__.py:74] - qlib successfully initialized based on client settings.\n",
      "[16816:MainThread](2022-05-24 20:17:09,153) INFO - qlib.Initialization - [__init__.py:76] - data_path={'__DEFAULT_FREQ': PosixPath('/home/jeffye/.qlib/qlib_data/kaggle')}\n",
      "[16816:MainThread](2022-05-24 20:17:09,167) INFO - qlib.Initialization - [config.py:402] - default_conf: client.\n",
      "[16816:MainThread](2022-05-24 20:17:09,169) INFO - qlib.Initialization - [__init__.py:74] - qlib successfully initialized based on client settings.\n",
      "[16816:MainThread](2022-05-24 20:17:09,170) INFO - qlib.Initialization - [__init__.py:76] - data_path={'__DEFAULT_FREQ': PosixPath('/home/jeffye/.qlib/qlib_data/kaggle')}\n"
     ]
    }
   ],
   "source": [
    "## 自动重新加载更改的模块\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# normalize_class_name f\"YahooNormalize{self.region.upper()}{self.interval}\" -->YahooNormalizeCN1d\n",
    "sys.path.append(\"/mnt/d/code/quant/qlib/scripts/data_collector/yahoo\")  # in qlib source\n",
    "from typing import Iterable\n",
    "from loguru import logger\n",
    "from data_collector.yahoo.collector import YahooNormalizeCN, YahooNormalize1d, YahooNormalize, YahooNormalize1dExtend, YahooNormalizeCN1dExtend\n",
    "\n",
    "qlib_data_1d_dir = \"~/.qlib/qlib_data/kaggle\"\n",
    "qlib_data_1d_dir = str(Path(qlib_data_1d_dir).expanduser().resolve())\n",
    "\n",
    "class YahooNormalizeCN1doffline(YahooNormalizeCN, YahooNormalize1d):\n",
    "    def _get_calendar_list(self) -> Iterable[pd.Timestamp]:\n",
    "\n",
    "        import qlib\n",
    "        from qlib.data import D\n",
    "\n",
    "        qlib.init(provider_uri=qlib_data_1d_dir)\n",
    "        return list(D.calendar(freq=\"day\"))\n",
    "    \n",
    "    def normalize(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # normalize\n",
    "        df = self.normalize_yahoo(df, self._calendar_list, self._date_field_name, self._symbol_field_name)\n",
    "        print('normalize_yahoo', df)\n",
    "        # adjusted price\n",
    "        df = self.adjusted_price(df)\n",
    "        print('adjusted_price', df)\n",
    "        \n",
    "        df = df.drop(columns=['RowId', 'ExpectedDividend', 'SupervisionFlag'])\n",
    "        # print(df)\n",
    "        df = self._manual_adj_data(df)\n",
    "        print('_manual_adj_data', df)\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_yahoo(\n",
    "        df: pd.DataFrame,\n",
    "        calendar_list: list = None,\n",
    "        date_field_name: str = \"date\",\n",
    "        symbol_field_name: str = \"symbol\",\n",
    "        last_close: float = None,\n",
    "    ):\n",
    "        input('normalize_yahoo')\n",
    "        if df.empty:\n",
    "            return df\n",
    "        symbol = df.loc[df[symbol_field_name].first_valid_index(), symbol_field_name]\n",
    "        columns = copy.deepcopy(YahooNormalize.COLUMNS)\n",
    "        df = df.copy()\n",
    "        df.set_index(date_field_name, inplace=True)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df = df[~df.index.duplicated(keep=\"first\")]\n",
    "        \n",
    "        # print(symbol, df)\n",
    "        \n",
    "        if calendar_list is not None:\n",
    "            df = df.reindex(\n",
    "                pd.DataFrame(index=calendar_list)\n",
    "                .loc[\n",
    "                    pd.Timestamp(df.index.min()).date() : pd.Timestamp(df.index.max()).date()\n",
    "                    + pd.Timedelta(hours=23, minutes=59)\n",
    "                ]\n",
    "                .index\n",
    "            )\n",
    "            # print('after reindex', df)\n",
    "            # input(\"please input:\")\n",
    "        df.sort_index(inplace=True)\n",
    "        df.loc[(df[\"volume\"] <= 0) | np.isnan(df[\"volume\"]), list(set(df.columns) - {symbol_field_name})] = np.nan\n",
    "\n",
    "        change_series = YahooNormalize.calc_change(df, last_close)\n",
    "        # NOTE: The data obtained by Yahoo finance sometimes has exceptions\n",
    "        # WARNING: If it is normal for a `symbol(exchange)` to differ by a factor of *89* to *111* for consecutive trading days,\n",
    "        # WARNING: the logic in the following line needs to be modified\n",
    "        _count = 0\n",
    "        while True:\n",
    "            # NOTE: may appear unusual for many days in a row\n",
    "            change_series = YahooNormalize.calc_change(df, last_close)\n",
    "            _mask = (change_series >= 89) & (change_series <= 111)\n",
    "            if not _mask.any():\n",
    "                break\n",
    "            _tmp_cols = [\"high\", \"close\", \"low\", \"open\", \"adjclose\"]\n",
    "            df.loc[_mask, _tmp_cols] = df.loc[_mask, _tmp_cols] / 100\n",
    "            _count += 1\n",
    "            if _count >= 10:\n",
    "                _symbol = df.loc[df[symbol_field_name].first_valid_index()][\"symbol\"]\n",
    "                logger.warning(\n",
    "                    f\"{_symbol} `change` is abnormal for {_count} consecutive days, please check the specific data file carefully\"\n",
    "                )\n",
    "\n",
    "        df[\"change\"] = YahooNormalize.calc_change(df, last_close)\n",
    "\n",
    "        columns += [\"change\"]\n",
    "        df.loc[(df[\"volume\"] <= 0) | np.isnan(df[\"volume\"]), columns] = np.nan\n",
    "\n",
    "        df[symbol_field_name] = symbol\n",
    "        df.index.names = [date_field_name]\n",
    "        return df.reset_index()\n",
    "    \n",
    "\n",
    "class YahooNormalize1dExtendFixed(YahooNormalizeCN, YahooNormalize1dExtend):\n",
    "    def _get_calendar_list(self) -> Iterable[pd.Timestamp]:\n",
    "        import qlib\n",
    "        from qlib.data import D\n",
    "\n",
    "        qlib.init(provider_uri=qlib_data_1d_dir)\n",
    "        \n",
    "        days_list = list(D.calendar(freq=\"day\"))\n",
    "        last_day = days_list[-1]\n",
    "        i = 1\n",
    "        while True:\n",
    "            day = last_day + pd.Timedelta(days=i)\n",
    "            if day < pd.Timestamp.now():\n",
    "                # print(day)\n",
    "                days_list.append(day)\n",
    "            else:\n",
    "                break\n",
    "            i += 1\n",
    "        \n",
    "        return days_list\n",
    "    \n",
    "    def _get_old_data(self, qlib_data_dir: [str, Path]):\n",
    "        import qlib\n",
    "        from qlib.data import D\n",
    "\n",
    "        qlib_data_dir = str(Path(qlib_data_dir).expanduser().resolve())\n",
    "        qlib.init(provider_uri=qlib_data_dir, expression_cache=None, dataset_cache=None)\n",
    "        df = D.features(D.instruments(\"all\"), [\"$close/$factor\", \"$adjclose/$close\"])\n",
    "        df.columns = [self._ori_close_field, self._first_close_field]\n",
    "        # print(df.columns)\n",
    "        # input(\"print columns\")\n",
    "        # df = df.astype({self._symbol_field_name: str,})\n",
    "        return df\n",
    "    \n",
    "#     def normalize(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "#         df = df.astype({self._symbol_field_name: str,})\n",
    "        \n",
    "#         _last_close = self._get_last_close(df)\n",
    "#         # reindex\n",
    "#         _last_date = self._get_last_date(df)\n",
    "        \n",
    "        \n",
    "#         df = self.normalize_yahoo(\n",
    "#             df, self._calendar_list, self._date_field_name, self._symbol_field_name, last_close=_last_close\n",
    "#         )\n",
    "#         # adjusted price\n",
    "#         df = self.adjusted_price(df)\n",
    "#         df = self._manual_adj_data(df)\n",
    "#         return df\n",
    "        \n",
    "    def print_all(self):\n",
    "        print(input)\n",
    "        print('iam here')\n",
    "        input('still here')\n",
    "        \n",
    "    def normalize(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print('input\\n', df)\n",
    "        df = df.astype({self._symbol_field_name: str,})\n",
    "        _last_close = self._get_last_close(df)\n",
    "        # reindex\n",
    "        _last_date = self._get_last_date(df)\n",
    "        print('_last_date', _last_date, _last_close, self._date_field_name)\n",
    "        if _last_date is not None:\n",
    "            df = df.set_index(self._date_field_name)\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "            df = df[~df.index.duplicated(keep=\"first\")]\n",
    "            _max_date = df.index.max()\n",
    "            df = df.reindex(self._calendar_list).loc[:_max_date].reset_index()\n",
    "            df = df[df[self._date_field_name] > _last_date]\n",
    "            if df.empty:\n",
    "                return pd.DataFrame()\n",
    "            _si = df[\"close\"].first_valid_index()\n",
    "            if _si > df.index[0]:\n",
    "                logger.warning(\n",
    "                    f\"{df.loc[_si][self._symbol_field_name]} missing data: {df.loc[:_si - 1][self._date_field_name].to_list()}\"\n",
    "                )\n",
    "        \n",
    "        logger.info(f'1. df={df}')\n",
    "        # normalize\n",
    "        df = self.normalize_yahoo(\n",
    "            df, self._calendar_list, self._date_field_name, self._symbol_field_name, last_close=_last_close\n",
    "        )\n",
    "        \n",
    "        logger.info(f'2. df={df}')\n",
    "        # adjusted price\n",
    "        df = self.adjusted_price(df)\n",
    "        df = self._manual_adj_data(df)\n",
    "        logger.info(f'3 df={df}')\n",
    "        return df\n",
    "        \n",
    "    def _manual_adj_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"manual adjust data: All fields (except change) are standardized according to the close of the first day\"\"\"\n",
    "        if df.empty:\n",
    "            return df\n",
    "        df = df.copy().drop(columns=[\"RowId\", \"ExpectedDividend\", \"SupervisionFlag\"])\n",
    "        df.sort_values(self._date_field_name, inplace=True)\n",
    "        df = df.set_index(self._date_field_name)\n",
    "        _close = self._get_first_close(df)\n",
    "        for _col in df.columns:\n",
    "            # NOTE: retain original adjclose, required for incremental updates\n",
    "            if _col in [self._symbol_field_name, \"adjclose\", \"change\"]:\n",
    "                continue\n",
    "            if _col == \"volume\":\n",
    "                df[_col] = df[_col] * _close\n",
    "            else:\n",
    "                df[_col] = df[_col] / _close\n",
    "        return df.reset_index()\n",
    "    \n",
    "kwargs = {'qlib_data_1d_dir': qlib_data_1d_dir, 'old_qlib_data_dir': qlib_data_1d_dir}\n",
    "\n",
    "yc = Normalize(\n",
    "            source_dir=source_dir,\n",
    "            target_dir=norm_dir,\n",
    "            # normalize_class=YahooNormalizeCN1doffline,\n",
    "            normalize_class=YahooNormalize1dExtendFixed,\n",
    "            max_workers=2,\n",
    "            date_field_name='date',\n",
    "            symbol_field_name=\"instrument\",\n",
    "            **kwargs,\n",
    "        )\n",
    "# yc.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 20:18:38.447 | WARNING  | __main__:normalize:174 - 8022 missing data: [Timestamp('2021-12-04 00:00:00'), Timestamp('2021-12-05 00:00:00')]\n",
      "2022-05-24 20:18:38.452 | INFO     | __main__:normalize:178 - 1. df=           date          RowId instrument    open    high     low   close  \\\n",
      "1202 2021-12-04            NaN        NaN     NaN     NaN     NaN     NaN   \n",
      "1203 2021-12-05            NaN        NaN     NaN     NaN     NaN     NaN   \n",
      "1204 2021-12-06  20211206_8022       8022  2228.0  2236.0  2195.0  2199.0   \n",
      "\n",
      "       volume  factor  ExpectedDividend SupervisionFlag  \n",
      "1202      NaN     NaN               NaN             NaN  \n",
      "1203      NaN     NaN               NaN             NaN  \n",
      "1204  67500.0     1.0               NaN           False  \n",
      "2022-05-24 20:18:38.473 | INFO     | __main__:normalize:184 - 2. df=        date          RowId instrument    open    high     low   close  \\\n",
      "0 2021-12-04            NaN       8022     NaN     NaN     NaN     NaN   \n",
      "1 2021-12-05            NaN       8022     NaN     NaN     NaN     NaN   \n",
      "2 2021-12-06  20211206_8022       8022  2228.0  2236.0  2195.0  2199.0   \n",
      "\n",
      "    volume  factor  ExpectedDividend SupervisionFlag  change  \n",
      "0      NaN     NaN               NaN             NaN     NaN  \n",
      "1      NaN     NaN               NaN             NaN     NaN  \n",
      "2  67500.0     1.0               NaN           False     NaN  \n",
      "2022-05-24 20:18:38.494 | INFO     | __main__:normalize:188 - 3 df=        date instrument  open  high  low  close  volume  factor  change\n",
      "0 2021-12-04       8022   NaN   NaN  NaN    NaN     NaN     NaN     NaN\n",
      "1 2021-12-05       8022   NaN   NaN  NaN    NaN     NaN     NaN     NaN\n",
      "2 2021-12-06       8022   NaN   NaN  NaN    NaN     NaN     NaN     NaN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "          date          RowId  instrument    open    high     low   close  \\\n",
      "0  2021-12-06  20211206_8022        8022  2228.0  2236.0  2195.0  2199.0   \n",
      "\n",
      "   volume  factor  ExpectedDividend  SupervisionFlag  \n",
      "0   67500     1.0               NaN            False  \n",
      "_last_date 2021-12-03 00:00:00 2228.0 date\n",
      "        date instrument  open  high  low  close  volume  factor  change\n",
      "0 2021-12-04       8022   NaN   NaN  NaN    NaN     NaN     NaN     NaN\n",
      "1 2021-12-05       8022   NaN   NaN  NaN    NaN     NaN     NaN     NaN\n",
      "2 2021-12-06       8022   NaN   NaN  NaN    NaN     NaN     NaN     NaN\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [122]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# yc._normalize_obj.print_all()\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# import inspect\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# print(yc._normalize_obj)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# lines = inspect.getsource(yc._normalize_obj.normalize)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# print(lines)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39m_end_date \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m         _mask \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_date_field_name]) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mTimestamp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_end_date)\n\u001b[1;32m     18\u001b[0m         df \u001b[38;5;241m=\u001b[39m df[_mask]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "# yc._target_dir\n",
    "file_list = list(yc._source_dir.glob(\"*.csv\"))\n",
    "for file_path in file_list:\n",
    "    # yc._executor(file_path)\n",
    "    # print(yc._normalize_obj)\n",
    "    file_path = Path(file_path)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = yc._normalize_obj.normalize(df)\n",
    "    print(df)\n",
    "    # yc._normalize_obj.print_all()\n",
    "    # import inspect\n",
    "    # print(yc._normalize_obj)\n",
    "    # lines = inspect.getsource(yc._normalize_obj.normalize)\n",
    "    # print(lines)\n",
    "    if df is not None and not df.empty:\n",
    "        if yc._normalize_obj._end_date is not None:\n",
    "            _mask = pd.to_datetime(df[self._date_field_name]) <= pd.Timestamp(self._end_date)\n",
    "            df = df[_mask]\n",
    "        df.to_csv(self._target_dir.joinpath(file_path.name), index=False)\n",
    "        logger.info(f\"writing {file_path.name} csv file {self._target_dir.joinpath(file_path.name)}\")\n",
    "    else:\n",
    "        logger.info(f\"df is empty or none {file_path}, {self._normalize_obj.normalize}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump bin\n",
    "csv_path=norm_dir\n",
    "csv_files = sorted(csv_path.glob(f\"*.csv\") if csv_path.is_dir() else [csv_path])\n",
    "if csv_files:\n",
    "    max_workers = 10\n",
    "    _dump = DumpDataUpdate(\n",
    "        csv_path=norm_dir,\n",
    "        qlib_dir=qlib_data_1d_dir,\n",
    "        exclude_fields=\"instrument,date,ExpectedDividend,SupervisionFlag,RowId\",\n",
    "        max_workers=max_workers,\n",
    "    )\n",
    "    _dump.dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## qlib update_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[24297:MainThread](2022-05-23 21:53:43,885) INFO - qlib.OnlineToolR - [utils.py:178] - Finished updating 0 online model predictions of backtest1.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'5dafd2aadf304aa9bf71861c94c68f99': MLflowRecorder(info={'class': 'Recorder', 'id': '5dafd2aadf304aa9bf71861c94c68f99', 'name': 'mlflow_recorder', 'experiment_id': '2', 'start_time': '2022-05-23 21:51:55', 'end_time': '2022-05-23 21:52:06', 'status': 'FINISHED'},\n",
       "                uri=file:/mnt/d/code/learn/pylearn/qlibs/mlruns,\n",
       "                artifact_uri=None,\n",
       "                client=<mlflow.tracking.client.MlflowClient object at 0x7ff460fe39d0>)}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "import fire\n",
    "import qlib\n",
    "from qlib.constant import REG_CN\n",
    "from qlib.model.trainer import task_train\n",
    "from qlib.workflow.online.utils import OnlineToolR\n",
    "from qlib.tests.config import CSI300_GBDT_TASK\n",
    "\n",
    "task = copy.deepcopy(CSI300_GBDT_TASK)\n",
    "\n",
    "task[\"record\"] = {\n",
    "    \"class\": \"SignalRecord\",\n",
    "    \"module_path\": \"qlib.workflow.record_temp\",\n",
    "}\n",
    "\n",
    "\n",
    "class UpdatePredExample:\n",
    "    def __init__(\n",
    "        self, provider_uri=\"~/.qlib/qlib_data/cn_data\", region=REG_CN, experiment_name=\"online_srv\", task_config=task\n",
    "    ):\n",
    "        qlib.init(provider_uri=provider_uri, region=region)\n",
    "        self.experiment_name = experiment_name\n",
    "        self.online_tool = OnlineToolR(self.experiment_name)\n",
    "        self.task_config = task_config\n",
    "\n",
    "    def first_train(self):\n",
    "        rec = task_train(self.task_config, experiment_name=self.experiment_name)\n",
    "        self.online_tool.reset_online_tag(rec)  # set to online model\n",
    "\n",
    "    def update_online_pred(self):\n",
    "        self.online_tool.update_online_pred()\n",
    "\n",
    "    def main(self):\n",
    "        self.first1_train()\n",
    "        self.update_online_pred()\n",
    "\n",
    "# updater = UpdatePredExample(provider_uri=\"~/.qlib/qlib_data/cn_data\", experiment_name=\"online_srv\")\n",
    "\n",
    "\n",
    "online_tool = OnlineToolR(backtest_experiment_name)\n",
    "online_tool.update_online_pred(to_date=start_date, from_date=end_date, exp_name=backtest_experiment_name) # default update to file, so we have read it for submission.\n",
    "\n",
    "experiment = R.get_exp(experiment_name=backtest_experiment_name)\n",
    "recs = experiment.list_recorders()  # SignalRecord .load(\"pred.pkl\")\n",
    "recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[24297:MainThread](2022-05-23 21:57:41,717) WARNING - qlib.workflow - [exp.py:307] - Please make sure the recorder name mlflow_recorder is unique, we will only return the latest recorder if there exist several matched the given name.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLflowRecorder(info={'class': 'Recorder', 'id': '5dafd2aadf304aa9bf71861c94c68f99', 'name': 'mlflow_recorder', 'experiment_id': '2', 'start_time': '2022-05-23 21:51:55', 'end_time': '2022-05-23 21:52:06', 'status': 'FINISHED'},\n",
       "               uri=file:/mnt/d/code/learn/pylearn/qlibs/mlruns,\n",
       "               artifact_uri=None,\n",
       "               client=<mlflow.tracking.client.MlflowClient object at 0x7ff461004f10>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.get_recorder()\n",
    "# list(recs.values())[0].load_object(\"pred.pkl\")\n",
    "# list(recs.values())[0].end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(list(recs.values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 19:36:32.588 | INFO     | data_collector.utils:get_calendar_list:68 - get calendar list: CSI300......\n",
      "2022-05-24 19:36:32.590 | INFO     | data_collector.utils:get_calendar_list:106 - end of get calendar list: CSI300.\n",
      "2022-05-24 19:36:32.591 | INFO     | data_collector.utils:get_calendar_list:68 - get calendar list: CSI300......\n",
      "2022-05-24 19:36:32.592 | INFO     | data_collector.utils:get_calendar_list:106 - end of get calendar list: CSI300.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4223"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# url used in this function https://www.szse.cn/api/report/exchange/onepersistenthour/monthList?month=2022-01&random=123455\n",
    "\n",
    "from data_collector.utils import get_calendar_list\n",
    "get_calendar_list()[-10:]\n",
    "len(get_calendar_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2022-05-24 19:55:39.209234')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Timestamp.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16816:MainThread](2022-05-24 20:03:07,751) INFO - qlib.Initialization - [config.py:402] - default_conf: client.\n",
      "[16816:MainThread](2022-05-24 20:03:07,754) INFO - qlib.Initialization - [__init__.py:74] - qlib successfully initialized based on client settings.\n",
      "[16816:MainThread](2022-05-24 20:03:07,756) INFO - qlib.Initialization - [__init__.py:76] - data_path={'__DEFAULT_FREQ': PosixPath('/home/jeffye/.qlib/qlib_data/kaggle')}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1202"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import qlib\n",
    "from qlib.data import D\n",
    "# qlib.init(provider_uri=\"~/.qlib/qlib_data/cn_data\")\n",
    "qlib.init(provider_uri=qlib_data_1d_dir)\n",
    "list(D.calendar(freq=\"day\"))[-10:]\n",
    "len(list(D.calendar(freq=\"day\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-03 00:00:00\n"
     ]
    }
   ],
   "source": [
    "last_day = list(D.calendar(freq=\"day\"))[-1]\n",
    "print(last_day)\n",
    "# i = 1\n",
    "# while True:\n",
    "#     day = last_day + pd.Timedelta(days=i)\n",
    "#     if day < pd.Timestamp.now():\n",
    "#         print(day)\n",
    "#     else:\n",
    "#         break\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
