{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# 迁移学习与微调，Transfer learning & fine-tuning\n",
    "\n",
    "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
    "**Date created:** 2020/04/15<br>\n",
    "**Last modified:** 2020/05/12<br>\n",
    "**Description:** Complete guide to transfer learning & fine-tuning in Keras. <br>\n",
    "**翻译：** 叶正"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## 设置，Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## 介绍，Introduction\n",
    "\n",
    "**Transfer learning** consists of taking features learned on one problem, and\n",
    "leveraging them on a new, similar problem. For instance, features from a model that has\n",
    "learned to identify racoons may be useful to kick-start a model meant to identify\n",
    " tanukis.\n",
    " \n",
    " **迁移学习** 就是把从一个问题中学习到的特征使用新的、类似的问题。例如，一个识别浣熊的模型中学到的特征，可能用来初始化识别狸猫的模型中是有用的。\n",
    "\n",
    "Transfer learning is usually done for tasks where your dataset has too little data to\n",
    " train a full-scale model from scratch.\n",
    " \n",
    " 迁移学习通常在当你只有较少的数据难以从头开始训练一个模型的时候使用。\n",
    "\n",
    "The most common incarnation of transfer learning in the context of deep learning is the\n",
    " following workflow:\n",
    " \n",
    " 深度学习中最典型的迁移学习**工作流**如下：\n",
    " \n",
    " 1. 从已经训练好的模型中取出部分层\n",
    " 2. 固定这些层的参数，在后面的训练中不破坏之前学到的信息\n",
    " 3. 在这些固定的层上面，添加一些新的、可以训练的层。这些层将学习如何把之前学到的老的特征变成在新的数据集上的预测。\n",
    " 4. 在新的数据集上，开始训练新的层。\n",
    "\n",
    "1. Take layers from a previously trained model.\n",
    "2. Freeze them, so as to avoid destroying any of the information they contain during\n",
    " future training rounds.\n",
    "3. Add some new, trainable layers on top of the frozen layers. They will learn to turn\n",
    " the old features into predictions on a  new dataset.\n",
    "4. Train the new layers on your dataset.\n",
    "\n",
    "A last, optional step, is **fine-tuning**, which consists of unfreezing the entire\n",
    "model you obtained above (or part of it), and re-training it on the new data with a\n",
    "very low learning rate. This can potentially achieve meaningful improvements, by\n",
    " incrementally adapting the pretrained features to the new data.\n",
    " \n",
    " 最后，也是可选的步骤，就是所谓的微调**fine-tuning**。这里包括unfreezing整个模型（或者部分模型），并且使用非常小的学习率重新训练。通过逐步地在新的数据上调节预训练好的特征，有潜力获得可观的提高。\n",
    "\n",
    "First, we will go over the Keras `trainable` API in detail, which underlies most\n",
    " transfer learning & fine-tuning workflows.\n",
    " \n",
    " 首先，我们将复习keras `trainable` API，这套api是大多数迁移学习和微调工作量的基础。\n",
    "\n",
    "Then, we'll demonstrate the typical workflow by taking a model pretrained on the\n",
    "ImageNet dataset, and retraining it on the Kaggle \"cats vs dogs\" classification\n",
    " dataset.\n",
    " \n",
    " 然后，展示典型的工作量：选取一个从ImageNet 数据集上训练好的模型，然后在Kaggle的\"cats vs dogs\"分类数据集上重新训练\n",
    "\n",
    "该实例从下列例子中修改的来。\n",
    "[Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python)\n",
    " and the 2016 blog post\n",
    "[\"building powerful image classification models using very little\n",
    " data\"](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Freezing layers: understanding the `trainable` attribute\n",
    "## 冻住一些层：理解`trainable`属性\n",
    "\n",
    "Layers & models have three weight attributes:\n",
    "\n",
    "层和模型有三个权重属性：\n",
    "\n",
    "- `weights` ：层所有权重变量的列表 list\n",
    "- `trainable_weights` : 训练过程中，以上中可以更新（通过梯度下降）来最小化损失函数。\n",
    "- `non_trainable_weights`： 以上中不能被训练的固定参数。\n",
    "通常，这些变量在前向传播过程中被更新。\n",
    "\n",
    "- `weights` is the list of all weights variables of the layer.\n",
    "- `trainable_weights` is the list of those that are meant to be updated (via gradient\n",
    " descent) to minimize the loss during training.\n",
    "- `non_trainable_weights` is the list of those that aren't meant to be trained.\n",
    " Typically they are updated by the model during the forward pass.\n",
    "\n",
    "**Example: the `Dense` layer has 2 trainable weights (kernel & bias)**\n",
    "\n",
    "**例子：`Dense`层有2个可训练的权重(kernel & bias)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 2 [<tf.Variable 'kernel:0' shape=(4, 3) dtype=float32, numpy=\n",
      "array([[-0.8190252 ,  0.7117919 ,  0.7209693 ],\n",
      "       [-0.34910846, -0.9135456 ,  0.6951951 ],\n",
      "       [-0.3523373 , -0.455983  ,  0.35899854],\n",
      "       [ 0.1363393 , -0.31095743,  0.7441169 ]], dtype=float32)>, <tf.Variable 'bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]\n",
      "trainable_weights: 2 [<tf.Variable 'kernel:0' shape=(4, 3) dtype=float32, numpy=\n",
      "array([[-0.8190252 ,  0.7117919 ,  0.7209693 ],\n",
      "       [-0.34910846, -0.9135456 ,  0.6951951 ],\n",
      "       [-0.3523373 , -0.455983  ,  0.35899854],\n",
      "       [ 0.1363393 , -0.31095743,  0.7441169 ]], dtype=float32)>, <tf.Variable 'bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]\n",
      "non_trainable_weights: 0\n"
     ]
    }
   ],
   "source": [
    "layer = keras.layers.Dense(3)\n",
    "layer.build((None, 4))  # Create the weights\n",
    "\n",
    "print(\"weights:\", len(layer.weights), layer.weights)\n",
    "print(\"trainable_weights:\", len(layer.trainable_weights),layer.trainable_weights)\n",
    "print(\"non_trainable_weights:\", len(layer.non_trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "通常，所有的权重都是可以训练的权重。keras自动的layer中只有`BatchNormalization`有不可训练的权重。`BatchNormalization`使用不可训练的权重来跟踪训练过程中输入的mean和variance。\n",
    "学习在自定义layers，如何使用不可训练权重，请看\n",
    "[guide to writing new layers from scratch](https://keras.io/guides/making_new_layers_and_models_via_subclassing/).\n",
    "\n",
    "In general, all weights are trainable weights. The only built-in layer that has\n",
    "non-trainable weights is the `BatchNormalization` layer. It uses non-trainable weights\n",
    " to keep track of the mean and variance of its inputs during training.\n",
    "To learn how to use non-trainable weights in your own custom layers, see the\n",
    "[guide to writing new layers from scratch](https://keras.io/guides/making_new_layers_and_models_via_subclassing/).\n",
    "\n",
    "**Example: the `BatchNormalization` layer has 2 trainable weights and 2 non-trainable\n",
    " weights**\n",
    " \n",
    " **例子：`BatchNormalization`层有3个可训练权重和2个不可训练权重**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 4\n",
      "trainable_weights: 2\n",
      "non_trainable_weights: 2\n"
     ]
    }
   ],
   "source": [
    "layer = keras.layers.BatchNormalization()\n",
    "layer.build((None, 4))  # Create the weights\n",
    "\n",
    "print(\"weights:\", len(layer.weights))\n",
    "print(\"trainable_weights:\", len(layer.trainable_weights))\n",
    "print(\"non_trainable_weights:\", len(layer.non_trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Layers & models also feature a boolean attribute `trainable`. Its value can be changed.\n",
    "Setting `layer.trainable` to `False` moves all the layer's weights from trainable to\n",
    "non-trainable.  This is called \"freezing\" the layer: the state of a frozen layer won't\n",
    "be updated during training (either when training with `fit()` or when training with\n",
    " any custom loop that relies on `trainable_weights` to apply gradient updates).\n",
    "\n",
    "**Example: setting `trainable` to `False`**\n",
    "\n",
    "层和模型都有一个布尔型的属性`trainable`，其值可以改变。把`layer.trainable`的值设置为`False`就可以把该层所有权重都变成不可训练。该过程我们称为冷冻\"freezing\"该层：该层的状态在训练过程中不会改变。\n",
    "**例子：设置`trainable` to `False**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(3)\n",
    "layer.build((None, 4))  # Create the weights\n",
    "layer.trainable = False  # Freeze the layer\n",
    "\n",
    "print(\"weights:\", len(layer.weights))\n",
    "print(\"trainable_weights:\", len(layer.trainable_weights))\n",
    "print(\"non_trainable_weights:\", len(layer.non_trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "When a trainable weight becomes non-trainable, its value is no longer updated during\n",
    " training.\n",
    "\n",
    "当可训练的权重被设置成不可训练non-trainable，其值在训练过程中不再可以更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 894us/step - loss: 0.0948\n"
     ]
    }
   ],
   "source": [
    "# Make a model with 2 layers\n",
    "layer1 = keras.layers.Dense(3, activation=\"relu\")\n",
    "layer2 = keras.layers.Dense(3, activation=\"sigmoid\")\n",
    "model = keras.Sequential([keras.Input(shape=(3,)), layer1, layer2])\n",
    "\n",
    "# Freeze the first layer\n",
    "layer1.trainable = False\n",
    "\n",
    "# Keep a copy of the weights of layer1 for later reference\n",
    "initial_layer1_weights_values = layer1.get_weights()\n",
    "\n",
    "# Train the model\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "model.fit(np.random.random((2, 3)), np.random.random((2, 3)))\n",
    "\n",
    "# Check that the weights of layer1 have not changed during training\n",
    "final_layer1_weights_values = layer1.get_weights()\n",
    "np.testing.assert_allclose(\n",
    "    initial_layer1_weights_values[0], final_layer1_weights_values[0]\n",
    ")\n",
    "np.testing.assert_allclose(\n",
    "    initial_layer1_weights_values[1], final_layer1_weights_values[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "不要混淆`layer.trainable`属性和`layer.__call__()`中参数`training` ，后者是控制该层在推理或者训练模式下是否运行前向过程。\n",
    "\n",
    "Do not confuse the `layer.trainable` attribute with the argument `training` in\n",
    "`layer.__call__()` (which controls whether the layer should run its forward pass in\n",
    " inference mode or training mode). For more information, see the\n",
    "[Keras FAQ](\n",
    "  https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Recursive setting of the `trainable` attribute 递归地设置 `trainable`\n",
    "\n",
    "If you set `trainable = False` on a model or on any layer that has sublayers,\n",
    "all children layers become non-trainable as well.\n",
    "\n",
    "如果我们再一个模型或者任何一个layer上设置`trainable = False`，则其所有子layers 也会变成non-trainable\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "inner_model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(3,)),\n",
    "        keras.layers.Dense(3, activation=\"relu\"),\n",
    "        keras.layers.Dense(3, activation=\"relu\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [keras.Input(shape=(3,)), inner_model, keras.layers.Dense(3, activation=\"sigmoid\"),]\n",
    ")\n",
    "\n",
    "model.trainable = False  # Freeze the outer model\n",
    "\n",
    "assert inner_model.trainable == False  # All layers in `model` are now frozen\n",
    "assert inner_model.layers[0].trainable == False  # `trainable` is propagated recursively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## 典型的迁移学习流程， The typical transfer-learning workflow\n",
    "\n",
    "This leads us to how a typical transfer learning workflow can be implemented in Keras:\n",
    "\n",
    "1. Instantiate a base model and load pre-trained weights into it.\n",
    "2. Freeze all layers in the base model by setting `trainable = False`.\n",
    "3. Create a new model on top of the output of one (or several) layers from the base\n",
    " model.\n",
    "4. Train your new model on your new dataset.\n",
    "\n",
    "keras中典型迁移学习流程实现如下：\n",
    "\n",
    "1. 生成一个基础模型并加载预训练好的权重\n",
    "2. 冷冻该模型中所有参数：`trainable = False`\n",
    "3. 在该基础模型的输出层（或者几个不同的输出层）基础上创建新的模型\n",
    "4. 在新的数据集上训练新的模型。\n",
    "\n",
    "Note that an alternative, more lightweight workflow could also be:\n",
    "\n",
    "1. Instantiate a base model and load pre-trained weights into it.\n",
    "2. Run your new dataset through it and record the output of one (or several) layers\n",
    " from the base model. This is called **feature extraction**.\n",
    "3. Use that output as input data for a new, smaller model.\n",
    "\n",
    "注意一个可替换且更轻量级的流程如下：\n",
    "\n",
    "1. 生成一个基础模型并加载预训练好的权重\n",
    "2. 把新数据放入该模型，然后记录该模型的输出（或者几个不同的输出层的输出）。这个过程称作为 **特征抽取**\n",
    "3. 使用以上输出为一个新的且更小的模型的输入\n",
    "\n",
    "A key advantage of that second workflow is that you only run the base model once on\n",
    " your data, rather than once per epoch of training. So it's a lot faster & cheaper.\n",
    "\n",
    "该流程的一个关键好处是：你只需要运行base模型一次，而不需要每个epoch的训练中都运行。所以将会更快和低成本。\n",
    "\n",
    "An issue with that second workflow, though, is that it doesn't allow you to dynamically\n",
    "modify the input data of your new model during training, which is required when doing\n",
    "data augmentation, for instance. Transfer learning is typically used for tasks when\n",
    "your new dataset has too little data to train a full-scale model from scratch, and in\n",
    "such scenarios data augmentation is very important. So in what follows, we will focus\n",
    " on the first workflow.\n",
    " \n",
    " 但是第二个流程的问题是，在训练过程中我们无法动态的修改模型的输入，也就是说我们无法做数据的增强等操作。\n",
    " 迁移学习一般用在只有很少数据可用于训练的任务中，因为数据太少我们无法从头开始训练一个好的模型，而且该条件下旺旺数据增强是非常重要的。接下来，我们主要关注第一个流程。\n",
    "\n",
    "Here's what the first workflow looks like in Keras:\n",
    "\n",
    "First, instantiate a base model with pre-trained weights.\n",
    "\n",
    "以下是第一个流程在Keras中使用的样例：\n",
    "\n",
    "首先，使用预训练好的权重实例化一个基础模型：\n",
    "\n",
    "\n",
    "```python\n",
    "base_model = keras.applications.Xception(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False)  # Do not include the ImageNet classifier at the top.\n",
    "```\n",
    "\n",
    "Then, freeze the base model.\n",
    "\n",
    "然后，冷冻该模型。\n",
    "\n",
    "```python\n",
    "base_model.trainable = False\n",
    "```\n",
    "\n",
    "Create a new model on top.\n",
    "\n",
    "在旧模型上，创建一个新的模型：\n",
    "\n",
    "```python\n",
    "inputs = keras.Input(shape=(150, 150, 3))\n",
    "# We make sure that the base_model is running in inference mode here,\n",
    "# by passing `training=False`. This is important for fine-tuning, as you will\n",
    "# learn in a few paragraphs.\n",
    "x = base_model(inputs, training=False)\n",
    "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "# A Dense classifier with a single unit (binary classification)\n",
    "outputs = keras.layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "```\n",
    "\n",
    "Train the model on new data.\n",
    "\n",
    "在新数据上训练该模型：\n",
    "\n",
    "```python\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])\n",
    "model.fit(new_dataset, epochs=20, callbacks=..., validation_data=...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## 微调，Fine-tuning\n",
    "\n",
    "Once your model has converged on the new data, you can try to unfreeze all or part of\n",
    " the base model and retrain the whole model end-to-end with a very low learning rate.\n",
    " \n",
    "一旦模型在新的数据上收敛了，我们就可以解冻所有或者部分base模型，并使用非常小的学习率端到端地重新训练整个模型。\n",
    "\n",
    "This is an optional last step that can potentially give you incremental improvements.\n",
    " It could also potentially lead to quick overfitting -- keep that in mind.\n",
    " \n",
    "最后一步是可选的，但有潜力进一步提高模型的性能，也可以导致overfitting过学习。\n",
    "\n",
    "It is critical to only do this step *after* the model with frozen layers has been\n",
    "trained to convergence. If you mix randomly-initialized trainable layers with\n",
    "trainable layers that hold pre-trained features, the randomly-initialized layers will\n",
    "cause very large gradient updates during training, which will destroy your pre-trained\n",
    " features.\n",
    " \n",
    "重要的是仅当**带冷冻层的模型训练到收敛**再进行这一步。\n",
    "如果把随机初始化的可训练的层与保存预训练特征的可训练层会在一起的话，随机初始化的层会导致非常大的梯度更新，\n",
    "从而破坏预训练的特征。\n",
    "\n",
    "It's also critical to use a very low learning rate at this stage, because\n",
    "you are training a much larger model than in the first round of training, on a dataset\n",
    " that is typically very small.\n",
    "As a result, you are at risk of overfitting very quickly if you apply large weight\n",
    " updates. Here, you only want to readapt the pretrained weights in an incremental way.\n",
    "\n",
    "另外一个关键点是，微调阶段使用非常小的学习率，因为该阶段要训练的模型比第一阶段大不少而且数据集较小。\n",
    "如果使用较大的权重更新，过学习的风险很大。这里，我们只应该逐步的改造预训练好的权重。\n",
    "\n",
    "This is how to implement fine-tuning of the whole base model:\n",
    "\n",
    "以下是如何实现对整个基础模型的微调\n",
    "\n",
    "\n",
    "```python\n",
    "# Unfreeze the base model，解冻基础模型\n",
    "base_model.trainable = True\n",
    "\n",
    "# It's important to recompile your model after you make any changes\n",
    "# to the `trainable` attribute of any inner layer, so that your changes\n",
    "# are take into account\n",
    "# 重新编译模型，使得更改生效\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-5),  # Very low learning rate\n",
    "              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "# Train end-to-end. Be careful to stop before you overfit!\n",
    "# 端到端训练，overfit前停止训练\n",
    "model.fit(new_dataset, epochs=10, callbacks=..., validation_data=...)\n",
    "```\n",
    "\n",
    "**Important note about `compile()` and `trainable`**\n",
    "\n",
    "**重要注意：`compile()` 和 `trainable`**\n",
    "\n",
    "Calling `compile()` on a model is meant to \"freeze\" the behavior of that model. This\n",
    " implies that the `trainable`\n",
    "attribute values at the time the model is compiled should be preserved throughout the\n",
    " lifetime of that model,\n",
    "until `compile` is called again. Hence, if you change any `trainable` value, make sure\n",
    " to call `compile()` again on your\n",
    "model for your changes to be taken into account.\n",
    "\n",
    "调用`compile()`意味着其对应的模型的行为就固定了。这意味着`trainable`属性在模型编译后就不能改了，除非`compile`被重新调用。因此，修改`trainable` 后必须重新编译。\n",
    "\n",
    "**Important notes about `BatchNormalization` layer**\n",
    "\n",
    "**`BatchNormalization`层的重要注意事项**\n",
    "\n",
    "Many image models contain `BatchNormalization` layers. That layer is a special case on\n",
    " every imaginable count. Here are a few things to keep in mind.\n",
    " \n",
    " 很多图像模型包含`BatchNormalization`层。\n",
    " 以下几点需要注意。\n",
    " \n",
    " - `BatchNormalization` 包含2个在训练过程中需要更新得non-trainable的权重。这些事跟踪输入的均值和方差。\n",
    " - 当`bn_layer.trainable = False`时， `BatchNormalization`层将运行推理模式，并且不会更新mean & variance。\n",
    " 这跟其它层基本都不一样，因为权重可训练性与推理/训练模型是两个正交的概念。\n",
    "[weight trainability & inference/training modes are two orthogonal concepts](\n",
    "  https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute).\n",
    "但是这两个概念在`BatchNormalization`中纠缠在一起\n",
    "\n",
    "- 当我们解冻一个包含`BatchNormalization`层的模型来做微调时，需要记住在推理模式时设置`training=False`。\n",
    "否则学习到的模型整个都会被破坏。\n",
    "\n",
    "在该指南的的最后面，将在一个端到端的例子中实践展示这种模式。\n",
    "  \n",
    "- `BatchNormalization` contains 2 non-trainable weights that get updated during\n",
    "training. These are the variables tracking the mean and variance of the inputs.\n",
    "- When you set `bn_layer.trainable = False`, the `BatchNormalization` layer will\n",
    "run in inference mode, and will not update its mean & variance statistics. This is not\n",
    "the case for other layers in general, as\n",
    "[weight trainability & inference/training modes are two orthogonal concepts](\n",
    "  https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute).\n",
    "But the two are tied in the case of the `BatchNormalization` layer.\n",
    "- When you unfreeze a model that contains `BatchNormalization` layers in order to do\n",
    "fine-tuning, you should keep the `BatchNormalization` layers in inference mode by\n",
    " passing `training=False` when calling the base model.\n",
    "Otherwise the updates applied to the non-trainable weights will suddenly destroy\n",
    "what the model has learned.\n",
    "\n",
    "You'll see this pattern in action in the end-to-end example at the end of this guide.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Transfer learning & fine-tuning with a custom training loop\n",
    "## 迁移学习&微调中自定义训练loop\n",
    "\n",
    "If instead of `fit()`, you are using your own low-level training loop, the workflow\n",
    "stays essentially the same. You should be careful to only take into account the list\n",
    " `model.trainable_weights` when applying gradient updates:\n",
    "\n",
    "如果不使用`fit()`，需要使用自定义的底层的训练循环，该流程本质上还是一样。\n",
    "你需要非常小心，在应用梯度更新时，仅对 `model.trainable_weights`列表中的更新。\n",
    "\n",
    "```python\n",
    "# Create base model\n",
    "base_model = keras.applications.Xception(\n",
    "    weights='imagenet',\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False)\n",
    "# Freeze base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create new model on top.\n",
    "inputs = keras.Input(shape=(150, 150, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "outputs = keras.layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "# Iterate over the batches of a dataset.\n",
    "for inputs, targets in new_dataset:\n",
    "    # Open a GradientTape.\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass.\n",
    "        predictions = model(inputs)\n",
    "        # Compute the loss value for this batch.\n",
    "        loss_value = loss_fn(targets, predictions)\n",
    "\n",
    "    # Get gradients of loss wrt the *trainable* weights.\n",
    "    gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "    # Update the weights of the model.\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Likewise for fine-tuning.\n",
    "\n",
    "微调也类似"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## An end-to-end example: fine-tuning an image classification model on a cats vs. dogs\n",
    "## 端到端例子：在cats vs. dogs数据集上微调图像分类模型\n",
    "\n",
    " dataset 数据集\n",
    "\n",
    "To solidify these concepts, let's walk you through a concrete end-to-end transfer\n",
    "learning & fine-tuning example. We will load the Xception model, pre-trained on\n",
    " ImageNet, and use it on the Kaggle \"cats vs. dogs\" classification dataset.\n",
    " \n",
    "为了巩固这些概念，我们一起过一个具体的端到端的迁移学习和微调的例子。本例子中使用Xception模型，在ImageNet上训练的，并且使用它来解决kaggle的\"cats vs. dogs\"的数据集分类问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Getting the data\n",
    "### 获取数据\n",
    "\n",
    "First, let's fetch the cats vs. dogs dataset using TFDS. If you have your own dataset,\n",
    "you'll probably want to use the utility\n",
    "`tf.keras.preprocessing.image_dataset_from_directory` to generate similar labeled\n",
    " dataset objects from a set of images on disk filed into class-specific folders.\n",
    " \n",
    "首先，使用TFDS获取该数据集。如果你已经有了该数据集，可以使用`tf.keras.preprocessing.image_dataset_from_directory`来从硬盘上图像集中生产类似的带标签的数据集对象。\n",
    "\n",
    "Transfer learning is most useful when working with very small datasets. To keep our\n",
    "dataset small, we will use 40% of the original training data (25,000 images) for\n",
    " training, 10% for validation, and 10% for testing.\n",
    " \n",
    "迁移学习在数据集非常小的时候是最有用的。为了保持我们的数据集较小，将使用40%的原始数据 (25,000 images)来训练，10% 为验证集，10%为测试集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'op' and 'message'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAbortedError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow_datasets/core/utils/py_utils.py\u001b[0m in \u001b[0;36mtry_reraise\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m     \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow_datasets/core/registered.py\u001b[0m in \u001b[0;36mbuilder\u001b[0;34m(name, **builder_init_kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m       prefix=\"Failed to construct dataset {}\".format(name)):\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbuilder_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbuilder_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow_datasets/core/api_utils.py\u001b[0m in \u001b[0;36mdisallow_positional_args_dec\u001b[0;34m(fn, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0m_check_required\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_dir, config, version)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Use the code version (do not restore data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_from_bucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow_datasets/core/dataset_info.py\u001b[0m in \u001b[0;36minitialize_from_bucket\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0mtmp_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdtemp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tfds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m     \u001b[0mdata_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcs_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcs_dataset_info_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow_datasets/core/utils/gcs_utils.py\u001b[0m in \u001b[0;36mgcs_dataset_info_files\u001b[0;34m(dataset_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[0;34m\"\"\"Return paths to GCS files in the given dataset directory.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgcs_listdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposixpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGCS_DATASET_INFO_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow_datasets/core/utils/gcs_utils.py\u001b[0m in \u001b[0;36mgcs_listdir\u001b[0;34m(dir_name)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0mroot_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcs_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mis_gcs_disabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mfile_exists_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0m_pywrap_file_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileExists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAbortedError\u001b[0m: All 10 retry attempts failed. The last failure: Unavailable: Error executing an HTTP request: libcurl code 35 meaning 'SSL connect error', error details: BoringSSL SSL_connect: SSL_ERROR_SYSCALL in connection to www.googleapis.com:443 \n\t when reading metadata of gs://tfds-data/dataset_info/cats_vs_dogs/4.0.0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-bd2a9ad47ab4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_progress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m train_ds, validation_ds, test_ds = tfds.load(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"cats_vs_dogs\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Reserve 10% for validation and 10% for test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow_datasets/core/api_utils.py\u001b[0m in \u001b[0;36mdisallow_positional_args_dec\u001b[0;34m(fn, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0m_check_no_positional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mismethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallowed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0m_check_required\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdisallow_positional_args_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=no-value-for-parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow_datasets/core/registered.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m   \u001b[0mdbuilder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mbuilder_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow_datasets/core/registered.py\u001b[0m in \u001b[0;36mbuilder\u001b[0;34m(name, **builder_init_kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m   with py_utils.try_reraise(\n\u001b[1;32m    243\u001b[0m       prefix=\"Failed to construct dataset {}\".format(name)):\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbuilder_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbuilder_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow_datasets/core/utils/py_utils.py\u001b[0m in \u001b[0;36mtry_reraise\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow_datasets/core/utils/py_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(prefix, suffix)\u001b[0m\n\u001b[1;32m    390\u001b[0m   \u001b[0msuffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m   \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_value\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_traceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'op' and 'message'"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "train_ds, validation_ds, test_ds = tfds.load(\n",
    "    \"cats_vs_dogs\",\n",
    "    # Reserve 10% for validation and 10% for test\n",
    "    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n",
    "    as_supervised=True,  # Include labels\n",
    ")\n",
    "\n",
    "print(\"Number of training samples: %d\" % tf.data.experimental.cardinality(train_ds))\n",
    "print(\n",
    "    \"Number of validation samples: %d\" % tf.data.experimental.cardinality(validation_ds)\n",
    ")\n",
    "print(\"Number of test samples: %d\" % tf.data.experimental.cardinality(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "These are the first 9 images in the training dataset -- as you can see, they're all\n",
    " different sizes.\n",
    " \n",
    "以下展示训练集中前9张图片，都不一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, (image, label) in enumerate(train_ds.take(9)):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(int(label))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We can also see that label 1 is \"dog\" and label 0 is \"cat\".\n",
    "\n",
    "标签为1的是 \"dog\"，0是\"cat\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Standardizing the data\n",
    "### 数据标准化\n",
    "\n",
    "Our raw images have a variety of sizes. In addition, each pixel consists of 3 integer\n",
    "values between 0 and 255 (RGB level values). This isn't a great fit for feeding a\n",
    " neural network. We need to do 2 things:\n",
    "\n",
    "- Standardize to a fixed image size. We pick 150x150.\n",
    "- Normalize pixel values between -1 and 1. We'll do this using a `Normalization` layer as\n",
    " part of the model itself.\n",
    " \n",
    " 原始图片有着不同的大小。此外，每个像素点包含3个0-255的整型数值（RGB）。不便于直接放入神经网络训练，我们需要做一下两件事：\n",
    " \n",
    " - 标准化每张图片到固定大小。本例子选择150*150\n",
    " - 规范化像素值到-1到1之间，我们使用`Normalization`层来实现这个，该层也作为模型的一层。\n",
    "\n",
    "In general, it's a good practice to develop models that take raw data as input, as\n",
    "opposed to models that take already-preprocessed data. The reason being that, if your\n",
    "model expects preprocessed data, any time you export your model to use it elsewhere\n",
    "(in a web browser, in a mobile app), you'll need to reimplement the exact same\n",
    "preprocessing pipeline. This gets very tricky very quickly. So we should do the least\n",
    " possible amount of preprocessing before hitting the model.\n",
    " \n",
    "通常来说，相对使用已经处理好的数据作为输入，使用原始数据作为输入input来开发模型是很好的做法。\n",
    "原因是在不同应用环境（app，浏览器），我们不必重新实现预处理过程。\n",
    "\n",
    "Here, we'll do image resizing in the data pipeline (because a deep neural network can\n",
    "only process contiguous batches of data), and we'll do the input value scaling as part\n",
    " of the model, when we create it.\n",
    " \n",
    "这里，我们将在数据pipeline中实现图像resizing大小调整（因为深度神经网络只能处理了连续小批的数据），\n",
    "并且将把输入数据缩放作为模型的一部分。\n",
    "\n",
    "Let's resize images to 150x150:\n",
    "\n",
    "调整图像大小为150x150:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "size = (150, 150)\n",
    "\n",
    "train_ds = train_ds.map(lambda x, y: (tf.image.resize(x, size), y))\n",
    "validation_ds = validation_ds.map(lambda x, y: (tf.image.resize(x, size), y))\n",
    "test_ds = test_ds.map(lambda x, y: (tf.image.resize(x, size), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Besides, let's batch the data and use caching & prefetching to optimize loading speed.\n",
    "\n",
    "此外，把数据准备成batch小批，并使用缓冲和预读取来优化读取速度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_ds = train_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
    "validation_ds = validation_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
    "test_ds = test_ds.cache().batch(batch_size).prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Using random data augmentation\n",
    "### 使用随机数据增强\n",
    "\n",
    "When you don't have a large image dataset, it's a good practice to artificially\n",
    " introduce sample diversity by applying random yet realistic transformations to\n",
    "the training images, such as random horizontal flipping or small random rotations. This\n",
    "helps expose the model to different aspects of the training data while slowing down\n",
    " overfitting.\n",
    " \n",
    "当没有大的数据集时，人工地通过使用随机但现实的变换原始训练数据，从而引入样本的多样性是一个好的办法。\n",
    "例如随机水平翻转或小的随机旋转。这有助于把训练数据的不同方面暴露给模型，从而减慢过学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "        layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's visualize what the first image of the first batch looks like after various random\n",
    " transformations:\n",
    " \n",
    "下面可视化第一个batch中的第一张图片随机变换后的样子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for images, labels in train_ds.take(1):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    first_image = images[0]\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        augmented_image = data_augmentation(\n",
    "            tf.expand_dims(first_image, 0), training=True\n",
    "        )\n",
    "        plt.imshow(augmented_image[0].numpy().astype(\"int32\"))\n",
    "        plt.title(int(labels[i]))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Build a model\n",
    "## 构建模型\n",
    "\n",
    "Now let's built a model that follows the blueprint we've explained earlier.\n",
    "\n",
    "现在开始构建模型，如前面所述。\n",
    "\n",
    "Note that:\n",
    "\n",
    "- We add a `Normalization` layer to scale input values (initially in the `[0, 255]`\n",
    " range) to the `[-1, 1]` range.\n",
    "- We add a `Dropout` layer before the classification layer, for regularization.\n",
    "- We make sure to pass `training=False` when calling the base model, so that\n",
    "it runs in inference mode, so that batchnorm statistics don't get updated\n",
    "even after we unfreeze the base model for fine-tuning.\n",
    "\n",
    "注意：\n",
    "\n",
    "- 我们需要增加一个`Normalization`层把输入数值缩放到`[-1, 1]`\n",
    "- 在分类层中增加`Dropout`层进行规范化\n",
    "- 调用基础模型时确保传入`training=False`，因为此刻是推理模式批量。这样规范化统计量不会被更新，即使当我们解冻基础模型进行微调时也一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "base_model = keras.applications.Xception(\n",
    "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False,\n",
    ")  # Do not include the ImageNet classifier at the top.\n",
    "\n",
    "# Freeze the base_model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create new model on top\n",
    "inputs = keras.Input(shape=(150, 150, 3))\n",
    "x = data_augmentation(inputs)  # Apply random data augmentation\n",
    "\n",
    "# Pre-trained Xception weights requires that input be normalized\n",
    "# from (0, 255) to a range (-1., +1.), the normalization layer\n",
    "# does the following, outputs = (inputs - mean) / sqrt(var)\n",
    "norm_layer = keras.layers.experimental.preprocessing.Normalization()\n",
    "mean = np.array([127.5] * 3)\n",
    "var = mean ** 2\n",
    "# Scale inputs to [-1, +1]\n",
    "x = norm_layer(x)\n",
    "norm_layer.set_weights([mean, var])\n",
    "\n",
    "# The base model contains batchnorm layers. We want to keep them in inference mode\n",
    "# when we unfreeze the base model for fine-tuning, so we make sure that the\n",
    "# base_model is running in inference mode here.\n",
    "x = base_model(x, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "outputs = keras.layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Train the top layer\n",
    "## 训练最上面的层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()],\n",
    ")\n",
    "\n",
    "epochs = 20\n",
    "model.fit(train_ds, epochs=epochs, validation_data=validation_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Do a round of fine-tuning of the entire model\n",
    "## 微调整个模型一轮\n",
    "\n",
    "Finally, let's unfreeze the base model and train the entire model end-to-end with a low\n",
    " learning rate.\n",
    " \n",
    "最后，解冻基础模型并且使用较小的学习率端到端训练整个模型\n",
    "\n",
    "Importantly, although the base model becomes trainable, it is still running in\n",
    "inference mode since we passed `training=False` when calling it when we built the\n",
    "model. This means that the batch normalization layers inside won't update their batch\n",
    "statistics. If they did, they would wreck havoc on the representations learned by the\n",
    " model so far.\n",
    " \n",
    "重要的是，尽管基础模型变得可以训练了，它任然是运行在推理模式，因为构建模型时传入了 `training=False` 。\n",
    "内部的batch normalization层将不会更新batch统计量。如果更新了，将会破坏模型到目前为止所学习到的表示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Unfreeze the base_model. Note that it keeps running in inference mode\n",
    "# since we passed `training=False` when calling it. This means that\n",
    "# the batchnorm layers will not update their batch statistics.\n",
    "# This prevents the batchnorm layers from undoing all the training\n",
    "# we've done so far.\n",
    "base_model.trainable = True\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-5),  # Low learning rate\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()],\n",
    ")\n",
    "\n",
    "epochs = 10\n",
    "model.fit(train_ds, epochs=epochs, validation_data=validation_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "After 10 epochs, fine-tuning gains us a nice improvement here.\n",
    "\n",
    "训练10个epochs后，微调获得非常好的提高。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "transfer_learning",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
