{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# TensorFlow Keras 介绍-工程师版\n",
    "\n",
    "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
    "**Date created:** 2020/04/01<br>\n",
    "**Last modified:** 2020/04/28<br>\n",
    "**Description:** 使用TensorFlow keras高级api构建真实世界机器学习解决方案你所需要知道的 (Everything you need to know to use Keras to build real-world machine learning solutions.)<br>\n",
    "**翻译：** 叶正 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## 设置-Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## 介绍\n",
    "\n",
    "你是一位正在寻找基于tensorflow-keras驱动的深度学习在实际产品中解决方案的工程师吗？\n",
    "本指南将为你介绍tf-keras 核心api概念和使用方法。\n",
    "\n",
    "本指南中，你将学到以下知识:\n",
    "\n",
    "- Tensorflow tensor 和 gradient tape\n",
    "- 训练模型前的数据准备 (转化为 NumPy 数组ndarray 或者 `tf.data.Dataset` 对象).\n",
    "- 数据预处理：如特征标注化(feature normalization) 或词汇表索引 vocabulary indexing.\n",
    "- 模型构建：数据到预测模型,使用Keras Functional API.\n",
    "- 使用keras自带的`fit()`方法训练模型，同时可以存储checkpoints, 监控指标和容错\n",
    "- 模型评估和推理（test data）\n",
    "- 自定义 `fit()`功能, 如对抗网络GAN训练.\n",
    "- 使用多GPU加速训练\n",
    "- 模型优化：参数调优（hyperparameter tuning.）\n",
    "- 在移动设备和 IoT 设备上部署机器学习模型\n",
    "\n",
    "在该指南最后，你可以接着学下以下内容来增强你对这些概念的理解\n",
    "\n",
    "- 图像分类，Image classification\n",
    "- 文本分类，Text classification\n",
    "- 信用卡欺诈检测，Credit card fraud detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量，Tensors\n",
    "\n",
    "TensorFlow 是可微编程的一个基本构造层。它的核心像Numpy，是一个可以对N维矩阵（tensors）进行操作控制的框架。\n",
    "\n",
    "然而，Numpy与TensorFlow有三个关键不同之处：\n",
    "\n",
    "- 1.TensorFlow可以利用硬件如GPUs和TPUs，进行加速\n",
    "- 2.TensorFlow能对任意可微矩阵自动计算梯度\n",
    "- 3.TensorFlow的计算可以分配到一台或多台机器的大量设备上。\n",
    "\n",
    "首先认识一下TensorFlow的核心：Tensor\n",
    "\n",
    "**常量Tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[5 2]\n",
      " [1 3]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[5, 2], [1, 3]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "它的值可以通过调用`.numpy()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 2],\n",
       "       [1, 3]], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "像Numpy数组，对变量赋予dtype和shape的特征，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype: <dtype: 'int32'>\n",
      "shape: (2, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"dtype:\", x.dtype)\n",
    "print(\"shape:\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常用`tf.ones`和`tf.zeros`（就像np.ones和np.zeros）新建常量`tensors` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.]\n",
      " [1.]], shape=(2, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.]\n",
      " [0.]], shape=(2, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.ones(shape=(2, 1)))\n",
    "print(tf.zeros(shape=(2, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建随机常量型张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.35567674  0.53794354]\n",
      " [ 1.3858515  -1.4235883 ]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal(shape=(2, 2), mean=0.0, stddev=1.0)\n",
    "#x = tf.random.uniform(shape=(2, 2), minval=0, maxval=10, dtype=\"int32\")\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 变量，Variables\n",
    "\n",
    "特殊的tensors，可以储存可变的状态，例如模型的权重weights\n",
    "\n",
    "可以创建带初始值的`Variable`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[ 0.01790815,  1.2378219 ],\n",
      "       [ 0.12776296, -0.18256545]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "initial_value = tf.random.normal(shape=(2, 2))\n",
    "a = tf.Variable(initial_value)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用`.assign(value)`，`.assign_add(increment)`或者`.assign_sub(decrement)`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[-0.83476335,  0.79339784],\n",
      "       [-0.15659136, -0.6752024 ]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "new_value = tf.random.normal(shape=(2, 2))\n",
    "a.assign(new_value)\n",
    "print(a)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        assert a[i, j] == new_value[i, j]\n",
    "\n",
    "added_value = tf.random.normal(shape=(2, 2))\n",
    "a.assign_add(added_value)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        assert a[i, j] == new_value[i, j] + added_value[i, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度\n",
    "\n",
    "另一个与Numpy主要不同在于，可以自动查找任何可微表达式的梯度。只需打开`GradientTape`，通过`tape.watch()` watching，建立可微表达式并用作输入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.44801906 -0.33583528]\n",
      " [ 0.61287636 -0.8127741 ]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.normal(shape=(2, 2))\n",
    "b = tf.random.normal(shape=(2, 2))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(a)  # Start recording the history of operations applied to `a`\n",
    "    c = tf.sqrt(tf.square(a) + tf.square(b))  # Do some math using `a`\n",
    "    # What's the gradient of `c` with respect to `a`?\n",
    "    dc_da = tape.gradient(c, a)\n",
    "    print(dc_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过嵌套tapes，可以计算高阶倒数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.73551667 -0.18915759]\n",
      " [ 0.4022748  -0.47815204]], shape=(2, 2), dtype=float32) <class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.normal(shape=(2, 2))\n",
    "b = tf.random.normal(shape=(2, 2))\n",
    "\n",
    "with tf.GradientTape() as outer_tape:\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(a)\n",
    "        c = tf.sqrt(tf.square(a) + tf.square(b))\n",
    "        dc_da = tape.gradient(c, a)\n",
    "        print(dc_da, type(dc_da))\n",
    "    d2c_da2 = outer_tape.gradient(dc_da, a)\n",
    "    print(d2c_da2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## 数据加载和预处理，Data loading & preprocessing\n",
    "\n",
    "神经网络无法直接处理原始数据，如文本文件、经过编码的JPEG图像文件或者CSV文件。\n",
    "神经网络只能处理**向量化vectorized**和**standardized标准化的**表示\n",
    "\n",
    "- 文本文件需要读入后转化为**tf string tensor**，然后分隔成单词(token)。最后对字词建立索引并转换成整数型tensor。\n",
    "- 图片数据需要读入后并解码成整型integer tensor，然后转换成浮点型并归一化成较小的数值（通常0~1）.\n",
    "- CSV数据首先需要解析，将数值型属性转换成浮点型floating tensor，对categorical 类别型属性索引并转换成整型tensor。\n",
    "通常对每个属性值进行归一化，使其具有零平均值和单位方差。\n",
    "\n",
    "开始！\n",
    "\n",
    "## 数据加载，Data loading\n",
    "\n",
    "tf-Keras 模型接受3中类型的输入inputs:\n",
    "\n",
    "- **NumPy arrays**, 与Scikit-Learn和其他Python库类似。如果数据能读入内存，这是个不错的选择\n",
    "- **[TensorFlow `Dataset` objects](https://www.tensorflow.org/guide/data)**. TensorFlow Dataset objects，这可以提高性能，更适合于数据不能读入内存的数据集来说，且数据从硬盘或其他分布式文件系统读取的方式。\n",
    "- **Python generators** 可以生成不同批次的数据（例如：定制的`keras.utils.Sequence`的子类）。 \n",
    "\n",
    "在训练模型前，数据形式需要符合这三种之一。如果数据集较大，且在GPU上训练模型，建议考虑使用`Dataset`对象，因为这个类可以处理好性能关键的具体工作：\n",
    "- 当GPU忙的时候，可以在CPU上异步地预处理数据集，并缓冲成队列。\n",
    "- 将数据预读入GPU内存，在GPU处理完前一批数据时可以立即获得数据，因此可以充分利用GPU。\n",
    "\n",
    "\n",
    "Keras有一些列工具可以将硬盘上原始数据转换成Dataset：\n",
    "- `tf.keras.preprocessing.image_dataset_from_directory` 将存储在特定分类文件夹中的图形文件转换成带标签的图形tensor数据集。\n",
    "- `tf.keras.preprocessing.text_dataset_from_directory` 与上述类似，但针对文本文件。\n",
    "\n",
    "此外，TensorFlow `tf.data`包含其他类似的工具，例如`tf.data.experimental.make_csv_dataset`，从CSV文件加载结构化数据。\n",
    "\n",
    "\n",
    "**例子：从硬盘上图形文件中获取带标注的数据集**\n",
    "\n",
    "假设图形文件按类别存储在不同的文件夹中，如下所示：\n",
    "\n",
    "```\n",
    "main_directory/\n",
    "...class_a/\n",
    "......a_image_1.jpg\n",
    "......a_image_2.jpg\n",
    "...class_b/\n",
    "......b_image_1.jpg\n",
    "......b_image_2.jpg\n",
    "```\n",
    "\n",
    "可以操作如下：\n",
    "\n",
    "```python\n",
    "# 创建数据集\n",
    "dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "  'path/to/main_directory', batch_size=64, image_size=(200, 200))\n",
    "\n",
    "# 迭代访问该dataset生成的数据batches\n",
    "for data, labels in dataset:\n",
    "   print(data.shape)  # (64, 200, 200, 3)\n",
    "   print(data.dtype)  # float32\n",
    "   print(labels.shape)  # (64,)\n",
    "   print(labels.dtype)  # int32\n",
    "```\n",
    "样本的标签可以是它所在文件夹的数字字母序号。很自然，这也可以显示的赋值，如：`class_names=['class_a', 'class_b']`，标签`0`赋值给class_a，标签`1`赋值给`class_b`。\n",
    "\n",
    "\n",
    "**例子：从文本文件获取带标签的数据集**\n",
    "\n",
    "同样地，后缀为`.txt`的文件分类存储在不同文件夹中，你可以：\n",
    "\n",
    "```python\n",
    "dataset = keras.preprocessing.text_dataset_from_directory(\n",
    "  'path/to/main_directory', batch_size=64)\n",
    "\n",
    "# 样例\n",
    "for data, labels in dataset:\n",
    "   print(data.shape)  # (64,)\n",
    "   print(data.dtype)  # string\n",
    "   print(labels.shape)  # (64,)\n",
    "   print(labels.dtype)  # int32\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Keras数据预处理，Data preprocessing with Keras\n",
    "\n",
    "当数据是字符串/整型/浮点型Nmumpy 矩阵，或者是`Dataset`对象（或者Python生成器）用于生成成批的字符串/整型/浮点型 tensor，此时就需要数据预处理**preprocess**。\n",
    "\n",
    "这就是说：\n",
    "- 字符串型数据的分词，并索引属性，Tokenization of string data, followed by token indexing.\n",
    "- 特征归一化，Feature normalization.\n",
    "- Rescaling，数据缩放至更小值（一般来说，神经网络的输出数值应该接近零，通常希望数据零平均值和单位方差，或者数据在`[0,1]`。）\n",
    "\n",
    "\n",
    "### 理想的机器学习模型是端到端的 end-to-end\n",
    "\n",
    "通常，需要设法使**数据预处理尽可能成为模型的一部分**，而不是外加一个数据处理通道。这是因为当需要重复使用模型时，外加的数据预处理**可移植性较差**。比如一个处理文本的模型：使用一个特殊的分词算法和一个专门的词汇索引。当需要迁移模型至移动app或JavaScriptapp，需要用目标语言重新设立预处理。这可能非常棘手，任何一下点与原处理过程不一致就可能彻底让模型无效，或者严重降低它的效果。\n",
    "\n",
    "如果能简单的导出端对端的模型就会变得很简单，因为预处理已经包含在其中。理想的模型是期望输入越接近原始数据越好：图形模型最好是`[0,255]`RGB像素值，文本模型最好是utf-8的字符串。这样使用导出模型就不需要知道预处理环节。\n",
    "\n",
    "\n",
    "### Keras预处理层 Using Keras preprocessing layers\n",
    "\n",
    "\n",
    "在Keras中，模型内置预处理常使用预处理层**preprocessing layers**。包括：\n",
    "- `TextVectorization`层：使原始字符串型文本数据的向量化\n",
    "- `Normalization`层：属性值标准化\n",
    "- 图形缩放、修剪、图形数据增强,Image rescaling, cropping, or image data augmentation\n",
    "\n",
    "使用Keras预处理层的主要好处：在训练中或训练后直接引入模型，使得模型可移植性变强。\n",
    "\n",
    "有些预处理层have a state:\n",
    "- `TextVectorization`：保留词或词组到整型索引的映射\n",
    "- `Normalization`：保留特征的平均值和方差\n",
    "\n",
    "\n",
    "预处理层的状态可以在部分训练样本或全部样本上调用`layer.adapt(data)`获得。\n",
    "\n",
    "**例子：将字符串转换成整型词索引序列**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[4 5 2 9 3]\n",
      " [7 6 2 8 3]], shape=(2, 5), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "# Example training data, of dtype `string`.\n",
    "training_data = np.array([[\"This is the 1st sample.\"], [\"And here's the 2nd sample.\"]])\n",
    "\n",
    "# Create a TextVectorization layer instance. It can be configured to either\n",
    "# return integer token indices, or a dense token representation (e.g. multi-hot\n",
    "# or TF-IDF). The text standardization and text splitting algorithms are fully\n",
    "# configurable.\n",
    "vectorizer = TextVectorization(output_mode=\"int\")\n",
    "\n",
    "# Calling `adapt` on an array or dataset makes the layer generate a vocabulary\n",
    "# index for the data, which can then be reused when seeing new data.\n",
    "vectorizer.adapt(training_data)\n",
    "\n",
    "# After calling adapt, the layer is able to encode any n-gram it has seen before\n",
    "# in the `adapt()` data. Unknown n-grams are encoded via an \"out-of-vocabulary\"\n",
    "# token.\n",
    "integer_data = vectorizer(training_data)\n",
    "print(integer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**例子：将字符串转换成one-hot编码的双词序列**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0.]], shape=(2, 17), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "# Example training data, of dtype `string`.\n",
    "training_data = np.array([[\"This is the 1st sample.\"], [\"And here's the 2nd sample.\"]])\n",
    "\n",
    "# Create a TextVectorization layer instance. It can be configured to either\n",
    "# return integer token indices, or a dense token representation (e.g. multi-hot\n",
    "# or TF-IDF). The text standardization and text splitting algorithms are fully\n",
    "# configurable.\n",
    "vectorizer = TextVectorization(output_mode=\"binary\", ngrams=2)\n",
    "\n",
    "# Calling `adapt` on an array or dataset makes the layer generate a vocabulary\n",
    "# index for the data, which can then be reused when seeing new data.\n",
    "vectorizer.adapt(training_data)\n",
    "\n",
    "# After calling adapt, the layer is able to encode any n-gram it has seen before\n",
    "# in the `adapt()` data. Unknown n-grams are encoded via an \"out-of-vocabulary\"\n",
    "# token.\n",
    "integer_data = vectorizer(training_data)\n",
    "print(integer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**例子：标准化属性值**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "\n",
    "# Example image data, with values in the [0, 255] range\n",
    "training_data = np.random.randint(0, 256, size=(64, 200, 200, 3)).astype(\"float32\")\n",
    "\n",
    "normalizer = Normalization(axis=-1)\n",
    "normalizer.adapt(training_data)\n",
    "\n",
    "normalized_data = normalizer(training_data)\n",
    "print(\"var: %.4f\" % np.var(normalized_data))\n",
    "print(\"mean: %.4f\" % np.mean(normalized_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**例子：缩放和中心裁剪图像**\n",
    "\n",
    "`Rescaling`层和`CenterCrop`层都是无状态的stateless，因此不需要调用`adapt（）`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import CenterCrop\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "\n",
    "# Example image data, with values in the [0, 255] range\n",
    "training_data = np.random.randint(0, 256, size=(64, 200, 200, 3)).astype(\"float32\")\n",
    "\n",
    "cropper = CenterCrop(height=150, width=150)\n",
    "scaler = Rescaling(scale=1.0 / 255)\n",
    "\n",
    "output_data = scaler(cropper(training_data))\n",
    "print(\"shape:\", output_data.shape)\n",
    "print(\"min:\", np.min(output_data))\n",
    "print(\"max:\", np.max(output_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## 采用Keras Functional API建立模型\n",
    "\n",
    "**模型的一层A \"layer\"简单说就是输入输出的转换**。比如：线性映射层就是将输入映射到16维属性空间：\n",
    "\n",
    "```python\n",
    "dense = keras.layers.Dense(units=16)\n",
    "```\n",
    "\n",
    "而一个模型\"model\"就是由多个层`layer`组成的有向无环图。一个模型可以想象成一个大的层，里面含很多子层可以通过引入数据训练。\n",
    "\n",
    "最常用且最有效的办法建立Keras模型就是`funtional API`。可以从指定特定形状（dtype可选）的输入开始采用功能API建立模型。Keras里，如果每个维度可变，则可指定为None。例如：输入为200*200的RGB图形可以为（200，200，3），但是输入为任意大小RGB的图像则可定义为（None，None，3）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Let's say we expect our inputs to be RGB images of arbitrary size\n",
    "inputs = keras.Input(shape=(None, None, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "定义好输入形式后，可以在输入的基础上链接层转换直到得到最终输出结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# Center-crop images to 150x150\n",
    "x = CenterCrop(height=150, width=150)(inputs)\n",
    "# Rescale images to [0, 1]\n",
    "x = Rescaling(scale=1.0 / 255)(x)\n",
    "\n",
    "# Apply some convolution and pooling layers\n",
    "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=(3, 3))(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=(3, 3))(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "\n",
    "# Apply global average pooling to get flat feature vectors\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add a dense classifier on top\n",
    "num_classes = 10\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "当你像搭积木一样定义好由不同层组成的有向无环图时，就建立了你的输入到输出的转化，也就是生成了一个模型对象："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "这个模型就想一个大的layer，可以输入一个batch的数据，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "data = np.random.randint(0, 256, size=(64, 200, 200, 3)).astype(\"float32\")\n",
    "processed_data = model(data)\n",
    "print(processed_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "可以打印出模型的摘要，其显示的是你的数据在模型的每个阶段是如何做变换的。这对程序调试非常有用。\n",
    " \n",
    " 需要注意的是，每层输出都会显示`batch的大小batch size`。这里batch大小为None，表明模型可以处理任意batch大小的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "当你的模型有多个输入和输出的时候，Functional API使得模型的构建更加的容易。\n",
    "\n",
    "想更深入得了解次部分，请看[guide to the Functional API](/guides/functional_api/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## 使用 keras model的 `fit()`方法进行训练\n",
    "\n",
    "现在，已经学会了的：\n",
    "- 怎么准备数据\n",
    "- 怎么建立处理数据的模型\n",
    "\n",
    "下一步就是在数据上训练模型。`Model`类具有内置训练循环，`fit()`方法。`Dataset`对象、可以参数batch 数据的`Python生成器`或者`Numpy矩阵`。\n",
    "\n",
    "在调用`fit()`前，需要指定`优化器optimizer`和`损失函数loss function`。这就是`compile()`:\n",
    "\n",
    "\n",
    "```python\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=keras.losses.CategoricalCrossentropy())\n",
    "```\n",
    "\n",
    "损失函数和优化器可以通过字符串标识符指定：\n",
    "\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "```\n",
    "\n",
    "一旦模型编译了，就可以给模型输入数据。以下就是给模型输入Numpy数据的例子：\n",
    "\n",
    "```python\n",
    "model.fit(numpy_array_of_samples, numpy_array_of_labels,\n",
    "          batch_size=32, epochs=10)\n",
    "```\n",
    "\n",
    "除了数据，还需要指定2个关键参数：`batch_size`和重复次数（`epochs`）。以下是训练时batch为32个样本，重复10次的例子。\n",
    "\n",
    "\n",
    "```python\n",
    "model.fit(dataset_of_samples_and_labels, epochs=10)\n",
    "```\n",
    "\n",
    "因为从数据集上生成的数据通常是分了批的，通常不需要指定batch大小。\n",
    "\n",
    "以下是MINIST数字分类的例子：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Get the data as Numpy arrays\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Build a simple model\n",
    "inputs = keras.Input(shape=(28, 28))\n",
    "x = layers.experimental.preprocessing.Rescaling(1.0 / 255)(inputs)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
    "\n",
    "# Train the model for 1 epoch from Numpy data\n",
    "batch_size = 64\n",
    "print(\"Fit on NumPy data\")\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=1)\n",
    "\n",
    "# Train the model for 1 epoch using a dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "print(\"Fit on Dataset\")\n",
    "history = model.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "调用Fit()时返回“history”对象，记录整个训练过程发生了什么。`history.history`词典包含每个epoch时的metrics值（本例子中只有一个metric，loss）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "深入 `fit()`, 请看：\n",
    "[guide to training & evaluation with the built-in Keras methods](\n",
    "  /guides/training_with_built_in_methods/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### 跟踪性能指标\n",
    "\n",
    "当训练模型时，需要跟踪如`分类准确率`、`精度`、`召回率`，`AUC`等指标。此外，不仅在训练数据集上在验证数据集上也需要监控这些指标。\n",
    "\n",
    "**监控指标**\n",
    "\n",
    "可以将指标对象赋值给`compile()`，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    ")\n",
    "history = model.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**将验证数据传递给`fit()`**\n",
    "\n",
    "将验证数据传递给`fit()`可以监控验证损失和验证指标。验证指标再每次重复后会上报。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "history = model.fit(dataset, epochs=1, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Using callbacks for checkpointing (and more) 用callbacks做保持模型\n",
    "\n",
    "如果训练时间较长，那么在训练过程中定时保存模型就尤为重要。训练过程一旦崩溃，就可以利用保存的模型重新训练重新开始。\n",
    "\n",
    "Keras一个重要特征就是**callbacks**，在`fit()`中配置。Callbacks是在训练过程中不同节点调用的对象，尤其是：\n",
    "- 在每个batch的开始和结束\n",
    "- 每个epoch的开始和结束\n",
    "\n",
    "Callbacks 是使得模型训练可以完全脚本化的一种方法\n",
    "\n",
    "你可以使用callbacks周期性的来存储你的模型。\n",
    "\n",
    "举例: 使用`ModelCheckpoint` callback 来在每个epoch结束时存储模型。\n",
    "\n",
    "```python\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='path/to/my/model_{epoch}',\n",
    "        save_freq='epoch')\n",
    "]\n",
    "model.fit(dataset, epochs=2, callbacks=callbacks)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "也可以使用callbacks周期性的更改学习率，把监控的各种metrics发到slack机器人、邮件通知等。\n",
    "\n",
    "深入详见 [callbacks API documentation](/api/callbacks/) 和\n",
    "[guide to writing custom callbacks](/guides/writing_your_own_callbacks/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### 使用TensorBoard监控训练过程\n",
    "\n",
    "keras 命令行中进度条不是最友好的方法来监控模型的loss和metrics。更好的选择是\n",
    "[TensorBoard](https://www.tensorflow.org/tensorboard), 一个基于web的应用，可以实时的显示loss，metrics以及更多。\n",
    "\n",
    "使用方法如下：传入 `keras.callbacks.TensorBoard` callback:\n",
    "\n",
    "```python\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "]\n",
    "model.fit(dataset, epochs=2, callbacks=callbacks)\n",
    "```\n",
    "\n",
    "tensorboard启动方法:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=./logs\n",
    "```\n",
    "\n",
    "更多使用方法请看：\n",
    "[Here's more information](https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### 调用 `fit()`后: 评估测试性能和生产对新数据的预测\n",
    "\n",
    "模型训练好后，可以使用 `evaluate()`评估模型在新数据集上的loss和metrics："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(val_dataset)  # returns loss and metrics\n",
    "print(\"loss: %.2f\" % loss)\n",
    "print(\"acc: %.2f\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    " 也可以使用`predict()`预测，`predict()`是用来预测新数据不需要标签，所以不返回loss等指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(val_dataset)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## `fit()`中使用自定义的训练步骤training step\n",
    "\n",
    "默认设置，`fit()`是配置为监督学习环境。如果需要不同的训练过程（如对抗网络GAN的训练循环），可以提供自定义的实现`Model.train_step()`，keras内部`fit()`方法会重复的调用该方法。\n",
    "\n",
    "\n",
    "Metrics, callbacks,可以如常工作\n",
    "\n",
    "下面是重新实现 `fit()`：\n",
    "\n",
    "```python\n",
    "class CustomModel(keras.Model):\n",
    "  def train_step(self, data):\n",
    "    # Unpack the data. Its structure depends on your model and\n",
    "    # on what you pass to `fit()`.\n",
    "    x, y = data\n",
    "    with tf.GradientTape() as tape:\n",
    "      y_pred = self(x, training=True)  # Forward pass\n",
    "      # Compute the loss value\n",
    "      # (the loss function is configured in `compile()`)\n",
    "      loss = self.compiled_loss(y, y_pred,regularization_losses=self.losses)\n",
    "        \n",
    "    # Compute gradients\n",
    "    trainable_vars = self.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_vars)\n",
    "    # Update weights\n",
    "    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "    # Update metrics (includes the metric that tracks the loss)\n",
    "    self.compiled_metrics.update_state(y, y_pred)\n",
    "    # Return a dict mapping metric names to current value\n",
    "    return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "# Construct and compile an instance of CustomModel\n",
    "inputs = keras.Input(shape=(32,))\n",
    "outputs = keras.layers.Dense(1)(inputs)\n",
    "model = CustomModel(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=[...])\n",
    "\n",
    "# Just use `fit` as usual\n",
    "model.fit(dataset, epochs=3, callbacks=...)\n",
    "```\n",
    "\n",
    "想更深入请看:\n",
    "[\"Customizing what happens in `fit()`\"](/guides/customizing_what_happens_in_fit/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## 使用即可执行eager execution来调试你的模型\n",
    "\n",
    "如果自定义training steps or custom layers，通常需要对其进行调试。\n",
    "\n",
    " The debugging experience is an integral part of a framework: with Keras, the debugging\n",
    " workflow is designed with the user in mind.\n",
    "\n",
    "默认情况下，keras模型会编译成高度优化的计算图，执行速度更快。也就是说模型中所写的python code(e.g. in a custom `train_step`)，并不是实际执行的code。这使得debug较为困难\n",
    "\n",
    "通城Debugging最后能一步一步的执行，大家都喜欢用print(打印出过程信息)，设置你还想用`pdb`。这时我们需要使用即可执行eager execution模型，如下：\n",
    "\n",
    "参数设置 `run_eagerly=True`，在 `compile()`方法中:\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='mse', run_eagerly=True)\n",
    "```\n",
    "\n",
    "当然，该中方法的不足是模型要显著的慢一些。当模型完成调试时，正式训练时还是建议使用计算图模式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## 使用多GPU加速训练\n",
    "\n",
    "tf Keras 自带工业级的多GPU支持，和分布式multi-worker训练。通过 `tf.distribute` API实现.\n",
    "\n",
    "如果你的机器上有多个GPU，可以同时使用所有GPU训练模型：\n",
    "\n",
    "- 创建 `tf.distribute.MirroredStrategy` 对象\n",
    "- 在strategy's scope内构建和编译模型\n",
    "- 跟之前一样调用`fit()` and `evaluate()` \n",
    "\n",
    "```python\n",
    "# Create a MirroredStrategy.\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Open a strategy scope.\n",
    "with strategy.scope():\n",
    "  # Everything that creates variables should be under the strategy scope.\n",
    "  # In general this is only model construction & `compile()`.\n",
    "  model = Model(...)\n",
    "  model.compile(...)\n",
    "\n",
    "# Train the model on all available devices.\n",
    "train_dataset, val_dataset, test_dataset = get_dataset()\n",
    "model.fit(train_dataset, epochs=2, validation_data=val_dataset)\n",
    "\n",
    "# Test the model on all available devices.\n",
    "model.evaluate(test_dataset)\n",
    "```\n",
    "\n",
    "For a detailed introduction to multi-GPU & distributed training, see\n",
    "[this guide](/guides/distributed_training/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## GPU设备上同步进行预处理 VS. 异步在主机CPU上预处理\n",
    "\n",
    "前文中讲述了预处理，其中直接在模型中使用(`CenterCrop` and `Rescaling`)等预处理层。\n",
    "\n",
    "如果我们想在设备上做预处理，预处理作为模型的一部分是一个很好的选择。比如，GPU加速的特征标注化或图像数据扩展（image augmentation）。\n",
    "\n",
    "但是这种预处理在以下情况不适合：特别是使用`TextVectorization`层进行文本预处理。由于其序列特性且只能在CPU上运行，在CPU上使用异步处理是个更好想法。\n",
    "\n",
    "异步处理时，预处理操作在CPU上运行。当你的GPU忙时（处理上一个batch的数据），预处理后的samples会缓存到一个队列queue中。在GPU可用前，就可以把已经在queue中缓冲的处理好的样本提前获取（prefetching）到GPU内存中。这就保证了预处理不会阻塞GPU的使用。\n",
    "\n",
    "异步预处理，使用`dataset.map`来注入预处理操作到数据处理流程pipeline中即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Example training data, of dtype `string`.\n",
    "samples = np.array([[\"This is the 1st sample.\"], [\"And here's the 2nd sample.\"]])\n",
    "labels = [[0], [1]]\n",
    "\n",
    "# Prepare a TextVectorization layer.\n",
    "vectorizer = TextVectorization(output_mode=\"int\")\n",
    "vectorizer.adapt(samples)\n",
    "\n",
    "# Asynchronous preprocessing: the text vectorization is part of the tf.data pipeline.\n",
    "# First, create a dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((samples, labels)).batch(2)\n",
    "# Apply text vectorization to the samples\n",
    "dataset = dataset.map(lambda x, y: (vectorizer(x), y))\n",
    "# Prefetch with a buffer size of 2 batches\n",
    "dataset = dataset.prefetch(2)\n",
    "\n",
    "# Our model should expect sequences of integers as inputs\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = layers.Embedding(input_dim=10, output_dim=32)(inputs)\n",
    "outputs = layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", run_eagerly=True)\n",
    "model.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "与文本向量化预处理作为模型一部分对比："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Our dataset will yield samples that are strings\n",
    "dataset = tf.data.Dataset.from_tensor_slices((samples, labels)).batch(2)\n",
    "\n",
    "# Our model should expect strings as inputs\n",
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "x = vectorizer(inputs)\n",
    "x = layers.Embedding(input_dim=10, output_dim=32)(x)\n",
    "outputs = layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", run_eagerly=True)\n",
    "model.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "当模型训练也在CPU上的时候，我们将基本感受不到以上两种设置的性能差别。\n",
    "但当在GPU上训练时，在CPU异步缓冲预处理，会带来极大的速度提升。\n",
    "\n",
    "训练结束后，如果我们想导出一个端到端的模型（包含预处理层），也会很容易因为`TextVectorization`也是一个layer。\n",
    "\n",
    "\n",
    "```python\n",
    "inputs = keras.Input(shape=(1,), dtype='string')\n",
    "x = vectorizer(inputs)\n",
    "outputs = trained_model(x)\n",
    "end_to_end_model = keras.Model(inputs, outputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## 通过超参数调整（hyperparameter tuning）寻找最优模型配置\n",
    "Finding the best model configuration with hyperparameter tuning\n",
    "\n",
    "\n",
    "当我们有了一个可使用的模型，就会想如何优化其配置 -- 模型架构选择，层的大小等。凭个人直觉或经验很难找到最优的模型，一个更系统化的方法是：hyperparameter search.\n",
    "\n",
    "可以使用\n",
    "[Keras Tuner](https://keras-team.github.io/keras-tuner/documentation/tuners/) 来找到keras模型的最优超参数。tf keras中 调用 `fit()`即可，就是这么容易。\n",
    "\n",
    "那它是如何工作的呢？\n",
    "\n",
    "首先，把你的模型定义放在一个函数中，该函数只有一个名为`hp`的参数。\n",
    "这个函数中，我们把想要调优的超参数替换为超参数采用的方法, e.g. `hp.Int()` or `hp.Choice()`:\n",
    "\n",
    "\n",
    "```python\n",
    "def build_model(hp):\n",
    "    inputs = keras.Input(shape=(784,))\n",
    "    x = layers.Dense(\n",
    "        units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
    "        activation='relu'))(inputs)\n",
    "    outputs = layers.Dense(10, activation='softmax')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate',\n",
    "                      values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "```\n",
    "\n",
    "该函数须返回的是编译后的模型。\n",
    "\n",
    "下一步，创建一个tuner对象，指定优化目标和其它的搜索参数：\n",
    "\n",
    "\n",
    "```python\n",
    "import kerastuner\n",
    "\n",
    "tuner = kerastuner.tuners.Hyperband(\n",
    "  build_model,\n",
    "  objective='val_loss',\n",
    "  max_epochs=100,\n",
    "  max_trials=200,\n",
    "  executions_per_trial=2,\n",
    "  directory='my_dir')\n",
    "```\n",
    "\n",
    "最后，使用`search()`方法就可以开始搜索，该方法接受跟 `Model.fit()`一样的参数：\n",
    "\n",
    "```python\n",
    "tuner.search(dataset, validation_data=val_dataset)\n",
    "```\n",
    "\n",
    "搜索结束后，可以使用一下方法得到最优参数模型：\n",
    "\n",
    "```python\n",
    "models = tuner.get_best_models(num_models=2)\n",
    "```\n",
    "\n",
    "或打印出结果摘要:\n",
    "\n",
    "```python\n",
    "tuner.results_summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在移动设备和 IoT 设备上部署机器学习模型\n",
    "- 选择模型：选择新模型或重新训练现有模型。\n",
    "- 转换：使用 TensorFlow Lite Converter 将 TensorFlow 模型转换为压缩平面缓冲区。\n",
    "- 部署：获取压缩的 .tflite 文件，并将其加载到移动设备或嵌入式设备中。\n",
    "- 优化：通过将 32 位浮点数转换为更高效的 8 位整数进行量化，或者在 GPU 上运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## 端到端的例子\n",
    "\n",
    "To familiarize yourself with the concepts in this introduction, see the following\n",
    " end-to-end examples:\n",
    "\n",
    "- [Text classification](/examples/nlp/text_classification_from_scratch/)\n",
    "- [Image classification](/examples/vision/image_classification_from_scratch/)\n",
    "- [Credit card fraud detection](/examples/structured_data/imbalanced_classification/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## 进阶学习资料\n",
    "\n",
    "- Learn more about the\n",
    "[Functional API](/guides/functional_api/).\n",
    "- Learn more about the\n",
    "[features of `fit()` and `evaluate()`](/guides/training_with_built_in_methods/).\n",
    "- Learn more about\n",
    "[callbacks](/guides/writing_your_own_callbacks/).\n",
    "- Learn more about\n",
    "[creating your own custom training steps](/guides/customizing_what_happens_in_fit/).\n",
    "- Learn more about\n",
    "[multi-GPU and distributed training](/guides/distributed_training/).\n",
    "- Learn how to do [transfer learning](/guides/transfer_learning/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "intro_to_keras_for_engineers",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
