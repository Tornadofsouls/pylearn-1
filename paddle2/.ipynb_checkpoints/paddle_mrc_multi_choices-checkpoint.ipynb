{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeffye/anaconda3/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "\n",
    "# from paddle.fluid.dataloader import Dataset\n",
    "from paddle.io import Dataset\n",
    "from sklearn.model_selection import *\n",
    "# 导入相关的模块\n",
    "import paddle\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.data import Stack, Pad, Tuple\n",
    "import paddle.nn.functional as F\n",
    "import numpy as np\n",
    "from functools import partial  # partial()函数可以用来固定某些参数值，并返回一个新的callable对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    15421.000000\n",
      "mean      1039.781272\n",
      "std        435.583878\n",
      "min         38.000000\n",
      "25%        744.000000\n",
      "50%       1067.000000\n",
      "75%       1251.000000\n",
      "max       3047.000000\n",
      "Name: content_len, dtype: float64\n",
      "count    2444.000000\n",
      "mean     1054.175941\n",
      "std       438.861006\n",
      "min        52.000000\n",
      "25%       758.000000\n",
      "50%      1067.000000\n",
      "75%      1262.000000\n",
      "max      4640.000000\n",
      "Name: content_len, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2021-04-13 15:45:32,708] [    INFO]\u001b[0m - Found /home/jeffye/.paddlenlp/models/bert-base-chinese/bert-base-chinese-vocab.txt\u001b[0m\n",
      "\u001b[32m[2021-04-13 15:45:32,723] [    INFO]\u001b[0m - Already cached /home/jeffye/.paddlenlp/models/bert-base-chinese/bert-base-chinese.pdparams\u001b[0m\n",
      "/home/jeffye/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/layers.py:1303: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/home/jeffye/anaconda3/lib/python3.8/site-packages/paddle/fluid/dygraph/layers.py:1303: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-744410bf1b0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprepare_train_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "train_file_path = \"/mnt/d/datasets/nlp/mrc/haihua/train.json\"\n",
    "validation_file_path = \"/mnt/d/datasets/nlp/mrc/haihua/validation.json\"\n",
    "\n",
    "with open(train_file_path, 'r', encoding='utf-8')as f:  # 读入json文件\n",
    "    train_data = json.load(f)\n",
    "\n",
    "train_df = []\n",
    "\n",
    "for i in range(len(train_data)):  # 将每个文章-问题-答案作为一条数据\n",
    "    data = train_data[i]\n",
    "    content = data['Content']\n",
    "    questions = data['Questions']\n",
    "    for question in questions:\n",
    "        question['Content'] = content\n",
    "        train_df.append(question)\n",
    "\n",
    "# print(train_df[0])\n",
    "\n",
    "train_df = pd.DataFrame(train_df)  # 转换成csv表格更好看一点\n",
    "\n",
    "# print(train_df.head())\n",
    "\n",
    "with open(validation_file_path, 'r', encoding='utf-8')as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "test_df = []\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    data = test_data[i]\n",
    "    content = data['Content']\n",
    "    questions = data['Questions']\n",
    "    cls = data['Type']\n",
    "    diff = data['Diff']\n",
    "    for question in questions:\n",
    "        question['Content'] = content\n",
    "        question['Type'] = cls\n",
    "        question['Diff'] = diff\n",
    "        test_df.append(question)\n",
    "\n",
    "test_df = pd.DataFrame(test_df)\n",
    "\n",
    "train_df['content_len'] = train_df['Content'].apply(len)  # 统计content文本长度\n",
    "test_df['content_len'] = test_df['Content'].apply(len)\n",
    "\n",
    "print(train_df.content_len.describe())\n",
    "print(test_df.content_len.describe())\n",
    "\n",
    "# plt.title('content_length') #content非常长，绝大部分都远大于512\n",
    "# plt.plot(sorted(train_df.content_len))\n",
    "# plt.show()\n",
    "\n",
    "train_df['label'] = train_df['Answer'].apply(lambda x: ['A', 'B', 'C', 'D'].index(x))  # 将标签从ABCD转成0123\n",
    "test_df['label'] = 0\n",
    "\n",
    "train_df.to_csv('train.csv', index=False)\n",
    "test_df.to_csv('test.csv', index=False)\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "train_df['label'] = train_df['Answer'].apply(lambda x: ['A', 'B', 'C', 'D'].index(x))  # 将标签从ABCD转成0123\n",
    "test_df['label'] = 0\n",
    "\n",
    "from paddlenlp.datasets import load_dataset\n",
    "\n",
    "\n",
    "# def read(data_path, df_source):\n",
    "#     for i, row in df_source.iterrows():\n",
    "#         yield {'questions': row['Question'], 'choices': row['Choices'], 'labels': row['label']}\n",
    "#\n",
    "#\n",
    "# import inspect\n",
    "#\n",
    "# print(inspect.isfunction(read))\n",
    "# print(type(partial(read, df_source=train_df)))\n",
    "#\n",
    "# # data_path为read()方法的参数\n",
    "# train_ds = load_dataset(partial(read, df_source=train_df), data_path=train_file_path, lazy=False)\n",
    "# # iter_ds = load_dataset(partial(read, df_source=train_df), data_path=train_file_path, lazy=True)\n",
    "# test_ds = load_dataset(partial(read, df_source=test_df), data_path=train_file_path, lazy=False)\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.df = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):  # 将一条数据从(文章,问题,4个选项)转成(文章,问题,选项1)、(文章,问题,选项2)...\n",
    "        label = self.df.label.values[idx]\n",
    "        question = self.df.Question.values[idx]\n",
    "        content = self.df.Content.values[idx]\n",
    "        choice = self.df.Choices.values[idx][2:-2].split('\\', \\'')\n",
    "        if len(choice) < 4:  # 如果选项不满四个，就补“不知道”\n",
    "            for i in range(4 - len(choice)):\n",
    "                choice.append('D．不知道')\n",
    "\n",
    "        content = [content for i in range(len(choice))]\n",
    "        pair = [question + ' ' + i[2:] for i in choice]\n",
    "\n",
    "        if idx >= 15420 or idx <= 1:\n",
    "            print('idx', idx)\n",
    "            print(content, )\n",
    "            print(pair)\n",
    "            print(label)\n",
    "        return content, pair, label\n",
    "\n",
    "\n",
    "train_ds = MyDataset(train_df)\n",
    "test_ds = MyDataset(test_df)\n",
    "\n",
    "MODEL_NAME = \"bert-base-chinese\"\n",
    "# 调用ppnlp.transformers.BertTokenizer进行数据处理，tokenizer可以把原始输入文本转化成模型model可接受的输入数据格式。\n",
    "tokenizer = ppnlp.transformers.BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = ppnlp.transformers.BertForSequenceClassification.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "import json\n",
    "import paddle\n",
    "from paddlenlp.metrics.squad import squad_evaluate, compute_prediction\n",
    "\n",
    "\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    all_start_logits = []\n",
    "    all_end_logits = []\n",
    "    tic_eval = time.time()\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids = batch\n",
    "        start_logits_tensor, end_logits_tensor = model(input_ids,\n",
    "                                                       token_type_ids)\n",
    "\n",
    "        for idx in range(start_logits_tensor.shape[0]):\n",
    "            if len(all_start_logits) % 1000 == 0 and len(all_start_logits):\n",
    "                print(\"Processing example: %d\" % len(all_start_logits))\n",
    "                print('time per 1000:', time.time() - tic_eval)\n",
    "                tic_eval = time.time()\n",
    "\n",
    "            all_start_logits.append(start_logits_tensor.numpy()[idx])\n",
    "            all_end_logits.append(end_logits_tensor.numpy()[idx])\n",
    "\n",
    "    all_predictions, _, _ = compute_prediction(\n",
    "        data_loader.dataset.data, data_loader.dataset.new_data,\n",
    "        (all_start_logits, all_end_logits), False, 20, 30)\n",
    "\n",
    "    # Can also write all_nbest_json and scores_diff_json files if needed\n",
    "    with open('prediction.json', \"w\", encoding='utf-8') as writer:\n",
    "        writer.write(\n",
    "            json.dumps(\n",
    "                all_predictions, ensure_ascii=False, indent=4) + \"\\n\")\n",
    "\n",
    "    squad_evaluate(\n",
    "        examples=data_loader.dataset.data,\n",
    "        preds=all_predictions,\n",
    "        is_whitespace_splited=False)\n",
    "\n",
    "    count = 0\n",
    "    for example in data_loader.dataset.data:\n",
    "        count += 1\n",
    "        print()\n",
    "        print('问题：', example['question'])\n",
    "        print('原文：', ''.join(example['context']))\n",
    "        print('答案：', all_predictions[example['id']])\n",
    "        if count >= 5:\n",
    "            break\n",
    "\n",
    "    model.train()\n",
    "\n",
    "\n",
    "def prepare_train_features(examples, tokenizer, doc_stride, max_seq_length):\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    contexts = [examples[i]['context'] for i in range(len(examples))]\n",
    "    questions = [examples[i]['question'] for i in range(len(examples))]\n",
    "\n",
    "    tokenized_examples = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        stride=doc_stride,\n",
    "        max_seq_len=max_seq_length)\n",
    "\n",
    "    print(len(examples), len(tokenized_examples))\n",
    "    \n",
    "    # Let's label those examples!\n",
    "    for i, tokenized_example in enumerate(tokenized_examples):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_example[\"input_ids\"]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "        # help us compute the start_positions and end_positions.\n",
    "        offsets = tokenized_example['offset_mapping']\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_example['token_type_ids']\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = tokenized_example['overflow_to_sample']\n",
    "        answers = examples[sample_index]['answers']\n",
    "        answer_starts = examples[sample_index]['answer_starts']\n",
    "\n",
    "        # Start/end character index of the answer in the text.\n",
    "        start_char = answer_starts[0]\n",
    "        end_char = start_char + len(answers[0])\n",
    "\n",
    "        # Start token index of the current span in the text.\n",
    "        token_start_index = 0\n",
    "        while sequence_ids[token_start_index] != 1:\n",
    "            token_start_index += 1\n",
    "\n",
    "        # End token index of the current span in the text.\n",
    "        token_end_index = len(input_ids) - 1\n",
    "        while sequence_ids[token_end_index] != 1:\n",
    "            token_end_index -= 1\n",
    "        # Minus one more to reach actual text\n",
    "        token_end_index -= 1\n",
    "\n",
    "        # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "        if not (offsets[token_start_index][0] <= start_char and\n",
    "                offsets[token_end_index][1] >= end_char):\n",
    "            tokenized_examples[i][\"start_positions\"] = cls_index\n",
    "            tokenized_examples[i][\"end_positions\"] = cls_index\n",
    "        else:\n",
    "            # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "            # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "            while token_start_index < len(offsets) and offsets[\n",
    "                token_start_index][0] <= start_char:\n",
    "                token_start_index += 1\n",
    "            tokenized_examples[i][\"start_positions\"] = token_start_index - 1\n",
    "            while offsets[token_end_index][1] >= end_char:\n",
    "                token_end_index -= 1\n",
    "            tokenized_examples[i][\"end_positions\"] = token_end_index + 1\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "\n",
    "def prepare_validation_features(examples, tokenizer, doc_stride, max_seq_length):\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    contexts = [examples[i]['context'] for i in range(len(examples))]\n",
    "    questions = [examples[i]['question'] for i in range(len(examples))]\n",
    "\n",
    "    tokenized_examples = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        stride=doc_stride,\n",
    "        max_seq_len=max_seq_length)\n",
    "\n",
    "    # For validation, there is no need to compute start and end positions\n",
    "    for i, tokenized_example in enumerate(tokenized_examples):\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_example['token_type_ids']\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = tokenized_example['overflow_to_sample']\n",
    "        tokenized_examples[i][\"example_id\"] = examples[sample_index]['id']\n",
    "\n",
    "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "        # position is part of the context or not.\n",
    "        tokenized_examples[i][\"offset_mapping\"] = [\n",
    "            (o if sequence_ids[k] == 1 else None)\n",
    "            for k, o in enumerate(tokenized_example[\"offset_mapping\"])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples, tokenizer, doc_stride, max_seq_length):\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    contexts = [examples[i]['context'] for i in range(len(examples))]\n",
    "    questions = [examples[i]['question'] for i in range(len(examples))]\n",
    "\n",
    "    tokenized_examples = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        stride=doc_stride,\n",
    "        max_seq_len=max_seq_length)\n",
    "\n",
    "    print(len(examples), len(tokenized_examples))\n",
    "    \n",
    "    # Let's label those examples!\n",
    "    for i, tokenized_example in enumerate(tokenized_examples):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_example[\"input_ids\"]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "        # help us compute the start_positions and end_positions.\n",
    "        offsets = tokenized_example['offset_mapping']\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_example['token_type_ids']\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = tokenized_example['overflow_to_sample']\n",
    "        answers = examples[sample_index]['answers']\n",
    "        answer_starts = examples[sample_index]['answer_starts']\n",
    "\n",
    "        # Start/end character index of the answer in the text.\n",
    "        start_char = answer_starts[0]\n",
    "        end_char = start_char + len(answers[0])\n",
    "\n",
    "        # Start token index of the current span in the text.\n",
    "        token_start_index = 0\n",
    "        while sequence_ids[token_start_index] != 1:\n",
    "            token_start_index += 1\n",
    "\n",
    "        # End token index of the current span in the text.\n",
    "        token_end_index = len(input_ids) - 1\n",
    "        while sequence_ids[token_end_index] != 1:\n",
    "            token_end_index -= 1\n",
    "        # Minus one more to reach actual text\n",
    "        token_end_index -= 1\n",
    "\n",
    "        # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "        if not (offsets[token_start_index][0] <= start_char and\n",
    "                offsets[token_end_index][1] >= end_char):\n",
    "            tokenized_examples[i][\"start_positions\"] = cls_index\n",
    "            tokenized_examples[i][\"end_positions\"] = cls_index\n",
    "        else:\n",
    "            # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "            # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "            while token_start_index < len(offsets) and offsets[\n",
    "                token_start_index][0] <= start_char:\n",
    "                token_start_index += 1\n",
    "            tokenized_examples[i][\"start_positions\"] = token_start_index - 1\n",
    "            while offsets[token_end_index][1] >= end_char:\n",
    "                token_end_index -= 1\n",
    "            tokenized_examples[i][\"end_positions\"] = token_end_index + 1\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 484\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'answers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-94ca622698ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m ]\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprepare_train_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-b42a9d60e465>\u001b[0m in \u001b[0;36mprepare_train_features\u001b[0;34m(examples, tokenizer, doc_stride, max_seq_length)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# One example can give several spans, this is the index of the example containing this span of text.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0msample_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_example\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'overflow_to_sample'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0manswers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0manswer_starts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer_starts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'answers'"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    {'context': '天气真好哦'* 5, 'question': '和大量水分减少'},\n",
    "    {'context': '明天的也天气真好哦大幅度' * 50, 'question': '和大量水分减少'},\n",
    "]\n",
    "\n",
    "prepare_train_features(examples, tokenizer, 1,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
